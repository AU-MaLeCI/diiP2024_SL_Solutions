{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxCvjMEV1xSa"
      },
      "source": [
        "# Image classification with Vision Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (2.13.1)\n",
            "Requirement already satisfied: gdown in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (5.2.0)\n",
            "Requirement already satisfied: tensorflow[and-cuda] in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (2.13.1)\n",
            "\u001b[33mWARNING: tensorflow 2.13.1 does not provide the extra 'and-cuda'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: absl-py>=1.0.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (1.63.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (18.1.1)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (4.25.3)\n",
            "Requirement already satisfied: setuptools in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (69.5.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorflow[and-cuda]) (0.34.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from gdown) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (2.29.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (3.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from requests[socks]->gdown) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (2.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (3.18.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (3.2.2)\n",
            "Downloading tensorflow_addons-0.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.0/612.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow[and-cuda] tensorflow-addons keras gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILR88UnP1xSe",
        "outputId": "993b31d0-f09b-4a9a-cb43-06f274aaf4c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-16 09:38:11.289092: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-05-16 09:38:11.290293: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-05-16 09:38:11.316196: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-05-16 09:38:11.317017: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-16 09:38:11.800616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/home/io/miniconda3/envs/diip/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "N9xhF3VZM1zO",
        "outputId": "6ef93e17-a7ce-42e8-cd94-05949716dedd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n7sG3QHw0JAtJYYpjUU4RhghLqAV1ry4\n",
            "To: /mnt/d/FILES/Code/Teaching/diiP2024_SL_Solutions/data/pretrained_vit.h5\n",
            "100%|██████████| 87.3M/87.3M [00:02<00:00, 32.1MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'./data/pretrained_vit.h5'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ROOT = \"./data\"\n",
        "PRETRAINED_MODEL_PATH = ROOT + '/pretrained_vit.h5'\n",
        "MODEL_PATH = ROOT + '/vit.keras'\n",
        "\n",
        "gdown.download(\n",
        "    'https://drive.google.com/uc?id=1n7sG3QHw0JAtJYYpjUU4RhghLqAV1ry4',\n",
        "    PRETRAINED_MODEL_PATH,\n",
        "    quiet=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NGb21S-1xSf"
      },
      "source": [
        "#### Load CIFAR100 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ofc1U3j1xSf",
        "outputId": "cd808621-4347-408d-fa76-becb85726f2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 9s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFqdU-JN1xSg"
      },
      "source": [
        "#### Model configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mD9yehK91xSg"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_A4UTly1xSg"
      },
      "source": [
        "#### Prepare data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gu1uykAC1xSh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-16 09:41:51.098155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-05-16 09:41:51.118994: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD3rouwN1xSh"
      },
      "source": [
        "#### Implement multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2ptVRcLm1xSh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHNfySPa1xSi"
      },
      "source": [
        "#### Implement patch creation layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Wc-oCzD81xSi"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vevZy8vl1xSi"
      },
      "source": [
        "#### Visualize image patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "3sbANnYk1xSi",
        "outputId": "14db1a88-f3a4-4101-b3ae-12a1378d5c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image size: 72 X 72\n",
            "Patch size: 6 X 6\n",
            "Patches per image: 144\n",
            "Elements per patch: 108\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFICAYAAAAyFGczAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWn0lEQVR4nO3dSY8kd1rH8SciMjIrq7LWrqruqt7bvXjDHmZsyQIsw2iEBiEBYhmODEIj8Q54C8MFIcSBKyAuc0DihBAakDBisMGNPe4ZG9u9Vi/V1VVdW2ZGLrFwMDf06PlZanmQ5vs5P3oyMzLzl3H4P/kkTdM0BgD4P9Kf9BMAgP+vCEgAcBCQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4WmrhH/3530h1KxtrYc31f9R63b/x72FNM9UGgYr+VKobj8dhzdxSLvWqqkSqOzwYhjUT8XWmrfgtPXv2tNTr/PmzUl2ex7+z1XQg9Upr7X1q5fF7kGYzUq8kE3qlz/Zeoq7LsKap4s+imVlTanWl8BHK2gtSr9mFE2HNONE+/7sHh1Ldk89uhjXNtC/1+tcffCTVcQcJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA55kmZjtCPVNbcfhzUvrS5JvV7+ubfCmixpS72mk1qqK6u4Ls21qZYm0S7v0XE8CXE4iKdtzMwGk3hiZaaj/S4m9UiqK4fx9EvW0qaP0nxOqmta8fuuXn+zLKyotY+PmTg9Uiv3JuL0TtKKn7+ZWauaxEXDPanXYPg0rMlmtKmc9Y72nl989dX4MXPtWqi4gwQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoBDPijeK4+kuqaOD1HP9xalXmm2EteIh4GTVKzL4oOmo7qSeg0K7aB1k+zHj1lqvYbT+NB2IRzsNjNr59qh5zzvhDVZK64xM6szra5JhN92pcbMGlNep3YtVIk0a6A9ZiN+tqXvSqqtqWjK+NB5MzyWetWD+PNvZjaYXQprZhaXpV4q7iABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwCFP0kym8VoAM20qITft7/eTNJ5qqcQBh6lw8t/MrN+PX+fucbzWwMxsdy9eP2FmdnwU/319WWnXX7hklrW1aQnLulJZrTxoov0VfiN+JFOLR1GaRluNoWxJaITH+7xQ+0BmUj/tmlXiaoamiesS06bEUmHNQyK+59VUmxKbDOJpvkKc3lFxBwkADgISABwEJAA4CEgAcBCQAOAgIAHAQUACgIOABACHfFBc+1t6k07diud3bSKcWR1OtQPgT4+1lRG7u7thzf7BjtSrGGsHYJU1D52Odrg7y+JD+EqNmVmaanWN8J5X4nueiB8O5RB4rX5khc+2unBBrVMObUsn2M0sEQ+nmzCgIR+IF4YDmkQcSFAGDcwsy+K4akptnYiKO0gAcBCQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcMiTNIkYpaU0/aJNmBwMirBm53BP6vX0QKsbD+N1Cq20lHrleUeqS1ptoUioMTNrlGkJbXLBxL/yV4Yv5PUHda09pDBlkogfWumZqZNAWpnVyvMXe1mjXTOlYSOuSdCmj7TnlYovtMqEKb1UjjQJd5AA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHAQkADgISABwEJAA4NAnaYRT7GZm/eE4rLn38LHU687W3bDmoL8v9SoG2k6aXJj4yNvqhIy400X4nUrECQcTpmR6iwtSp9WNDakuUXanqLtmxPkRZUpGHH7RJ1aeKeHZJep+nmf4kOr1l6axnt17aWaWZnFdpez6+QK4gwQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoBDPig+nGprBoYT4aD4g/tSr89ufRbWZLmW8ZNiKNUVh8dClXqwVTvcXUsHeJ/d+oNT585JrVZObkp1SUt5nerOAu1wsdItFU9QK1dWPUAtr5YQDoE36iqFZ0m8/tr+BrWT9tlWrpn8PRFxBwkADgISABwEJAA4CEgAcBCQAOAgIAHAQUACgIOABAAHAQkADnmS5gfvXZfqmjo+YT8eDaRe7VbcK+/MaL2yrlR3vFeENWU5kXplqTYJIc0uyBMawioC4T0yM6vEupawDiIT3kszfZBDWfOQqKMcTRWXqFMtjfgCaqFf8hOYpNHHX+IScZWCvDKiiqf5EuG9/CK4gwQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQAhzxJ89HNe1LdqB/vdKmnI6nXtIpPxS9k2iRNJkx7mGm7R9Isl3qVpbbHJ0vjfqtXnpN6rV04H9ZcvHRV6nXmkvaY1u6EJVWqfdRGjXbNRkU8jVUN+1KvrIg/s8nxntSrGcaTWGZmqfA5U6aF/rdQq1NGVuSdNMrjaWW1WKhNnD3D52/cQQKAi4AEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcAhHxRfWt2U6h4Xd8Oa/tGh1CvN46c3KbW/WG+pf5lvcb+OcDDazGxQaY+5sHY6rLn4+ptSr9n1k2HNpN2Wej0aTKW6yWF88H9/ovU6GA+luk4rPlycTcXVGONxWLPSmpV6rS72pDpTnpt8+6Idjm6EA9nilhBLhLPdjbhLoVHWT5hZXcf91F4q7iABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwCFP0silaTyl0STayoI0ifO7FKdVpuLETdKKH3Omra1vKCbaNeudiidpjo61qYSD4X5Y08xpv4vdVKurjuKVBcfH8fMyM9s/0OoubsYTQzNz2sRQX7hPOJ5fkXqtzM5JdWbxxFCTap8zcZBG2rhg4iRKqky1iJM0tfiYVRW/0MqYpAGALwUBCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAQz4ovv3gkVQ37McHYJNEOwDbCIdRy6l2ADzLtMfsdOO/1s/EA9QLS9qh4dmTG2FNZdrh+nocrzaYmZuReiXCQX0zs35ZhjWdJW0VwcWlRanuxIwwkDDpS73aM/G1befqZ1b7PKoHshXiOXHp4HYlriYphfUNJtWY1eLhbqWq5KA4AHw5CEgAcBCQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA45Emag91drbCOpyqyljZtoAwlpJk2YdLKtZdaTIr4MVvaJMqVKy9LdaOFpbCmP4yvq5lZN43rlttdqdfOcKDVTeNrlk2097wXDwKZmdni2lpYM51o0y91pVzbI6lX0jsj1WVZJ6xJK/FiJOrOBaGkeXaTQJU4VVSZVldOJ3GNMNX1RXAHCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOeZImFU/YT4Xj+qm4k6Y3F098bJzelHptbmh1c7PxY54+Fe+QMTM7c/GyVNcsnQhr6kZ7q+aFXS2Li9p+mI+2t6W6T/b3wpqtR1qvRx/flOqeFvGUT9Jov//DYTwJZMfafptJe16rE6ZCZmfj99LMLMu0z8bsTDwBNtfSJtMWukKv2Xi/k5lZKexRMjPb3Xkc1hSjfamXijtIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAcBCQAOOSD4nMdrfTSC1fCml96802p1/MvvRDWnD1/Xuq1tBIfxjYzW1paDmtmO9rKhaylXbNWKz44n+XaAd40jR8zqYX/3jezN8XD0Y+O4nUEd55oKzvu3bwt1d3/6NOw5tatO1Kv8Ux8zfKutqbi6PBAqrMkfg+6i9pBa+3dNBv3j8OaotQObT8U6kaT+DC8mVlRDKW69cWlsEYONBF3kADgICABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHAQkADgkA+eV20tS7/1B78X1nzzG78s9VrtxVMte7s7Uq/Dg6dS3amr14SqL/93pRCnEm7e+iis2b57X+q1uxuvUjAz29uPJ2ke7GnXfzAZSXXTYhzWNNrAkxVpvE5kr9CuRVFoz39YlWHN3eKJ1Gva1FrdJJ5+KSfaapVRGT//aa31akx7/q9kSViz3tbWiai4gwQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQAhzxJs3D2lFT3zsN4suX7f/GXUq9OFm/bSMbxRIWZ2UIa730xMzvz9jthzerqqtRrcbaj1S0thTX3nxxIva5/8OO410Ntkma/0HbSjKbxVMWhuOukX2t1ipE4oTFp4omPSpxWSUtxqmUUT0ZVufb1rDPtPieeQzHLMu35J2n83Uwr5RHNslr7btbC9p3xVN3Qo+EOEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA75oPjLLyqrCMwWuu2w5v7tO1KvcSWsGVAP8FbaAdL3b94La2Z6s1KvpB5Kda25ubBmZxAfxjYze/BoN6wZl9rh+irVrq0JB5onLfEwcK7VWSX8nX8lHqBu4gPNjXgtctPepySJn1ve0r6eiXhQPBNeZzLSPrOVsBqj1c6lXmmqHiiPv8ON+D1XcQcJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA55kmZ8dCzVradx5j7Y+lTqdXQQn+qveyekXr3lJalufq4bP+ZQm5Y4sbwg1V3+2TfCmg/vxRM+Zmb3i6OwppwIUyhmtjgQJpnMbDqMVzO05rXpoyIRP5LCT3taa6+zbsVTMonwd/9mZrm4sqAW1olkbW3CJE2059bqF2FNfVdbx1EXca+Ny89JvVbPnJXqklH8po+G8YTPF8EdJAA4CEgAcBCQAOAgIAHAQUACgIOABAAHAQkADgISABzyQfGnY+3Q6o8exn/n/+RYO2hdDOOD4sOJlvFzk6lUt5/Gh25nJ4dSr5lhT6qbfe31sObl9Xmp17uPZsKa7d0DqVfxwYdS3eHek7Bm+doVqdfCC1elur5wCLwrriIwYR1EO9FWQbRq7XM2HMbPvxprB8CrqXY4On1yENbMCWsZzMzOntsMa9742qtSL8u0IYKtx/EQxH450B5TxB0kADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHAQkADjkSZqr5+JVBGZmxXQnrLnw2itSr2wUTyUMt+9KvZq2dlr/MF8La7LBU6nXsKNNQny8FU+ibD26I/XKpvE1a+9pk0B1X1uzcWoxnvI5uKOtjJhbXpbq5k/EqzZm0rbUazyOp7/qcbxiwMys7Me9zMxO5PFzO728LvV6dOuWVLe19TCsufTiV6Ver7x6Iaz57V97S+rV39emX4oynvKpxNUYKu4gAcBBQAKAg4AEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcAhT9KcXL4t1Q0G8VTI89d+Xup18cT5sObTf9KeV299Sao794u/E9b0+9rekdm29vtTC1M+f/2321KvRFhPUu9p0x7zS6ekurn5eCpkU7wW2cqiVPewiPcVHW3vSr2Wup2wZm15ReqVC73MzNZXlsKarzx/Ser1wYH2Ou8cxXV7u9qUWJJdC2tOb2qfn9kz2sRTnsWTNJmcaBruIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcBCQAODQj1Vq52RtfTUPax4//kzq9cPv/1tYc3hT67V++kCqa5bfDmtOnzunPebyqlTX7ca/U7//G1+Xev3pX30vrLl32Jd6XX7uqlQ3Nxcf9H3honZo+Cu/8IZU9/a774U1f3/j76Re07oKa8ZD7XD9aKINEex9+mlY89E770q9ikJbB6HcD+3tPJA6Xb9+I6z54z/RrsXSsjYcsHYiDqHFhTmp17e/o62W4A4SABwEJAA4CEgAcBCQAOAgIAHAQUACgIOABAAHAQkADgISABzyJE03nZHqerO9sKa2ZanX996Lp1ru3dRWEaTvP5TqvrYTTyV867d+U+r18Y1PpLrdx4/DmtsP4lUWZmY/fu8/w5psXEq9urPaVML8Qvx+3vihdi3++8bHUt2Drfj9rPaOpV7DMr4eo8FE6pWkmVSX1cK9SaatIqjy+DtnZtZdjL/DrVYj9dp9chjW/MO/xNM2ZmatWW1NRacbfx7ne12p17e/84dSHXeQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcBCQAOCQJ2lef/FXpLrN9RfDmlMrl6Vek8O1sOa73/0zqddgoO1hOXX6bFiztH5S6rX9IJ6QMTO7/mG8V+f9D7SphP5oGNZkbW1C4/bWrlTX3ot3ujy5eVPqVQ6PpLrGkrCmTrSPdzoTT5hknVmpl+XaVEjWiic+Wh1tkqlpaa+zauJr1m5pk0BpK763Wj0xL/W6cuW8VHftufi7efnChtRLxR0kADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHPJB8dde/XWprp3Gh0P7x9rf169tnAlrTp3RDpk+fKgd2v7nt+OVBe/8x39JvazSfn+Gg3FYU1fiAV6L66bD+DC5mdnxzj2prs7idRDjafwazcw6Pe1wcdKKD7vXmXa4O23Hderh+kZcuZBmeVjTamuHztM0PgBuZlbV8WqJJNdWq8z34rpvfv2rUq/f/dW3pLqTK/HB+TzTVkaouIMEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAEfSNI109Hw02JMaNlUd1jx9ui/1+vCTO2HNj350S+r1ySda3dbWVliz/Xhb6rX39ECqGw7jyaJqEk9BmJkl9SjuVWqTTE2q/X525lfimt4JqVc+I642ECZR6kScMGnia5vn2oTMwnxPqjtzajWsef7yBanX/Jw2cTPsx9+7hTnt+r907VJY8/LlTanXxrL2mInF2VJrcWa9zVekOu4gAcBBQAKAg4AEAAcBCQAOAhIAHAQkADgISABwEJAA4JAPio+LXa1jfJbTskQ7dFun8UaIstIOA48K7S//Dw6Pwpr72ztSr5u370p1d+4+CGu2HmmPuf20H9YcHRVSr7IW3kwT//K/EX+L0/gAuJnZTCeuW57X1iRsnlwKa56/elHqdfnSOanuwpn1sGa+q12LptI+22lShTVt0w5ad1rxd1hdflBV8fMyM2uSuGOjxYGtbP6MVMcdJAA4CEgAcBCQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA45Ema4bG2ZqCp48zNxFxW/mI91YZyTPz3fUuENQPS5IiZWRJPApmZTcr4dQ6KodRLmQR6sqdN0hTFVKrrF/H0zpODA6lXLY5CbK7Fax7ObqxJvTbW4nUQJxbnpF4mTKuYmU1Hg7Dm6Dh+L83MWsJUi5lZIkyiJOJnVpmMasRJLPHbZJbGqzHqWltNsnjyJe0hpSoA+ClEQAKAg4AEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcCh76QZaDtRpP0S4rKKTDhin6hTLdrLtLqJT/+rEwJppk04ZFk8vaBOAlkiPLdaa5aIUy3KNSuFms97iR8OpZ/aS7lk4ntujTZJM5nGU0plqU0yqbtf6jp+bnnekXrlebwvR4wWeTItE+oa8fr3Vq9KddxBAoCDgAQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwCH+v7pZnqt/665UaQdItVZaL/UwbZrErzMRDnabmaXC+gYzs0S4aJVwyPfzuvhAcyYf1Feff9ywKcdSr1I4QG1mNhXqEvH3P03j91N5PDOzstTep8biutFIW40xGo20xxS+K91uV+o1P98LazqdGamXni3K+/ls7/m4gwQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQAh7xyAQB+2nAHCQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQCO/wEQ2p7pkd/PiAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 144 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HetptiXF1xSj"
      },
      "source": [
        "#### Implement the patch encoding layer\n",
        "\n",
        "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
        "vector of size `projection_dim`. In addition, it adds a learnable position\n",
        "embedding to the projected vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kHrWwi6W1xSj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h1Babts1xSj"
      },
      "source": [
        "#### Build the ViT model\n",
        "\n",
        "The ViT model consists of multiple Transformer blocks,\n",
        "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
        "applied to the sequence of patches. The Transformer blocks produce a\n",
        "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
        "classifier head with softmax to produce the final class probabilities output.\n",
        "\n",
        "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
        "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
        "as the image representation, all the outputs of the final Transformer block are\n",
        "reshaped with `layers.Flatten()` and used as the image\n",
        "representation input to the classifier head.\n",
        "Note that the `layers.GlobalAveragePooling1D` layer\n",
        "could also be used instead to aggregate the outputs of the Transformer block,\n",
        "especially when the number of patches and the projection dimensions are large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e4mXDVeo1xSk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vWG9AXi1xSk"
      },
      "source": [
        "## Compile and evaluate the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "NGTCdTxt1xSk",
        "outputId": "527fffcd-5984-4044-ffdd-da73af8ae410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 31s 96ms/step - loss: 1.8552 - accuracy: 0.5527 - top-5-accuracy: 0.8202\n",
            "Test accuracy: 55.27%\n",
            "Test top 5 accuracy: 82.02%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def run_experiment(model, pretrained):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = PRETRAINED_MODEL_PATH if pretrained else MODEL_PATH\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    if not pretrained:\n",
        "        history = model.fit(\n",
        "            x=x_train,\n",
        "            y=y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=num_epochs,\n",
        "            validation_split=0.1,\n",
        "            callbacks=[checkpoint_callback],\n",
        "        )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "run_experiment(vit_classifier, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train a new model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "  6/176 [>.............................] - ETA: 6:55 - loss: 0.7821 - accuracy: 0.7591 - top-5-accuracy: 0.9635"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvit_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[15], line 24\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(model, pretrained)\u001b[0m\n\u001b[1;32m     16\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     17\u001b[0m     checkpoint_filepath,\n\u001b[1;32m     18\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pretrained:\n\u001b[0;32m---> 24\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(checkpoint_filepath)\n\u001b[1;32m     34\u001b[0m _, accuracy, top_5_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n",
            "File \u001b[0;32m~/miniconda3/envs/diip/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniconda3/envs/diip/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/miniconda3/envs/diip/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniconda3/envs/diip/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/miniconda3/envs/diip/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/miniconda3/envs/diip/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/diip/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/miniconda3/envs/diip/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
            "File \u001b[0;32m~/miniconda3/envs/diip/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
            "File \u001b[0;32m~/miniconda3/envs/diip/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "run_experiment(vit_classifier, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAGKlpuf1xSk"
      },
      "source": [
        "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
        "82% top-5 accuracy on the test data."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image_classification_with_vision_transformer",
      "provenance": []
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
