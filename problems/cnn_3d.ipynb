{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3D Convonlutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use this to install all dependencies on the local machine.\n",
        "# This is not needed for running on Google Colab.\n",
        "\n",
        "# %pip install tensorflow[and-cuda] keras gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_89kaZLcM1jI"
      },
      "outputs": [],
      "source": [
        "import tensorflow # This is the ML library we will use\n",
        "from tensorflow.keras.models import Sequential # This is the Sequential layer that runs all the sublayers inside\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv3D, MaxPooling3D # These are the layer we will use in the model\n",
        "from tensorflow.keras.utils import to_categorical # This is used to transform labels to categorical format\n",
        "import h5py # This is used to work with h5 files\n",
        "import numpy as np # This is used to handle arrays of data\n",
        "import matplotlib.pyplot as plt # This is to load plotting functions\n",
        "import gdown # This is used to download data from Google Drive\n",
        "import os # This is used to work with file system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare paths of the data and models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROOT = \"./data\"\n",
        "PRETRAINED_MODEL_PATH = ROOT + '/pretrained_3d_cnn.h5'\n",
        "MODEL_PATH = ROOT + '/3d_cnn.keras'\n",
        "DATSET_PATH = ROOT + '/full_dataset_vectors.h5'\n",
        "\n",
        "os.makedirs(ROOT, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download the 3D MNIST dataset and the pretrained model files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmr6gAQRM1jF"
      },
      "outputs": [],
      "source": [
        "gdown.download(\n",
        "    'https://drive.google.com/uc?id=1-jDaDuk5ePsRtS1TU85zJ2euBUPc05mU',\n",
        "    DATSET_PATH,\n",
        "    quiet=False\n",
        ")\n",
        "gdown.download(\n",
        "    'https://drive.google.com/uc?id=1EG5g_xYCH7gtV6Zdrt1F_h6AQRQHzyGd',\n",
        "    PRETRAINED_MODEL_PATH,\n",
        "    quiet=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnRIHz16M1jI"
      },
      "outputs": [],
      "source": [
        "batch_size = 100 # Number of images to process at once \n",
        "no_epochs = 10 # Numbe of epochs to train the model\n",
        "learning_rate = 0.001 # Rate of change of model parameters each training step\n",
        "no_classes = 10 # Number of classes in the dataset\n",
        "validation_split = 0.2 # Fraction of data to be used for validation \n",
        "verbosity = 1 # How verbose the training should be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhtHJC9WM1jJ"
      },
      "outputs": [],
      "source": [
        "# Convert 1D vector into 3D values, provided by the 3D MNIST authors at\n",
        "# https://www.kaggle.com/daavoo/3d-mnist\n",
        "def array_to_color(array, cmap=\"Oranges\"):\n",
        "  s_m = plt.cm.ScalarMappable(cmap=cmap)\n",
        "  return s_m.to_rgba(array)[:,:-1]\n",
        "\n",
        "# Reshape data into format that can be handled by Conv3D layers.\n",
        "# Courtesy of Sam Berglin; Zheming Lian; Jiahui Jang - University of Wisconsin-Madison\n",
        "# Report - https://github.com/sberglin/Projects-and-Papers/blob/master/3D%20CNN/Report.pdf\n",
        "# Code - https://github.com/sberglin/Projects-and-Papers/blob/master/3D%20CNN/network_final_version.ipynb\n",
        "def rgb_data_transform(data):\n",
        "  data_t = []\n",
        "  for i in range(data.shape[0]):\n",
        "    data_t.append(array_to_color(data[i]).reshape(16, 16, 16, 3))\n",
        "  return np.asarray(data_t, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the dataset from the downloaded data file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoPbMAXCM1jJ"
      },
      "outputs": [],
      "source": [
        "# Load the HDF5 data file\n",
        "with h5py.File(DATSET_PATH, \"r\") as hf:\n",
        "\n",
        "    # Split the data into training/test features/targets\n",
        "    X_train = hf[\"X_train\"][:]\n",
        "    targets_train = hf[\"y_train\"][:]\n",
        "    X_test = hf[\"X_test\"][:]\n",
        "    targets_test = hf[\"y_test\"][:]\n",
        "\n",
        "    # Determine sample shape\n",
        "    sample_shape = (16, 16, 16, 3)\n",
        "\n",
        "    # Reshape data into 3D format\n",
        "    X_train = rgb_data_transform(X_train)\n",
        "    X_test = rgb_data_transform(X_test)\n",
        "\n",
        "    # Convert target vectors to categorical targets\n",
        "    targets_train = to_categorical(targets_train).astype(np.int64)\n",
        "    targets_test = to_categorical(targets_test).astype(np.int64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create the model\n",
        "\n",
        "Use model.add function to add the following layers:\n",
        "\n",
        "Conv3D layer with 32 output channels, kernel size of 3x3x3, relu activation function, he_uniform function for kernel initializer, input_shape=sample_shape\n",
        "\n",
        "MaxPooling3D layer with 2x2x2 size\n",
        "\n",
        "Conv3D layer with 64 output channels, kernel size of 3x3x3, relu activation function, he_uniform function for kernel initializer\n",
        "\n",
        "MaxPooling3D layer with 2x2x2 size\n",
        "\n",
        "Flatten layer\n",
        "\n",
        "Dense layer with 256 output dimensions, relu activation function, he_uniform function for kernel initializer\n",
        "\n",
        "Dense layer with `no_classes` output dimensions and softmax activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "    <code>\n",
        "    model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape))<br>\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))<br>\n",
        "    model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))<br>\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))<br>\n",
        "    model.add(Flatten())<br>\n",
        "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))<br>\n",
        "    model.add(Dense(no_classes, activation='softmax'))<br>\n",
        "    </code>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1OYmwlaM1jK"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model = Sequential()\n",
        "...\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2vHZCCkM1jK"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit model to data \n",
        "history = model.fit(X_train, targets_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=no_epochs,\n",
        "            verbose=verbosity,\n",
        "            validation_split=validation_split)\n",
        "model.save(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the pretrained model weights into a separate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretrained_model = tensorflow.keras.models.load_model(PRETRAINED_MODEL_PATH)\n",
        "pretrained_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate generalization metrics\n",
        "score_pretrained = pretrained_model.evaluate(X_test, targets_test, verbose=0)\n",
        "score = model.evaluate(X_test, targets_test, verbose=0)\n",
        "print(f'Pretrained model. Test loss: {score_pretrained[0]} / Test accuracy: {score_pretrained[1]}')\n",
        "print(f'Trained model. Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMscxtHoM1jL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot history: Categorical crossentropy & Accuracy\n",
        "plt.plot(history.history['loss'], label='Categorical crossentropy (training data)')\n",
        "plt.plot(history.history['val_loss'], label='Categorical crossentropy (validation data)')\n",
        "plt.plot(history.history['accuracy'], label='Accuracy (training data)')\n",
        "plt.plot(history.history['val_accuracy'], label='Accuracy (validation data)')\n",
        "plt.title('Model performance for 3D MNIST Keras Conv3D example')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
