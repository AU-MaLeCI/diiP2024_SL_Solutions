{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D53KDb2hVU_X"
      },
      "source": [
        "# Training a CNN model on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (3.7.5)\n",
            "Requirement already satisfied: seaborn in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (0.13.2)\n",
            "Requirement already satisfied: torch in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (2.3.0)\n",
            "Requirement already satisfied: torchvision in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (2.3.0)\n",
            "Requirement already satisfied: numpy in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (1.24.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (2.9.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (6.4.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: filelock in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib seaborn torch torchvision torchaudio numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0z7zspfLVU_Y"
      },
      "outputs": [],
      "source": [
        "# Dependencies\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set() # this import just makes the plots prettier\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgaQ7FaCVU_g",
        "outputId": "2b0f9f23-03a1-47cf-f079-431bef89dd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n",
        "Collect the CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_set = datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "test_set = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = (\n",
        "    \"plane\",\n",
        "    \"car\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"deer\",\n",
        "    \"dog\",\n",
        "    \"frog\",\n",
        "    \"horse\",\n",
        "    \"ship\",\n",
        "    \"truck\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show example images from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bird  dog   dog   deer \n"
          ]
        }
      ],
      "source": [
        "\n",
        "def imshow(img):\n",
        "    # Unnormalize\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# Print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Cifar10Classifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.body = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=6,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2,\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=6,\n",
        "                out_channels=16,\n",
        "                kernel_size=5,\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(16 * 6 * 6, 120),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(120, 84),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(84, 10),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.body(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Cifar10Classifier(\n",
              "  (body): Sequential(\n",
              "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Flatten(start_dim=1, end_dim=-1)\n",
              "    (7): Linear(in_features=576, out_features=120, bias=True)\n",
              "    (8): ReLU()\n",
              "    (9): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (10): ReLU()\n",
              "    (11): Linear(in_features=84, out_features=10, bias=True)\n",
              "    (12): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Cifar10Classifier()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a loss function and an optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/io/miniconda3/envs/diip/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/home/io/miniconda3/envs/diip/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/12500], Loss: 2.3244\n",
            "Epoch [1/2], Step [200/12500], Loss: 2.4876\n",
            "Epoch [1/2], Step [300/12500], Loss: 2.1205\n",
            "Epoch [1/2], Step [400/12500], Loss: 1.6693\n",
            "Epoch [1/2], Step [500/12500], Loss: 2.2079\n",
            "Epoch [1/2], Step [600/12500], Loss: 2.2760\n",
            "Epoch [1/2], Step [700/12500], Loss: 1.5788\n",
            "Epoch [1/2], Step [800/12500], Loss: 2.3035\n",
            "Epoch [1/2], Step [900/12500], Loss: 2.0136\n",
            "Epoch [1/2], Step [1000/12500], Loss: 1.6311\n",
            "Epoch [1/2], Step [1100/12500], Loss: 2.1624\n",
            "Epoch [1/2], Step [1200/12500], Loss: 2.1017\n",
            "Epoch [1/2], Step [1300/12500], Loss: 2.0545\n",
            "Epoch [1/2], Step [1400/12500], Loss: 1.7558\n",
            "Epoch [1/2], Step [1500/12500], Loss: 2.0219\n",
            "Epoch [1/2], Step [1600/12500], Loss: 2.0952\n",
            "Epoch [1/2], Step [1700/12500], Loss: 2.4017\n",
            "Epoch [1/2], Step [1800/12500], Loss: 2.0827\n",
            "Epoch [1/2], Step [1900/12500], Loss: 2.3183\n",
            "Epoch [1/2], Step [2000/12500], Loss: 2.3130\n",
            "Epoch [1/2], Step [2100/12500], Loss: 2.1494\n",
            "Epoch [1/2], Step [2200/12500], Loss: 2.1336\n",
            "Epoch [1/2], Step [2300/12500], Loss: 2.0488\n",
            "Epoch [1/2], Step [2400/12500], Loss: 1.3888\n",
            "Epoch [1/2], Step [2500/12500], Loss: 1.7129\n",
            "Epoch [1/2], Step [2600/12500], Loss: 2.2362\n",
            "Epoch [1/2], Step [2700/12500], Loss: 2.2715\n",
            "Epoch [1/2], Step [2800/12500], Loss: 2.0717\n",
            "Epoch [1/2], Step [2900/12500], Loss: 2.1471\n",
            "Epoch [1/2], Step [3000/12500], Loss: 1.4547\n",
            "Epoch [1/2], Step [3100/12500], Loss: 1.8601\n",
            "Epoch [1/2], Step [3200/12500], Loss: 1.6087\n",
            "Epoch [1/2], Step [3300/12500], Loss: 2.1287\n",
            "Epoch [1/2], Step [3400/12500], Loss: 2.3397\n",
            "Epoch [1/2], Step [3500/12500], Loss: 2.1862\n",
            "Epoch [1/2], Step [3600/12500], Loss: 1.7629\n",
            "Epoch [1/2], Step [3700/12500], Loss: 2.3055\n",
            "Epoch [1/2], Step [3800/12500], Loss: 2.0785\n",
            "Epoch [1/2], Step [3900/12500], Loss: 2.2751\n",
            "Epoch [1/2], Step [4000/12500], Loss: 2.1794\n",
            "Epoch [1/2], Step [4100/12500], Loss: 2.1552\n",
            "Epoch [1/2], Step [4200/12500], Loss: 2.1189\n",
            "Epoch [1/2], Step [4300/12500], Loss: 2.0419\n",
            "Epoch [1/2], Step [4400/12500], Loss: 2.4679\n",
            "Epoch [1/2], Step [4500/12500], Loss: 2.5127\n",
            "Epoch [1/2], Step [4600/12500], Loss: 1.6614\n",
            "Epoch [1/2], Step [4700/12500], Loss: 2.0896\n",
            "Epoch [1/2], Step [4800/12500], Loss: 2.1767\n",
            "Epoch [1/2], Step [4900/12500], Loss: 2.0099\n",
            "Epoch [1/2], Step [5000/12500], Loss: 1.9355\n",
            "Epoch [1/2], Step [5100/12500], Loss: 1.7539\n",
            "Epoch [1/2], Step [5200/12500], Loss: 2.3460\n",
            "Epoch [1/2], Step [5300/12500], Loss: 2.5235\n",
            "Epoch [1/2], Step [5400/12500], Loss: 2.2682\n",
            "Epoch [1/2], Step [5500/12500], Loss: 2.3503\n",
            "Epoch [1/2], Step [5600/12500], Loss: 2.4166\n",
            "Epoch [1/2], Step [5700/12500], Loss: 2.8285\n",
            "Epoch [1/2], Step [5800/12500], Loss: 2.4086\n",
            "Epoch [1/2], Step [5900/12500], Loss: 1.3558\n",
            "Epoch [1/2], Step [6000/12500], Loss: 2.2077\n",
            "Epoch [1/2], Step [6100/12500], Loss: 2.2435\n",
            "Epoch [1/2], Step [6200/12500], Loss: 1.8143\n",
            "Epoch [1/2], Step [6300/12500], Loss: 1.4198\n",
            "Epoch [1/2], Step [6400/12500], Loss: 1.6042\n",
            "Epoch [1/2], Step [6500/12500], Loss: 1.8983\n",
            "Epoch [1/2], Step [6600/12500], Loss: 1.8669\n",
            "Epoch [1/2], Step [6700/12500], Loss: 1.4824\n",
            "Epoch [1/2], Step [6800/12500], Loss: 2.1921\n",
            "Epoch [1/2], Step [6900/12500], Loss: 1.7351\n",
            "Epoch [1/2], Step [7000/12500], Loss: 0.8921\n",
            "Epoch [1/2], Step [7100/12500], Loss: 2.2881\n",
            "Epoch [1/2], Step [7200/12500], Loss: 1.2147\n",
            "Epoch [1/2], Step [7300/12500], Loss: 2.1457\n",
            "Epoch [1/2], Step [7400/12500], Loss: 1.5868\n",
            "Epoch [1/2], Step [7500/12500], Loss: 1.9729\n",
            "Epoch [1/2], Step [7600/12500], Loss: 1.9483\n",
            "Epoch [1/2], Step [7700/12500], Loss: 1.5714\n",
            "Epoch [1/2], Step [7800/12500], Loss: 1.8301\n",
            "Epoch [1/2], Step [7900/12500], Loss: 1.7543\n",
            "Epoch [1/2], Step [8000/12500], Loss: 2.0150\n",
            "Epoch [1/2], Step [8100/12500], Loss: 2.2744\n",
            "Epoch [1/2], Step [8200/12500], Loss: 2.3105\n",
            "Epoch [1/2], Step [8300/12500], Loss: 1.4769\n",
            "Epoch [1/2], Step [8400/12500], Loss: 2.2233\n",
            "Epoch [1/2], Step [8500/12500], Loss: 2.2124\n",
            "Epoch [1/2], Step [8600/12500], Loss: 2.2726\n",
            "Epoch [1/2], Step [8700/12500], Loss: 1.7520\n",
            "Epoch [1/2], Step [8800/12500], Loss: 1.9812\n",
            "Epoch [1/2], Step [8900/12500], Loss: 2.2637\n",
            "Epoch [1/2], Step [9000/12500], Loss: 2.0182\n",
            "Epoch [1/2], Step [9100/12500], Loss: 1.5811\n",
            "Epoch [1/2], Step [9200/12500], Loss: 3.3708\n",
            "Epoch [1/2], Step [9300/12500], Loss: 1.4858\n",
            "Epoch [1/2], Step [9400/12500], Loss: 1.5990\n",
            "Epoch [1/2], Step [9500/12500], Loss: 1.9736\n",
            "Epoch [1/2], Step [9600/12500], Loss: 2.2890\n",
            "Epoch [1/2], Step [9700/12500], Loss: 1.7589\n",
            "Epoch [1/2], Step [9800/12500], Loss: 2.0111\n",
            "Epoch [1/2], Step [9900/12500], Loss: 2.1237\n",
            "Epoch [1/2], Step [10000/12500], Loss: 2.1042\n",
            "Epoch [1/2], Step [10100/12500], Loss: 1.9068\n",
            "Epoch [1/2], Step [10200/12500], Loss: 2.0304\n",
            "Epoch [1/2], Step [10300/12500], Loss: 2.3996\n",
            "Epoch [1/2], Step [10400/12500], Loss: 1.9620\n",
            "Epoch [1/2], Step [10500/12500], Loss: 2.2119\n",
            "Epoch [1/2], Step [10600/12500], Loss: 1.9377\n",
            "Epoch [1/2], Step [10700/12500], Loss: 2.0303\n",
            "Epoch [1/2], Step [10800/12500], Loss: 1.9157\n",
            "Epoch [1/2], Step [10900/12500], Loss: 2.3708\n",
            "Epoch [1/2], Step [11000/12500], Loss: 2.0818\n",
            "Epoch [1/2], Step [11100/12500], Loss: 1.7833\n",
            "Epoch [1/2], Step [11200/12500], Loss: 2.0346\n",
            "Epoch [1/2], Step [11300/12500], Loss: 1.9539\n",
            "Epoch [1/2], Step [11400/12500], Loss: 2.5099\n",
            "Epoch [1/2], Step [11500/12500], Loss: 1.5456\n",
            "Epoch [1/2], Step [11600/12500], Loss: 2.0860\n",
            "Epoch [1/2], Step [11700/12500], Loss: 1.5582\n",
            "Epoch [1/2], Step [11800/12500], Loss: 2.1725\n",
            "Epoch [1/2], Step [11900/12500], Loss: 1.8495\n",
            "Epoch [1/2], Step [12000/12500], Loss: 1.6939\n",
            "Epoch [1/2], Step [12100/12500], Loss: 2.4759\n",
            "Epoch [1/2], Step [12200/12500], Loss: 1.8563\n",
            "Epoch [1/2], Step [12300/12500], Loss: 1.9758\n",
            "Epoch [1/2], Step [12400/12500], Loss: 1.9995\n",
            "Epoch [1/2], Step [12500/12500], Loss: 1.5473\n",
            "Epoch [2/2], Step [100/12500], Loss: 1.9378\n",
            "Epoch [2/2], Step [200/12500], Loss: 2.3841\n",
            "Epoch [2/2], Step [300/12500], Loss: 1.4145\n",
            "Epoch [2/2], Step [400/12500], Loss: 2.6813\n",
            "Epoch [2/2], Step [500/12500], Loss: 2.6312\n",
            "Epoch [2/2], Step [600/12500], Loss: 2.0644\n",
            "Epoch [2/2], Step [700/12500], Loss: 1.6716\n",
            "Epoch [2/2], Step [800/12500], Loss: 1.6931\n",
            "Epoch [2/2], Step [900/12500], Loss: 1.5619\n",
            "Epoch [2/2], Step [1000/12500], Loss: 2.0306\n",
            "Epoch [2/2], Step [1100/12500], Loss: 2.4173\n",
            "Epoch [2/2], Step [1200/12500], Loss: 2.0081\n",
            "Epoch [2/2], Step [1300/12500], Loss: 1.2880\n",
            "Epoch [2/2], Step [1400/12500], Loss: 2.1688\n",
            "Epoch [2/2], Step [1500/12500], Loss: 1.9384\n",
            "Epoch [2/2], Step [1600/12500], Loss: 1.9107\n",
            "Epoch [2/2], Step [1700/12500], Loss: 2.2996\n",
            "Epoch [2/2], Step [1800/12500], Loss: 1.8920\n",
            "Epoch [2/2], Step [1900/12500], Loss: 0.8561\n",
            "Epoch [2/2], Step [2000/12500], Loss: 1.0333\n",
            "Epoch [2/2], Step [2100/12500], Loss: 1.9538\n",
            "Epoch [2/2], Step [2200/12500], Loss: 1.7925\n",
            "Epoch [2/2], Step [2300/12500], Loss: 2.2756\n",
            "Epoch [2/2], Step [2400/12500], Loss: 2.0117\n",
            "Epoch [2/2], Step [2500/12500], Loss: 1.3341\n",
            "Epoch [2/2], Step [2600/12500], Loss: 1.9152\n",
            "Epoch [2/2], Step [2700/12500], Loss: 2.8947\n",
            "Epoch [2/2], Step [2800/12500], Loss: 1.0656\n",
            "Epoch [2/2], Step [2900/12500], Loss: 2.0221\n",
            "Epoch [2/2], Step [3000/12500], Loss: 1.3141\n",
            "Epoch [2/2], Step [3100/12500], Loss: 1.2530\n",
            "Epoch [2/2], Step [3200/12500], Loss: 2.3171\n",
            "Epoch [2/2], Step [3300/12500], Loss: 1.9648\n",
            "Epoch [2/2], Step [3400/12500], Loss: 1.5691\n",
            "Epoch [2/2], Step [3500/12500], Loss: 1.4977\n",
            "Epoch [2/2], Step [3600/12500], Loss: 2.3328\n",
            "Epoch [2/2], Step [3700/12500], Loss: 1.9449\n",
            "Epoch [2/2], Step [3800/12500], Loss: 2.4348\n",
            "Epoch [2/2], Step [3900/12500], Loss: 2.0547\n",
            "Epoch [2/2], Step [4000/12500], Loss: 2.0695\n",
            "Epoch [2/2], Step [4100/12500], Loss: 2.2517\n",
            "Epoch [2/2], Step [4200/12500], Loss: 1.6829\n",
            "Epoch [2/2], Step [4300/12500], Loss: 1.6141\n",
            "Epoch [2/2], Step [4400/12500], Loss: 2.5746\n",
            "Epoch [2/2], Step [4500/12500], Loss: 1.9828\n",
            "Epoch [2/2], Step [4600/12500], Loss: 1.6626\n",
            "Epoch [2/2], Step [4700/12500], Loss: 2.4002\n",
            "Epoch [2/2], Step [4800/12500], Loss: 1.7125\n",
            "Epoch [2/2], Step [4900/12500], Loss: 1.7377\n",
            "Epoch [2/2], Step [5000/12500], Loss: 2.3837\n",
            "Epoch [2/2], Step [5100/12500], Loss: 1.4361\n",
            "Epoch [2/2], Step [5200/12500], Loss: 2.7005\n",
            "Epoch [2/2], Step [5300/12500], Loss: 1.6054\n",
            "Epoch [2/2], Step [5400/12500], Loss: 1.7474\n",
            "Epoch [2/2], Step [5500/12500], Loss: 1.5655\n",
            "Epoch [2/2], Step [5600/12500], Loss: 1.4629\n",
            "Epoch [2/2], Step [5700/12500], Loss: 1.5672\n",
            "Epoch [2/2], Step [5800/12500], Loss: 1.7841\n",
            "Epoch [2/2], Step [5900/12500], Loss: 1.9690\n",
            "Epoch [2/2], Step [6000/12500], Loss: 1.6781\n",
            "Epoch [2/2], Step [6100/12500], Loss: 1.2162\n",
            "Epoch [2/2], Step [6200/12500], Loss: 2.6261\n",
            "Epoch [2/2], Step [6300/12500], Loss: 1.7219\n",
            "Epoch [2/2], Step [6400/12500], Loss: 2.2456\n",
            "Epoch [2/2], Step [6500/12500], Loss: 1.9232\n",
            "Epoch [2/2], Step [6600/12500], Loss: 1.4032\n",
            "Epoch [2/2], Step [6700/12500], Loss: 1.3544\n",
            "Epoch [2/2], Step [6800/12500], Loss: 1.6343\n",
            "Epoch [2/2], Step [6900/12500], Loss: 2.1322\n",
            "Epoch [2/2], Step [7000/12500], Loss: 2.3258\n",
            "Epoch [2/2], Step [7100/12500], Loss: 2.3428\n",
            "Epoch [2/2], Step [7200/12500], Loss: 1.4289\n",
            "Epoch [2/2], Step [7300/12500], Loss: 1.7964\n",
            "Epoch [2/2], Step [7400/12500], Loss: 1.9499\n",
            "Epoch [2/2], Step [7500/12500], Loss: 1.7137\n",
            "Epoch [2/2], Step [7600/12500], Loss: 2.2138\n",
            "Epoch [2/2], Step [7700/12500], Loss: 1.3714\n",
            "Epoch [2/2], Step [7800/12500], Loss: 1.9698\n",
            "Epoch [2/2], Step [7900/12500], Loss: 1.6667\n",
            "Epoch [2/2], Step [8000/12500], Loss: 0.9834\n",
            "Epoch [2/2], Step [8100/12500], Loss: 1.0137\n",
            "Epoch [2/2], Step [8200/12500], Loss: 1.6226\n",
            "Epoch [2/2], Step [8300/12500], Loss: 2.5904\n",
            "Epoch [2/2], Step [8400/12500], Loss: 2.2951\n",
            "Epoch [2/2], Step [8500/12500], Loss: 2.5032\n",
            "Epoch [2/2], Step [8600/12500], Loss: 2.2962\n",
            "Epoch [2/2], Step [8700/12500], Loss: 2.6873\n",
            "Epoch [2/2], Step [8800/12500], Loss: 1.3560\n",
            "Epoch [2/2], Step [8900/12500], Loss: 1.3778\n",
            "Epoch [2/2], Step [9000/12500], Loss: 2.4057\n",
            "Epoch [2/2], Step [9100/12500], Loss: 1.7119\n",
            "Epoch [2/2], Step [9200/12500], Loss: 1.5938\n",
            "Epoch [2/2], Step [9300/12500], Loss: 1.8804\n",
            "Epoch [2/2], Step [9400/12500], Loss: 1.8916\n",
            "Epoch [2/2], Step [9500/12500], Loss: 2.1544\n",
            "Epoch [2/2], Step [9600/12500], Loss: 1.5847\n",
            "Epoch [2/2], Step [9700/12500], Loss: 2.0194\n",
            "Epoch [2/2], Step [9800/12500], Loss: 2.0425\n",
            "Epoch [2/2], Step [9900/12500], Loss: 1.6489\n",
            "Epoch [2/2], Step [10000/12500], Loss: 2.5664\n",
            "Epoch [2/2], Step [10100/12500], Loss: 1.8805\n",
            "Epoch [2/2], Step [10200/12500], Loss: 1.8875\n",
            "Epoch [2/2], Step [10300/12500], Loss: 1.5681\n",
            "Epoch [2/2], Step [10400/12500], Loss: 1.4980\n",
            "Epoch [2/2], Step [10500/12500], Loss: 1.6391\n",
            "Epoch [2/2], Step [10600/12500], Loss: 1.7613\n",
            "Epoch [2/2], Step [10700/12500], Loss: 2.4848\n",
            "Epoch [2/2], Step [10800/12500], Loss: 1.8852\n",
            "Epoch [2/2], Step [10900/12500], Loss: 2.0743\n",
            "Epoch [2/2], Step [11000/12500], Loss: 2.3734\n",
            "Epoch [2/2], Step [11100/12500], Loss: 2.3024\n",
            "Epoch [2/2], Step [11200/12500], Loss: 1.8451\n",
            "Epoch [2/2], Step [11300/12500], Loss: 1.4076\n",
            "Epoch [2/2], Step [11400/12500], Loss: 2.0608\n",
            "Epoch [2/2], Step [11500/12500], Loss: 1.6480\n",
            "Epoch [2/2], Step [11600/12500], Loss: 2.4214\n",
            "Epoch [2/2], Step [11700/12500], Loss: 0.6949\n",
            "Epoch [2/2], Step [11800/12500], Loss: 1.6772\n",
            "Epoch [2/2], Step [11900/12500], Loss: 2.3684\n",
            "Epoch [2/2], Step [12000/12500], Loss: 1.9632\n",
            "Epoch [2/2], Step [12100/12500], Loss: 1.5464\n",
            "Epoch [2/2], Step [12200/12500], Loss: 1.5275\n",
            "Epoch [2/2], Step [12300/12500], Loss: 1.7536\n",
            "Epoch [2/2], Step [12400/12500], Loss: 1.8993\n",
            "Epoch [2/2], Step [12500/12500], Loss: 1.6553\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "def train(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Train the model\n",
        "    total_step = len(train_loader)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # For each batch in the training data\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute output and loss\n",
        "            output = model(images)\n",
        "            loss = loss_func(output, labels)\n",
        "\n",
        "            # Clear gradients for this training step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute gradients\n",
        "            loss.backward()\n",
        "            # Apply gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(\n",
        "                    \"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(\n",
        "                        epoch + 1, num_epochs, i + 1, total_step, loss.item()\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "train(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now test the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 36.89%\n"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            test_output = model(images)\n",
        "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
        "            correct += (pred_y == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        \n",
        "        accuracy = correct / float(total)\n",
        "        print('Test Accuracy of the model: %.2f%%' % (accuracy * 100))\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let us train more and see how the result changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/4], Step [100/12500], Loss: 2.4426\n",
            "Epoch [1/4], Step [200/12500], Loss: 1.6469\n",
            "Epoch [1/4], Step [300/12500], Loss: 1.9028\n",
            "Epoch [1/4], Step [400/12500], Loss: 0.8953\n",
            "Epoch [1/4], Step [500/12500], Loss: 1.6668\n",
            "Epoch [1/4], Step [600/12500], Loss: 1.5551\n",
            "Epoch [1/4], Step [700/12500], Loss: 0.7653\n",
            "Epoch [1/4], Step [800/12500], Loss: 2.8405\n",
            "Epoch [1/4], Step [900/12500], Loss: 2.4236\n",
            "Epoch [1/4], Step [1000/12500], Loss: 1.8627\n",
            "Epoch [1/4], Step [1100/12500], Loss: 2.0986\n",
            "Epoch [1/4], Step [1200/12500], Loss: 2.0645\n",
            "Epoch [1/4], Step [1300/12500], Loss: 1.2067\n",
            "Epoch [1/4], Step [1400/12500], Loss: 2.2337\n",
            "Epoch [1/4], Step [1500/12500], Loss: 1.8261\n",
            "Epoch [1/4], Step [1600/12500], Loss: 1.4659\n",
            "Epoch [1/4], Step [1700/12500], Loss: 1.7158\n",
            "Epoch [1/4], Step [1800/12500], Loss: 1.4601\n",
            "Epoch [1/4], Step [1900/12500], Loss: 1.7504\n",
            "Epoch [1/4], Step [2000/12500], Loss: 1.5606\n",
            "Epoch [1/4], Step [2100/12500], Loss: 1.9555\n",
            "Epoch [1/4], Step [2200/12500], Loss: 1.3925\n",
            "Epoch [1/4], Step [2300/12500], Loss: 0.9935\n",
            "Epoch [1/4], Step [2400/12500], Loss: 2.3459\n",
            "Epoch [1/4], Step [2500/12500], Loss: 1.8166\n",
            "Epoch [1/4], Step [2600/12500], Loss: 1.9677\n",
            "Epoch [1/4], Step [2700/12500], Loss: 1.8836\n",
            "Epoch [1/4], Step [2800/12500], Loss: 1.9750\n",
            "Epoch [1/4], Step [2900/12500], Loss: 1.7739\n",
            "Epoch [1/4], Step [3000/12500], Loss: 0.7325\n",
            "Epoch [1/4], Step [3100/12500], Loss: 1.5655\n",
            "Epoch [1/4], Step [3200/12500], Loss: 2.2551\n",
            "Epoch [1/4], Step [3300/12500], Loss: 1.6957\n",
            "Epoch [1/4], Step [3400/12500], Loss: 1.7429\n",
            "Epoch [1/4], Step [3500/12500], Loss: 1.0957\n",
            "Epoch [1/4], Step [3600/12500], Loss: 1.6360\n",
            "Epoch [1/4], Step [3700/12500], Loss: 1.5831\n",
            "Epoch [1/4], Step [3800/12500], Loss: 1.8510\n",
            "Epoch [1/4], Step [3900/12500], Loss: 2.2884\n",
            "Epoch [1/4], Step [4000/12500], Loss: 1.3943\n",
            "Epoch [1/4], Step [4100/12500], Loss: 0.4122\n",
            "Epoch [1/4], Step [4200/12500], Loss: 2.0889\n",
            "Epoch [1/4], Step [4300/12500], Loss: 2.0919\n",
            "Epoch [1/4], Step [4400/12500], Loss: 2.4316\n",
            "Epoch [1/4], Step [4500/12500], Loss: 1.2252\n",
            "Epoch [1/4], Step [4600/12500], Loss: 1.1802\n",
            "Epoch [1/4], Step [4700/12500], Loss: 1.5553\n",
            "Epoch [1/4], Step [4800/12500], Loss: 1.9155\n",
            "Epoch [1/4], Step [4900/12500], Loss: 2.2475\n",
            "Epoch [1/4], Step [5000/12500], Loss: 1.7929\n",
            "Epoch [1/4], Step [5100/12500], Loss: 2.1338\n",
            "Epoch [1/4], Step [5200/12500], Loss: 1.4522\n",
            "Epoch [1/4], Step [5300/12500], Loss: 2.2646\n",
            "Epoch [1/4], Step [5400/12500], Loss: 0.9865\n",
            "Epoch [1/4], Step [5500/12500], Loss: 1.9452\n",
            "Epoch [1/4], Step [5600/12500], Loss: 1.0281\n",
            "Epoch [1/4], Step [5700/12500], Loss: 1.2674\n",
            "Epoch [1/4], Step [5800/12500], Loss: 0.7287\n",
            "Epoch [1/4], Step [5900/12500], Loss: 2.2177\n",
            "Epoch [1/4], Step [6000/12500], Loss: 2.3697\n",
            "Epoch [1/4], Step [6100/12500], Loss: 2.3577\n",
            "Epoch [1/4], Step [6200/12500], Loss: 1.7284\n",
            "Epoch [1/4], Step [6300/12500], Loss: 1.3778\n",
            "Epoch [1/4], Step [6400/12500], Loss: 1.5714\n",
            "Epoch [1/4], Step [6500/12500], Loss: 1.7302\n",
            "Epoch [1/4], Step [6600/12500], Loss: 1.6642\n",
            "Epoch [1/4], Step [6700/12500], Loss: 1.2298\n",
            "Epoch [1/4], Step [6800/12500], Loss: 0.8651\n",
            "Epoch [1/4], Step [6900/12500], Loss: 1.9629\n",
            "Epoch [1/4], Step [7000/12500], Loss: 1.3150\n",
            "Epoch [1/4], Step [7100/12500], Loss: 0.8701\n",
            "Epoch [1/4], Step [7200/12500], Loss: 1.1484\n",
            "Epoch [1/4], Step [7300/12500], Loss: 1.4994\n",
            "Epoch [1/4], Step [7400/12500], Loss: 1.3483\n",
            "Epoch [1/4], Step [7500/12500], Loss: 0.6943\n",
            "Epoch [1/4], Step [7600/12500], Loss: 0.9854\n",
            "Epoch [1/4], Step [7700/12500], Loss: 2.4239\n",
            "Epoch [1/4], Step [7800/12500], Loss: 2.7675\n",
            "Epoch [1/4], Step [7900/12500], Loss: 1.0506\n",
            "Epoch [1/4], Step [8000/12500], Loss: 1.8979\n",
            "Epoch [1/4], Step [8100/12500], Loss: 1.9471\n",
            "Epoch [1/4], Step [8200/12500], Loss: 0.6635\n",
            "Epoch [1/4], Step [8300/12500], Loss: 1.7181\n",
            "Epoch [1/4], Step [8400/12500], Loss: 2.2623\n",
            "Epoch [1/4], Step [8500/12500], Loss: 2.4008\n",
            "Epoch [1/4], Step [8600/12500], Loss: 1.7994\n",
            "Epoch [1/4], Step [8700/12500], Loss: 2.1613\n",
            "Epoch [1/4], Step [8800/12500], Loss: 1.8689\n",
            "Epoch [1/4], Step [8900/12500], Loss: 2.0157\n",
            "Epoch [1/4], Step [9000/12500], Loss: 1.1053\n",
            "Epoch [1/4], Step [9100/12500], Loss: 1.6063\n",
            "Epoch [1/4], Step [9200/12500], Loss: 1.8778\n",
            "Epoch [1/4], Step [9300/12500], Loss: 1.4934\n",
            "Epoch [1/4], Step [9400/12500], Loss: 1.0117\n",
            "Epoch [1/4], Step [9500/12500], Loss: 0.9000\n",
            "Epoch [1/4], Step [9600/12500], Loss: 1.3540\n",
            "Epoch [1/4], Step [9700/12500], Loss: 0.9451\n",
            "Epoch [1/4], Step [9800/12500], Loss: 2.0188\n",
            "Epoch [1/4], Step [9900/12500], Loss: 3.3996\n",
            "Epoch [1/4], Step [10000/12500], Loss: 1.5124\n",
            "Epoch [1/4], Step [10100/12500], Loss: 1.1731\n",
            "Epoch [1/4], Step [10200/12500], Loss: 0.9078\n",
            "Epoch [1/4], Step [10300/12500], Loss: 1.4024\n",
            "Epoch [1/4], Step [10400/12500], Loss: 2.7373\n",
            "Epoch [1/4], Step [10500/12500], Loss: 2.3680\n",
            "Epoch [1/4], Step [10600/12500], Loss: 1.6614\n",
            "Epoch [1/4], Step [10700/12500], Loss: 0.6090\n",
            "Epoch [1/4], Step [10800/12500], Loss: 1.7808\n",
            "Epoch [1/4], Step [10900/12500], Loss: 3.3377\n",
            "Epoch [1/4], Step [11000/12500], Loss: 1.4122\n",
            "Epoch [1/4], Step [11100/12500], Loss: 1.7381\n",
            "Epoch [1/4], Step [11200/12500], Loss: 1.4568\n",
            "Epoch [1/4], Step [11300/12500], Loss: 1.3775\n",
            "Epoch [1/4], Step [11400/12500], Loss: 1.6558\n",
            "Epoch [1/4], Step [11500/12500], Loss: 0.8404\n",
            "Epoch [1/4], Step [11600/12500], Loss: 1.5485\n",
            "Epoch [1/4], Step [11700/12500], Loss: 1.3195\n",
            "Epoch [1/4], Step [11800/12500], Loss: 1.6417\n",
            "Epoch [1/4], Step [11900/12500], Loss: 1.3630\n",
            "Epoch [1/4], Step [12000/12500], Loss: 2.2912\n",
            "Epoch [1/4], Step [12100/12500], Loss: 2.6219\n",
            "Epoch [1/4], Step [12200/12500], Loss: 1.4125\n",
            "Epoch [1/4], Step [12300/12500], Loss: 1.9037\n",
            "Epoch [1/4], Step [12400/12500], Loss: 1.6782\n",
            "Epoch [1/4], Step [12500/12500], Loss: 2.8112\n",
            "Epoch [2/4], Step [100/12500], Loss: 0.9634\n",
            "Epoch [2/4], Step [200/12500], Loss: 0.7047\n",
            "Epoch [2/4], Step [300/12500], Loss: 1.3406\n",
            "Epoch [2/4], Step [400/12500], Loss: 0.6688\n",
            "Epoch [2/4], Step [500/12500], Loss: 2.4476\n",
            "Epoch [2/4], Step [600/12500], Loss: 1.7483\n",
            "Epoch [2/4], Step [700/12500], Loss: 1.5706\n",
            "Epoch [2/4], Step [800/12500], Loss: 1.8712\n",
            "Epoch [2/4], Step [900/12500], Loss: 2.2593\n",
            "Epoch [2/4], Step [1000/12500], Loss: 0.7738\n",
            "Epoch [2/4], Step [1100/12500], Loss: 2.6844\n",
            "Epoch [2/4], Step [1200/12500], Loss: 1.4968\n",
            "Epoch [2/4], Step [1300/12500], Loss: 1.4273\n",
            "Epoch [2/4], Step [1400/12500], Loss: 1.2135\n",
            "Epoch [2/4], Step [1500/12500], Loss: 1.8345\n",
            "Epoch [2/4], Step [1600/12500], Loss: 0.2817\n",
            "Epoch [2/4], Step [1700/12500], Loss: 1.2257\n",
            "Epoch [2/4], Step [1800/12500], Loss: 1.3254\n",
            "Epoch [2/4], Step [1900/12500], Loss: 1.6671\n",
            "Epoch [2/4], Step [2000/12500], Loss: 2.0278\n",
            "Epoch [2/4], Step [2100/12500], Loss: 1.6211\n",
            "Epoch [2/4], Step [2200/12500], Loss: 1.7704\n",
            "Epoch [2/4], Step [2300/12500], Loss: 1.7231\n",
            "Epoch [2/4], Step [2400/12500], Loss: 1.8061\n",
            "Epoch [2/4], Step [2500/12500], Loss: 1.3799\n",
            "Epoch [2/4], Step [2600/12500], Loss: 1.1061\n",
            "Epoch [2/4], Step [2700/12500], Loss: 1.6021\n",
            "Epoch [2/4], Step [2800/12500], Loss: 1.4247\n",
            "Epoch [2/4], Step [2900/12500], Loss: 1.4310\n",
            "Epoch [2/4], Step [3000/12500], Loss: 1.4461\n",
            "Epoch [2/4], Step [3100/12500], Loss: 0.1255\n",
            "Epoch [2/4], Step [3200/12500], Loss: 1.4132\n",
            "Epoch [2/4], Step [3300/12500], Loss: 2.2216\n",
            "Epoch [2/4], Step [3400/12500], Loss: 1.7345\n",
            "Epoch [2/4], Step [3500/12500], Loss: 2.2999\n",
            "Epoch [2/4], Step [3600/12500], Loss: 2.0335\n",
            "Epoch [2/4], Step [3700/12500], Loss: 1.2147\n",
            "Epoch [2/4], Step [3800/12500], Loss: 2.1728\n",
            "Epoch [2/4], Step [3900/12500], Loss: 1.7341\n",
            "Epoch [2/4], Step [4000/12500], Loss: 2.5110\n",
            "Epoch [2/4], Step [4100/12500], Loss: 2.1838\n",
            "Epoch [2/4], Step [4200/12500], Loss: 1.2349\n",
            "Epoch [2/4], Step [4300/12500], Loss: 1.8682\n",
            "Epoch [2/4], Step [4400/12500], Loss: 0.4752\n",
            "Epoch [2/4], Step [4500/12500], Loss: 2.1655\n",
            "Epoch [2/4], Step [4600/12500], Loss: 1.1804\n",
            "Epoch [2/4], Step [4700/12500], Loss: 1.1188\n",
            "Epoch [2/4], Step [4800/12500], Loss: 1.5033\n",
            "Epoch [2/4], Step [4900/12500], Loss: 1.0478\n",
            "Epoch [2/4], Step [5000/12500], Loss: 1.3990\n",
            "Epoch [2/4], Step [5100/12500], Loss: 2.1488\n",
            "Epoch [2/4], Step [5200/12500], Loss: 1.6447\n",
            "Epoch [2/4], Step [5300/12500], Loss: 1.3119\n",
            "Epoch [2/4], Step [5400/12500], Loss: 2.9017\n",
            "Epoch [2/4], Step [5500/12500], Loss: 1.6624\n",
            "Epoch [2/4], Step [5600/12500], Loss: 1.8628\n",
            "Epoch [2/4], Step [5700/12500], Loss: 1.5322\n",
            "Epoch [2/4], Step [5800/12500], Loss: 2.0040\n",
            "Epoch [2/4], Step [5900/12500], Loss: 1.6072\n",
            "Epoch [2/4], Step [6000/12500], Loss: 0.6658\n",
            "Epoch [2/4], Step [6100/12500], Loss: 1.4293\n",
            "Epoch [2/4], Step [6200/12500], Loss: 1.0328\n",
            "Epoch [2/4], Step [6300/12500], Loss: 1.7375\n",
            "Epoch [2/4], Step [6400/12500], Loss: 1.0631\n",
            "Epoch [2/4], Step [6500/12500], Loss: 2.0398\n",
            "Epoch [2/4], Step [6600/12500], Loss: 2.0552\n",
            "Epoch [2/4], Step [6700/12500], Loss: 1.7155\n",
            "Epoch [2/4], Step [6800/12500], Loss: 2.2303\n",
            "Epoch [2/4], Step [6900/12500], Loss: 0.9552\n",
            "Epoch [2/4], Step [7000/12500], Loss: 1.3875\n",
            "Epoch [2/4], Step [7100/12500], Loss: 1.7597\n",
            "Epoch [2/4], Step [7200/12500], Loss: 1.3980\n",
            "Epoch [2/4], Step [7300/12500], Loss: 1.1771\n",
            "Epoch [2/4], Step [7400/12500], Loss: 2.0464\n",
            "Epoch [2/4], Step [7500/12500], Loss: 1.0334\n",
            "Epoch [2/4], Step [7600/12500], Loss: 1.4942\n",
            "Epoch [2/4], Step [7700/12500], Loss: 1.4921\n",
            "Epoch [2/4], Step [7800/12500], Loss: 2.7584\n",
            "Epoch [2/4], Step [7900/12500], Loss: 1.4003\n",
            "Epoch [2/4], Step [8000/12500], Loss: 2.4060\n",
            "Epoch [2/4], Step [8100/12500], Loss: 1.3519\n",
            "Epoch [2/4], Step [8200/12500], Loss: 1.1392\n",
            "Epoch [2/4], Step [8300/12500], Loss: 1.1883\n",
            "Epoch [2/4], Step [8400/12500], Loss: 1.3032\n",
            "Epoch [2/4], Step [8500/12500], Loss: 1.6479\n",
            "Epoch [2/4], Step [8600/12500], Loss: 1.7652\n",
            "Epoch [2/4], Step [8700/12500], Loss: 1.6491\n",
            "Epoch [2/4], Step [8800/12500], Loss: 1.7010\n",
            "Epoch [2/4], Step [8900/12500], Loss: 1.9199\n",
            "Epoch [2/4], Step [9000/12500], Loss: 1.3273\n",
            "Epoch [2/4], Step [9100/12500], Loss: 1.8601\n",
            "Epoch [2/4], Step [9200/12500], Loss: 2.3845\n",
            "Epoch [2/4], Step [9300/12500], Loss: 2.4649\n",
            "Epoch [2/4], Step [9400/12500], Loss: 1.3062\n",
            "Epoch [2/4], Step [9500/12500], Loss: 1.8563\n",
            "Epoch [2/4], Step [9600/12500], Loss: 1.6171\n",
            "Epoch [2/4], Step [9700/12500], Loss: 0.2897\n",
            "Epoch [2/4], Step [9800/12500], Loss: 1.7751\n",
            "Epoch [2/4], Step [9900/12500], Loss: 1.3787\n",
            "Epoch [2/4], Step [10000/12500], Loss: 2.0221\n",
            "Epoch [2/4], Step [10100/12500], Loss: 1.5224\n",
            "Epoch [2/4], Step [10200/12500], Loss: 1.6580\n",
            "Epoch [2/4], Step [10300/12500], Loss: 1.7161\n",
            "Epoch [2/4], Step [10400/12500], Loss: 1.4001\n",
            "Epoch [2/4], Step [10500/12500], Loss: 1.7817\n",
            "Epoch [2/4], Step [10600/12500], Loss: 2.4151\n",
            "Epoch [2/4], Step [10700/12500], Loss: 1.4865\n",
            "Epoch [2/4], Step [10800/12500], Loss: 1.5628\n",
            "Epoch [2/4], Step [10900/12500], Loss: 1.5023\n",
            "Epoch [2/4], Step [11000/12500], Loss: 2.4941\n",
            "Epoch [2/4], Step [11100/12500], Loss: 2.3047\n",
            "Epoch [2/4], Step [11200/12500], Loss: 1.8460\n",
            "Epoch [2/4], Step [11300/12500], Loss: 1.6121\n",
            "Epoch [2/4], Step [11400/12500], Loss: 2.1578\n",
            "Epoch [2/4], Step [11500/12500], Loss: 1.5620\n",
            "Epoch [2/4], Step [11600/12500], Loss: 1.5011\n",
            "Epoch [2/4], Step [11700/12500], Loss: 2.6004\n",
            "Epoch [2/4], Step [11800/12500], Loss: 1.3751\n",
            "Epoch [2/4], Step [11900/12500], Loss: 2.1883\n",
            "Epoch [2/4], Step [12000/12500], Loss: 1.6028\n",
            "Epoch [2/4], Step [12100/12500], Loss: 1.0185\n",
            "Epoch [2/4], Step [12200/12500], Loss: 1.4944\n",
            "Epoch [2/4], Step [12300/12500], Loss: 1.3256\n",
            "Epoch [2/4], Step [12400/12500], Loss: 1.0641\n",
            "Epoch [2/4], Step [12500/12500], Loss: 1.2145\n",
            "Epoch [3/4], Step [100/12500], Loss: 1.2854\n",
            "Epoch [3/4], Step [200/12500], Loss: 2.1162\n",
            "Epoch [3/4], Step [300/12500], Loss: 1.9493\n",
            "Epoch [3/4], Step [400/12500], Loss: 0.8846\n",
            "Epoch [3/4], Step [500/12500], Loss: 1.2657\n",
            "Epoch [3/4], Step [600/12500], Loss: 1.3174\n",
            "Epoch [3/4], Step [700/12500], Loss: 1.4197\n",
            "Epoch [3/4], Step [800/12500], Loss: 1.5109\n",
            "Epoch [3/4], Step [900/12500], Loss: 1.3667\n",
            "Epoch [3/4], Step [1000/12500], Loss: 1.7012\n",
            "Epoch [3/4], Step [1100/12500], Loss: 1.8401\n",
            "Epoch [3/4], Step [1200/12500], Loss: 1.2507\n",
            "Epoch [3/4], Step [1300/12500], Loss: 0.9807\n",
            "Epoch [3/4], Step [1400/12500], Loss: 1.3398\n",
            "Epoch [3/4], Step [1500/12500], Loss: 2.3458\n",
            "Epoch [3/4], Step [1600/12500], Loss: 1.8713\n",
            "Epoch [3/4], Step [1700/12500], Loss: 1.7699\n",
            "Epoch [3/4], Step [1800/12500], Loss: 1.5613\n",
            "Epoch [3/4], Step [1900/12500], Loss: 1.9057\n",
            "Epoch [3/4], Step [2000/12500], Loss: 2.7966\n",
            "Epoch [3/4], Step [2100/12500], Loss: 2.5070\n",
            "Epoch [3/4], Step [2200/12500], Loss: 1.4584\n",
            "Epoch [3/4], Step [2300/12500], Loss: 1.6970\n",
            "Epoch [3/4], Step [2400/12500], Loss: 1.3508\n",
            "Epoch [3/4], Step [2500/12500], Loss: 1.2279\n",
            "Epoch [3/4], Step [2600/12500], Loss: 0.8609\n",
            "Epoch [3/4], Step [2700/12500], Loss: 2.3734\n",
            "Epoch [3/4], Step [2800/12500], Loss: 1.2832\n",
            "Epoch [3/4], Step [2900/12500], Loss: 2.2297\n",
            "Epoch [3/4], Step [3000/12500], Loss: 1.9990\n",
            "Epoch [3/4], Step [3100/12500], Loss: 1.8658\n",
            "Epoch [3/4], Step [3200/12500], Loss: 1.0322\n",
            "Epoch [3/4], Step [3300/12500], Loss: 1.0589\n",
            "Epoch [3/4], Step [3400/12500], Loss: 2.2989\n",
            "Epoch [3/4], Step [3500/12500], Loss: 1.0459\n",
            "Epoch [3/4], Step [3600/12500], Loss: 1.1938\n",
            "Epoch [3/4], Step [3700/12500], Loss: 1.6596\n",
            "Epoch [3/4], Step [3800/12500], Loss: 1.0012\n",
            "Epoch [3/4], Step [3900/12500], Loss: 1.5996\n",
            "Epoch [3/4], Step [4000/12500], Loss: 1.9656\n",
            "Epoch [3/4], Step [4100/12500], Loss: 1.4122\n",
            "Epoch [3/4], Step [4200/12500], Loss: 1.1385\n",
            "Epoch [3/4], Step [4300/12500], Loss: 1.6755\n",
            "Epoch [3/4], Step [4400/12500], Loss: 1.3688\n",
            "Epoch [3/4], Step [4500/12500], Loss: 1.8775\n",
            "Epoch [3/4], Step [4600/12500], Loss: 2.4645\n",
            "Epoch [3/4], Step [4700/12500], Loss: 2.2817\n",
            "Epoch [3/4], Step [4800/12500], Loss: 3.1061\n",
            "Epoch [3/4], Step [4900/12500], Loss: 2.0945\n",
            "Epoch [3/4], Step [5000/12500], Loss: 1.7804\n",
            "Epoch [3/4], Step [5100/12500], Loss: 2.4508\n",
            "Epoch [3/4], Step [5200/12500], Loss: 0.6759\n",
            "Epoch [3/4], Step [5300/12500], Loss: 1.6161\n",
            "Epoch [3/4], Step [5400/12500], Loss: 1.2906\n",
            "Epoch [3/4], Step [5500/12500], Loss: 2.1097\n",
            "Epoch [3/4], Step [5600/12500], Loss: 1.9297\n",
            "Epoch [3/4], Step [5700/12500], Loss: 1.2403\n",
            "Epoch [3/4], Step [5800/12500], Loss: 1.8822\n",
            "Epoch [3/4], Step [5900/12500], Loss: 1.2569\n",
            "Epoch [3/4], Step [6000/12500], Loss: 1.7613\n",
            "Epoch [3/4], Step [6100/12500], Loss: 1.4810\n",
            "Epoch [3/4], Step [6200/12500], Loss: 0.8168\n",
            "Epoch [3/4], Step [6300/12500], Loss: 1.2351\n",
            "Epoch [3/4], Step [6400/12500], Loss: 1.6036\n",
            "Epoch [3/4], Step [6500/12500], Loss: 1.1806\n",
            "Epoch [3/4], Step [6600/12500], Loss: 1.6943\n",
            "Epoch [3/4], Step [6700/12500], Loss: 1.9618\n",
            "Epoch [3/4], Step [6800/12500], Loss: 1.9773\n",
            "Epoch [3/4], Step [6900/12500], Loss: 2.6289\n",
            "Epoch [3/4], Step [7000/12500], Loss: 1.1896\n",
            "Epoch [3/4], Step [7100/12500], Loss: 1.1615\n",
            "Epoch [3/4], Step [7200/12500], Loss: 2.6881\n",
            "Epoch [3/4], Step [7300/12500], Loss: 1.4458\n",
            "Epoch [3/4], Step [7400/12500], Loss: 1.9128\n",
            "Epoch [3/4], Step [7500/12500], Loss: 1.4722\n",
            "Epoch [3/4], Step [7600/12500], Loss: 1.4526\n",
            "Epoch [3/4], Step [7700/12500], Loss: 1.2324\n",
            "Epoch [3/4], Step [7800/12500], Loss: 0.7825\n",
            "Epoch [3/4], Step [7900/12500], Loss: 1.9359\n",
            "Epoch [3/4], Step [8000/12500], Loss: 1.5394\n",
            "Epoch [3/4], Step [8100/12500], Loss: 1.7646\n",
            "Epoch [3/4], Step [8200/12500], Loss: 1.9666\n",
            "Epoch [3/4], Step [8300/12500], Loss: 1.8516\n",
            "Epoch [3/4], Step [8400/12500], Loss: 1.7390\n",
            "Epoch [3/4], Step [8500/12500], Loss: 1.1280\n",
            "Epoch [3/4], Step [8600/12500], Loss: 1.7441\n",
            "Epoch [3/4], Step [8700/12500], Loss: 1.7153\n",
            "Epoch [3/4], Step [8800/12500], Loss: 1.8792\n",
            "Epoch [3/4], Step [8900/12500], Loss: 0.7847\n",
            "Epoch [3/4], Step [9000/12500], Loss: 1.3535\n",
            "Epoch [3/4], Step [9100/12500], Loss: 1.7101\n",
            "Epoch [3/4], Step [9200/12500], Loss: 0.9138\n",
            "Epoch [3/4], Step [9300/12500], Loss: 2.0245\n",
            "Epoch [3/4], Step [9400/12500], Loss: 1.2143\n",
            "Epoch [3/4], Step [9500/12500], Loss: 1.9975\n",
            "Epoch [3/4], Step [9600/12500], Loss: 2.1098\n",
            "Epoch [3/4], Step [9700/12500], Loss: 1.7496\n",
            "Epoch [3/4], Step [9800/12500], Loss: 1.2761\n",
            "Epoch [3/4], Step [9900/12500], Loss: 1.5137\n",
            "Epoch [3/4], Step [10000/12500], Loss: 1.4665\n",
            "Epoch [3/4], Step [10100/12500], Loss: 1.2481\n",
            "Epoch [3/4], Step [10200/12500], Loss: 1.2299\n",
            "Epoch [3/4], Step [10300/12500], Loss: 1.8480\n",
            "Epoch [3/4], Step [10400/12500], Loss: 1.5622\n",
            "Epoch [3/4], Step [10500/12500], Loss: 1.7782\n",
            "Epoch [3/4], Step [10600/12500], Loss: 1.3856\n",
            "Epoch [3/4], Step [10700/12500], Loss: 2.1981\n",
            "Epoch [3/4], Step [10800/12500], Loss: 0.7424\n",
            "Epoch [3/4], Step [10900/12500], Loss: 2.2010\n",
            "Epoch [3/4], Step [11000/12500], Loss: 0.3005\n",
            "Epoch [3/4], Step [11100/12500], Loss: 2.2315\n",
            "Epoch [3/4], Step [11200/12500], Loss: 1.1716\n",
            "Epoch [3/4], Step [11300/12500], Loss: 1.7276\n",
            "Epoch [3/4], Step [11400/12500], Loss: 2.4192\n",
            "Epoch [3/4], Step [11500/12500], Loss: 1.9121\n",
            "Epoch [3/4], Step [11600/12500], Loss: 1.8261\n",
            "Epoch [3/4], Step [11700/12500], Loss: 1.5394\n",
            "Epoch [3/4], Step [11800/12500], Loss: 1.6183\n",
            "Epoch [3/4], Step [11900/12500], Loss: 1.7243\n",
            "Epoch [3/4], Step [12000/12500], Loss: 1.2868\n",
            "Epoch [3/4], Step [12100/12500], Loss: 2.1429\n",
            "Epoch [3/4], Step [12200/12500], Loss: 1.8111\n",
            "Epoch [3/4], Step [12300/12500], Loss: 1.2170\n",
            "Epoch [3/4], Step [12400/12500], Loss: 1.0775\n",
            "Epoch [3/4], Step [12500/12500], Loss: 1.2929\n",
            "Epoch [4/4], Step [100/12500], Loss: 1.3602\n",
            "Epoch [4/4], Step [200/12500], Loss: 1.0152\n",
            "Epoch [4/4], Step [300/12500], Loss: 0.7686\n",
            "Epoch [4/4], Step [400/12500], Loss: 2.5581\n",
            "Epoch [4/4], Step [500/12500], Loss: 1.1663\n",
            "Epoch [4/4], Step [600/12500], Loss: 0.1543\n",
            "Epoch [4/4], Step [700/12500], Loss: 1.1972\n",
            "Epoch [4/4], Step [800/12500], Loss: 2.0247\n",
            "Epoch [4/4], Step [900/12500], Loss: 0.8697\n",
            "Epoch [4/4], Step [1000/12500], Loss: 1.9912\n",
            "Epoch [4/4], Step [1100/12500], Loss: 1.6796\n",
            "Epoch [4/4], Step [1200/12500], Loss: 2.5393\n",
            "Epoch [4/4], Step [1300/12500], Loss: 0.7306\n",
            "Epoch [4/4], Step [1400/12500], Loss: 1.2709\n",
            "Epoch [4/4], Step [1500/12500], Loss: 2.2727\n",
            "Epoch [4/4], Step [1600/12500], Loss: 1.2815\n",
            "Epoch [4/4], Step [1700/12500], Loss: 1.4129\n",
            "Epoch [4/4], Step [1800/12500], Loss: 2.0943\n",
            "Epoch [4/4], Step [1900/12500], Loss: 1.0750\n",
            "Epoch [4/4], Step [2000/12500], Loss: 1.2395\n",
            "Epoch [4/4], Step [2100/12500], Loss: 2.5681\n",
            "Epoch [4/4], Step [2200/12500], Loss: 1.5463\n",
            "Epoch [4/4], Step [2300/12500], Loss: 1.2857\n",
            "Epoch [4/4], Step [2400/12500], Loss: 1.4371\n",
            "Epoch [4/4], Step [2500/12500], Loss: 1.3034\n",
            "Epoch [4/4], Step [2600/12500], Loss: 1.3821\n",
            "Epoch [4/4], Step [2700/12500], Loss: 0.6611\n",
            "Epoch [4/4], Step [2800/12500], Loss: 1.5118\n",
            "Epoch [4/4], Step [2900/12500], Loss: 1.7641\n",
            "Epoch [4/4], Step [3000/12500], Loss: 2.5517\n",
            "Epoch [4/4], Step [3100/12500], Loss: 0.5819\n",
            "Epoch [4/4], Step [3200/12500], Loss: 0.5933\n",
            "Epoch [4/4], Step [3300/12500], Loss: 1.9810\n",
            "Epoch [4/4], Step [3400/12500], Loss: 2.7510\n",
            "Epoch [4/4], Step [3500/12500], Loss: 1.6660\n",
            "Epoch [4/4], Step [3600/12500], Loss: 1.1577\n",
            "Epoch [4/4], Step [3700/12500], Loss: 1.2471\n",
            "Epoch [4/4], Step [3800/12500], Loss: 2.0590\n",
            "Epoch [4/4], Step [3900/12500], Loss: 0.9440\n",
            "Epoch [4/4], Step [4000/12500], Loss: 0.3480\n",
            "Epoch [4/4], Step [4100/12500], Loss: 1.3289\n",
            "Epoch [4/4], Step [4200/12500], Loss: 1.4716\n",
            "Epoch [4/4], Step [4300/12500], Loss: 2.3303\n",
            "Epoch [4/4], Step [4400/12500], Loss: 1.2370\n",
            "Epoch [4/4], Step [4500/12500], Loss: 1.2832\n",
            "Epoch [4/4], Step [4600/12500], Loss: 1.6093\n",
            "Epoch [4/4], Step [4700/12500], Loss: 1.0235\n",
            "Epoch [4/4], Step [4800/12500], Loss: 1.8647\n",
            "Epoch [4/4], Step [4900/12500], Loss: 2.1210\n",
            "Epoch [4/4], Step [5000/12500], Loss: 0.5133\n",
            "Epoch [4/4], Step [5100/12500], Loss: 0.4918\n",
            "Epoch [4/4], Step [5200/12500], Loss: 1.6223\n",
            "Epoch [4/4], Step [5300/12500], Loss: 1.2330\n",
            "Epoch [4/4], Step [5400/12500], Loss: 1.8225\n",
            "Epoch [4/4], Step [5500/12500], Loss: 1.7156\n",
            "Epoch [4/4], Step [5600/12500], Loss: 1.8461\n",
            "Epoch [4/4], Step [5700/12500], Loss: 1.6333\n",
            "Epoch [4/4], Step [5800/12500], Loss: 1.0263\n",
            "Epoch [4/4], Step [5900/12500], Loss: 0.9377\n",
            "Epoch [4/4], Step [6000/12500], Loss: 2.3368\n",
            "Epoch [4/4], Step [6100/12500], Loss: 0.7865\n",
            "Epoch [4/4], Step [6200/12500], Loss: 1.6432\n",
            "Epoch [4/4], Step [6300/12500], Loss: 1.8574\n",
            "Epoch [4/4], Step [6400/12500], Loss: 1.0627\n",
            "Epoch [4/4], Step [6500/12500], Loss: 1.5633\n",
            "Epoch [4/4], Step [6600/12500], Loss: 1.7908\n",
            "Epoch [4/4], Step [6700/12500], Loss: 1.0734\n",
            "Epoch [4/4], Step [6800/12500], Loss: 1.4422\n",
            "Epoch [4/4], Step [6900/12500], Loss: 1.3957\n",
            "Epoch [4/4], Step [7000/12500], Loss: 1.7247\n",
            "Epoch [4/4], Step [7100/12500], Loss: 1.4988\n",
            "Epoch [4/4], Step [7200/12500], Loss: 2.1704\n",
            "Epoch [4/4], Step [7300/12500], Loss: 1.7673\n",
            "Epoch [4/4], Step [7400/12500], Loss: 1.9167\n",
            "Epoch [4/4], Step [7500/12500], Loss: 1.4160\n",
            "Epoch [4/4], Step [7600/12500], Loss: 1.3704\n",
            "Epoch [4/4], Step [7700/12500], Loss: 0.7687\n",
            "Epoch [4/4], Step [7800/12500], Loss: 2.4814\n",
            "Epoch [4/4], Step [7900/12500], Loss: 1.8990\n",
            "Epoch [4/4], Step [8000/12500], Loss: 2.3026\n",
            "Epoch [4/4], Step [8100/12500], Loss: 2.5311\n",
            "Epoch [4/4], Step [8200/12500], Loss: 2.5905\n",
            "Epoch [4/4], Step [8300/12500], Loss: 1.6211\n",
            "Epoch [4/4], Step [8400/12500], Loss: 1.3915\n",
            "Epoch [4/4], Step [8500/12500], Loss: 3.4698\n",
            "Epoch [4/4], Step [8600/12500], Loss: 1.7518\n",
            "Epoch [4/4], Step [8700/12500], Loss: 1.5600\n",
            "Epoch [4/4], Step [8800/12500], Loss: 2.4689\n",
            "Epoch [4/4], Step [8900/12500], Loss: 0.6836\n",
            "Epoch [4/4], Step [9000/12500], Loss: 1.3381\n",
            "Epoch [4/4], Step [9100/12500], Loss: 1.7526\n",
            "Epoch [4/4], Step [9200/12500], Loss: 1.3261\n",
            "Epoch [4/4], Step [9300/12500], Loss: 1.3726\n",
            "Epoch [4/4], Step [9400/12500], Loss: 1.6047\n",
            "Epoch [4/4], Step [9500/12500], Loss: 1.5210\n",
            "Epoch [4/4], Step [9600/12500], Loss: 1.2547\n",
            "Epoch [4/4], Step [9700/12500], Loss: 0.7931\n",
            "Epoch [4/4], Step [9800/12500], Loss: 0.9397\n",
            "Epoch [4/4], Step [9900/12500], Loss: 1.0442\n",
            "Epoch [4/4], Step [10000/12500], Loss: 1.5964\n",
            "Epoch [4/4], Step [10100/12500], Loss: 1.7517\n",
            "Epoch [4/4], Step [10200/12500], Loss: 1.7323\n",
            "Epoch [4/4], Step [10300/12500], Loss: 1.9058\n",
            "Epoch [4/4], Step [10400/12500], Loss: 0.9704\n",
            "Epoch [4/4], Step [10500/12500], Loss: 1.8958\n",
            "Epoch [4/4], Step [10600/12500], Loss: 2.2194\n",
            "Epoch [4/4], Step [10700/12500], Loss: 1.7344\n",
            "Epoch [4/4], Step [10800/12500], Loss: 2.3026\n",
            "Epoch [4/4], Step [10900/12500], Loss: 1.8097\n",
            "Epoch [4/4], Step [11000/12500], Loss: 2.0078\n",
            "Epoch [4/4], Step [11100/12500], Loss: 1.6506\n",
            "Epoch [4/4], Step [11200/12500], Loss: 2.1187\n",
            "Epoch [4/4], Step [11300/12500], Loss: 0.6996\n",
            "Epoch [4/4], Step [11400/12500], Loss: 1.5160\n",
            "Epoch [4/4], Step [11500/12500], Loss: 1.0395\n",
            "Epoch [4/4], Step [11600/12500], Loss: 1.3399\n",
            "Epoch [4/4], Step [11700/12500], Loss: 1.6234\n",
            "Epoch [4/4], Step [11800/12500], Loss: 1.3657\n",
            "Epoch [4/4], Step [11900/12500], Loss: 1.5819\n",
            "Epoch [4/4], Step [12000/12500], Loss: 1.2954\n",
            "Epoch [4/4], Step [12100/12500], Loss: 1.3491\n",
            "Epoch [4/4], Step [12200/12500], Loss: 2.3346\n",
            "Epoch [4/4], Step [12300/12500], Loss: 2.0057\n",
            "Epoch [4/4], Step [12400/12500], Loss: 1.7701\n",
            "Epoch [4/4], Step [12500/12500], Loss: 1.3235\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "train(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 42.27%\n"
          ]
        }
      ],
      "source": [
        "test()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
