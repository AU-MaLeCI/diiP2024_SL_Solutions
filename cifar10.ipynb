{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D53KDb2hVU_X"
      },
      "source": [
        "# Training a CNN model on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (3.7.5)\n",
            "Requirement already satisfied: seaborn in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (0.13.2)\n",
            "Requirement already satisfied: torch in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (2.3.0)\n",
            "Requirement already satisfied: torchvision in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (2.3.0)\n",
            "Requirement already satisfied: numpy in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (1.24.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (2.9.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (6.4.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: filelock in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib seaborn torch torchvision torchaudio numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0z7zspfLVU_Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # This is to load plotting functions\n",
        "import seaborn as sns; sns.set() # This is to make the plots prettier\n",
        "import torch # This is the ML library we will use\n",
        "import torchvision # This is the supporting ML library for computer vision\n",
        "from torchvision import datasets # This is to access the CIFAR10 dataset\n",
        "from torch.utils.data import DataLoader # This is used to load the data efficiently\n",
        "import torchvision.transforms as transforms # This is used to transform data when preparing the dataset\n",
        "import numpy as np # This is used to handle arrays of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Select the device from GPU and CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgaQ7FaCVU_g",
        "outputId": "2b0f9f23-03a1-47cf-f079-431bef89dd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If GPU is available, it is chosen for computations, and if not, CPU will be used\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download the CIFAR10 dataset and create tran and test data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# This is used to normalize input images\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_set = datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "test_set = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = (\n",
        "    \"plane\",\n",
        "    \"car\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"deer\",\n",
        "    \"dog\",\n",
        "    \"frog\",\n",
        "    \"horse\",\n",
        "    \"ship\",\n",
        "    \"truck\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Show example images from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ship  car   deer  horse\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def imshow(img):\n",
        "    # Unnormalize\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# Print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create a simple CNN classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Cifar10Classifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.body = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=6,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2,\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=6,\n",
        "                out_channels=16,\n",
        "                kernel_size=5,\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(16 * 6 * 6, 120),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(120, 84),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(84, 10),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.body(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Cifar10Classifier(\n",
              "  (body): Sequential(\n",
              "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Flatten(start_dim=1, end_dim=-1)\n",
              "    (7): Linear(in_features=576, out_features=120, bias=True)\n",
              "    (8): ReLU()\n",
              "    (9): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (10): ReLU()\n",
              "    (11): Linear(in_features=84, out_features=10, bias=True)\n",
              "    (12): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Cifar10Classifier()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create a loss function and an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/io/miniconda3/envs/diip/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/home/io/miniconda3/envs/diip/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/12500], Loss: 2.2639\n",
            "Epoch [1/2], Step [200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [300/12500], Loss: 2.3751\n",
            "Epoch [1/2], Step [400/12500], Loss: 2.5028\n",
            "Epoch [1/2], Step [500/12500], Loss: 2.2883\n",
            "Epoch [1/2], Step [600/12500], Loss: 2.3042\n",
            "Epoch [1/2], Step [700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [800/12500], Loss: 2.2041\n",
            "Epoch [1/2], Step [900/12500], Loss: 2.4255\n",
            "Epoch [1/2], Step [1000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1100/12500], Loss: 2.3053\n",
            "Epoch [1/2], Step [1200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1400/12500], Loss: 1.4607\n",
            "Epoch [1/2], Step [1500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1700/12500], Loss: 1.6503\n",
            "Epoch [1/2], Step [1800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1900/12500], Loss: 2.1428\n",
            "Epoch [1/2], Step [2000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2100/12500], Loss: 2.3313\n",
            "Epoch [1/2], Step [2200/12500], Loss: 3.5168\n",
            "Epoch [1/2], Step [2300/12500], Loss: 1.7660\n",
            "Epoch [1/2], Step [2400/12500], Loss: 2.0807\n",
            "Epoch [1/2], Step [2500/12500], Loss: 1.8917\n",
            "Epoch [1/2], Step [2600/12500], Loss: 1.8842\n",
            "Epoch [1/2], Step [2700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2800/12500], Loss: 1.7718\n",
            "Epoch [1/2], Step [2900/12500], Loss: 1.9416\n",
            "Epoch [1/2], Step [3000/12500], Loss: 2.0458\n",
            "Epoch [1/2], Step [3100/12500], Loss: 2.8578\n",
            "Epoch [1/2], Step [3200/12500], Loss: 1.8786\n",
            "Epoch [1/2], Step [3300/12500], Loss: 1.9108\n",
            "Epoch [1/2], Step [3400/12500], Loss: 2.4572\n",
            "Epoch [1/2], Step [3500/12500], Loss: 2.3278\n",
            "Epoch [1/2], Step [3600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3700/12500], Loss: 2.3058\n",
            "Epoch [1/2], Step [3800/12500], Loss: 2.3685\n",
            "Epoch [1/2], Step [3900/12500], Loss: 2.2155\n",
            "Epoch [1/2], Step [4000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4100/12500], Loss: 2.3407\n",
            "Epoch [1/2], Step [4200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4300/12500], Loss: 1.3862\n",
            "Epoch [1/2], Step [4400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4500/12500], Loss: 2.3082\n",
            "Epoch [1/2], Step [4600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4700/12500], Loss: 2.4033\n",
            "Epoch [1/2], Step [4800/12500], Loss: 1.8100\n",
            "Epoch [1/2], Step [4900/12500], Loss: 1.8889\n",
            "Epoch [1/2], Step [5000/12500], Loss: 2.4810\n",
            "Epoch [1/2], Step [5100/12500], Loss: 2.0434\n",
            "Epoch [1/2], Step [5200/12500], Loss: 2.0853\n",
            "Epoch [1/2], Step [5300/12500], Loss: 1.8746\n",
            "Epoch [1/2], Step [5400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5500/12500], Loss: 1.8367\n",
            "Epoch [1/2], Step [5600/12500], Loss: 1.8785\n",
            "Epoch [1/2], Step [5700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5900/12500], Loss: 2.2153\n",
            "Epoch [1/2], Step [6000/12500], Loss: 2.0153\n",
            "Epoch [1/2], Step [6100/12500], Loss: 2.7103\n",
            "Epoch [1/2], Step [6200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6400/12500], Loss: 1.6316\n",
            "Epoch [1/2], Step [6500/12500], Loss: 2.3694\n",
            "Epoch [1/2], Step [6600/12500], Loss: 1.9217\n",
            "Epoch [1/2], Step [6700/12500], Loss: 1.6122\n",
            "Epoch [1/2], Step [6800/12500], Loss: 2.3028\n",
            "Epoch [1/2], Step [6900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7000/12500], Loss: 2.2674\n",
            "Epoch [1/2], Step [7100/12500], Loss: 2.3080\n",
            "Epoch [1/2], Step [7200/12500], Loss: 1.3257\n",
            "Epoch [1/2], Step [7300/12500], Loss: 1.7745\n",
            "Epoch [1/2], Step [7400/12500], Loss: 1.9798\n",
            "Epoch [1/2], Step [7500/12500], Loss: 1.3007\n",
            "Epoch [1/2], Step [7600/12500], Loss: 1.7305\n",
            "Epoch [1/2], Step [7700/12500], Loss: 2.3462\n",
            "Epoch [1/2], Step [7800/12500], Loss: 2.1807\n",
            "Epoch [1/2], Step [7900/12500], Loss: 1.7505\n",
            "Epoch [1/2], Step [8000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8300/12500], Loss: 2.3279\n",
            "Epoch [1/2], Step [8400/12500], Loss: 1.7819\n",
            "Epoch [1/2], Step [8500/12500], Loss: 1.9353\n",
            "Epoch [1/2], Step [8600/12500], Loss: 2.3683\n",
            "Epoch [1/2], Step [8700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8800/12500], Loss: 1.7343\n",
            "Epoch [1/2], Step [8900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9000/12500], Loss: 2.5133\n",
            "Epoch [1/2], Step [9100/12500], Loss: 1.2825\n",
            "Epoch [1/2], Step [9200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9300/12500], Loss: 0.9718\n",
            "Epoch [1/2], Step [9400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9500/12500], Loss: 1.8515\n",
            "Epoch [1/2], Step [9600/12500], Loss: 2.2464\n",
            "Epoch [1/2], Step [9700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9800/12500], Loss: 1.6521\n",
            "Epoch [1/2], Step [9900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10000/12500], Loss: 2.4167\n",
            "Epoch [1/2], Step [10100/12500], Loss: 2.3391\n",
            "Epoch [1/2], Step [10200/12500], Loss: 2.3091\n",
            "Epoch [1/2], Step [10300/12500], Loss: 2.9590\n",
            "Epoch [1/2], Step [10400/12500], Loss: 1.8267\n",
            "Epoch [1/2], Step [10500/12500], Loss: 1.7605\n",
            "Epoch [1/2], Step [10600/12500], Loss: 1.9103\n",
            "Epoch [1/2], Step [10700/12500], Loss: 2.4191\n",
            "Epoch [1/2], Step [10800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10900/12500], Loss: 2.3305\n",
            "Epoch [1/2], Step [11000/12500], Loss: 1.9950\n",
            "Epoch [1/2], Step [11100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11200/12500], Loss: 2.3203\n",
            "Epoch [1/2], Step [11300/12500], Loss: 0.8813\n",
            "Epoch [1/2], Step [11400/12500], Loss: 1.5200\n",
            "Epoch [1/2], Step [11500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11600/12500], Loss: 1.3162\n",
            "Epoch [1/2], Step [11700/12500], Loss: 1.8634\n",
            "Epoch [1/2], Step [11800/12500], Loss: 1.9046\n",
            "Epoch [1/2], Step [11900/12500], Loss: 1.8923\n",
            "Epoch [1/2], Step [12000/12500], Loss: 1.7274\n",
            "Epoch [1/2], Step [12100/12500], Loss: 2.3299\n",
            "Epoch [1/2], Step [12200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12400/12500], Loss: 1.8726\n",
            "Epoch [1/2], Step [12500/12500], Loss: 2.3110\n",
            "Epoch [2/2], Step [100/12500], Loss: 2.0101\n",
            "Epoch [2/2], Step [200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [400/12500], Loss: 1.9268\n",
            "Epoch [2/2], Step [500/12500], Loss: 1.8870\n",
            "Epoch [2/2], Step [600/12500], Loss: 2.4145\n",
            "Epoch [2/2], Step [700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [800/12500], Loss: 1.8081\n",
            "Epoch [2/2], Step [900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1000/12500], Loss: 2.4151\n",
            "Epoch [2/2], Step [1100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1200/12500], Loss: 1.8708\n",
            "Epoch [2/2], Step [1300/12500], Loss: 2.4994\n",
            "Epoch [2/2], Step [1400/12500], Loss: 2.5614\n",
            "Epoch [2/2], Step [1500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1600/12500], Loss: 2.3040\n",
            "Epoch [2/2], Step [1700/12500], Loss: 2.3234\n",
            "Epoch [2/2], Step [1800/12500], Loss: 2.3478\n",
            "Epoch [2/2], Step [1900/12500], Loss: 1.7534\n",
            "Epoch [2/2], Step [2000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2100/12500], Loss: 1.8211\n",
            "Epoch [2/2], Step [2200/12500], Loss: 2.3091\n",
            "Epoch [2/2], Step [2300/12500], Loss: 2.4709\n",
            "Epoch [2/2], Step [2400/12500], Loss: 1.3481\n",
            "Epoch [2/2], Step [2500/12500], Loss: 1.7560\n",
            "Epoch [2/2], Step [2600/12500], Loss: 1.2181\n",
            "Epoch [2/2], Step [2700/12500], Loss: 2.0269\n",
            "Epoch [2/2], Step [2800/12500], Loss: 1.7719\n",
            "Epoch [2/2], Step [2900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3000/12500], Loss: 2.3109\n",
            "Epoch [2/2], Step [3100/12500], Loss: 1.8148\n",
            "Epoch [2/2], Step [3200/12500], Loss: 2.3128\n",
            "Epoch [2/2], Step [3300/12500], Loss: 2.3081\n",
            "Epoch [2/2], Step [3400/12500], Loss: 2.3165\n",
            "Epoch [2/2], Step [3500/12500], Loss: 2.0637\n",
            "Epoch [2/2], Step [3600/12500], Loss: 2.3554\n",
            "Epoch [2/2], Step [3700/12500], Loss: 1.3934\n",
            "Epoch [2/2], Step [3800/12500], Loss: 1.3497\n",
            "Epoch [2/2], Step [3900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4100/12500], Loss: 2.4123\n",
            "Epoch [2/2], Step [4200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4300/12500], Loss: 1.8888\n",
            "Epoch [2/2], Step [4400/12500], Loss: 2.3094\n",
            "Epoch [2/2], Step [4500/12500], Loss: 1.7946\n",
            "Epoch [2/2], Step [4600/12500], Loss: 2.3406\n",
            "Epoch [2/2], Step [4700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4800/12500], Loss: 2.2584\n",
            "Epoch [2/2], Step [4900/12500], Loss: 2.3126\n",
            "Epoch [2/2], Step [5000/12500], Loss: 2.4659\n",
            "Epoch [2/2], Step [5100/12500], Loss: 2.3177\n",
            "Epoch [2/2], Step [5200/12500], Loss: 1.8807\n",
            "Epoch [2/2], Step [5300/12500], Loss: 1.8637\n",
            "Epoch [2/2], Step [5400/12500], Loss: 2.3272\n",
            "Epoch [2/2], Step [5500/12500], Loss: 2.0336\n",
            "Epoch [2/2], Step [5600/12500], Loss: 2.3834\n",
            "Epoch [2/2], Step [5700/12500], Loss: 1.8604\n",
            "Epoch [2/2], Step [5800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5900/12500], Loss: 2.3250\n",
            "Epoch [2/2], Step [6000/12500], Loss: 1.8821\n",
            "Epoch [2/2], Step [6100/12500], Loss: 2.7178\n",
            "Epoch [2/2], Step [6200/12500], Loss: 2.6247\n",
            "Epoch [2/2], Step [6300/12500], Loss: 2.3064\n",
            "Epoch [2/2], Step [6400/12500], Loss: 2.3222\n",
            "Epoch [2/2], Step [6500/12500], Loss: 2.3028\n",
            "Epoch [2/2], Step [6600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6700/12500], Loss: 1.8554\n",
            "Epoch [2/2], Step [6800/12500], Loss: 1.3523\n",
            "Epoch [2/2], Step [6900/12500], Loss: 1.5627\n",
            "Epoch [2/2], Step [7000/12500], Loss: 1.8356\n",
            "Epoch [2/2], Step [7100/12500], Loss: 1.7393\n",
            "Epoch [2/2], Step [7200/12500], Loss: 1.7138\n",
            "Epoch [2/2], Step [7300/12500], Loss: 2.3358\n",
            "Epoch [2/2], Step [7400/12500], Loss: 1.8031\n",
            "Epoch [2/2], Step [7500/12500], Loss: 1.8699\n",
            "Epoch [2/2], Step [7600/12500], Loss: 2.2200\n",
            "Epoch [2/2], Step [7700/12500], Loss: 2.1001\n",
            "Epoch [2/2], Step [7800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7900/12500], Loss: 1.8908\n",
            "Epoch [2/2], Step [8000/12500], Loss: 2.3228\n",
            "Epoch [2/2], Step [8100/12500], Loss: 1.7830\n",
            "Epoch [2/2], Step [8200/12500], Loss: 1.7375\n",
            "Epoch [2/2], Step [8300/12500], Loss: 2.3281\n",
            "Epoch [2/2], Step [8400/12500], Loss: 2.3046\n",
            "Epoch [2/2], Step [8500/12500], Loss: 1.2851\n",
            "Epoch [2/2], Step [8600/12500], Loss: 2.2540\n",
            "Epoch [2/2], Step [8700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8800/12500], Loss: 2.3149\n",
            "Epoch [2/2], Step [8900/12500], Loss: 2.9485\n",
            "Epoch [2/2], Step [9000/12500], Loss: 2.3511\n",
            "Epoch [2/2], Step [9100/12500], Loss: 2.1232\n",
            "Epoch [2/2], Step [9200/12500], Loss: 2.0654\n",
            "Epoch [2/2], Step [9300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9500/12500], Loss: 2.3053\n",
            "Epoch [2/2], Step [9600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9700/12500], Loss: 1.9312\n",
            "Epoch [2/2], Step [9800/12500], Loss: 1.8560\n",
            "Epoch [2/2], Step [9900/12500], Loss: 2.4733\n",
            "Epoch [2/2], Step [10000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10100/12500], Loss: 1.9575\n",
            "Epoch [2/2], Step [10200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10300/12500], Loss: 2.3095\n",
            "Epoch [2/2], Step [10400/12500], Loss: 1.7696\n",
            "Epoch [2/2], Step [10500/12500], Loss: 2.0591\n",
            "Epoch [2/2], Step [10600/12500], Loss: 2.0666\n",
            "Epoch [2/2], Step [10700/12500], Loss: 2.0686\n",
            "Epoch [2/2], Step [10800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10900/12500], Loss: 1.9647\n",
            "Epoch [2/2], Step [11000/12500], Loss: 1.8212\n",
            "Epoch [2/2], Step [11100/12500], Loss: 1.8898\n",
            "Epoch [2/2], Step [11200/12500], Loss: 2.1103\n",
            "Epoch [2/2], Step [11300/12500], Loss: 0.7600\n",
            "Epoch [2/2], Step [11400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11500/12500], Loss: 2.3556\n",
            "Epoch [2/2], Step [11600/12500], Loss: 2.3092\n",
            "Epoch [2/2], Step [11700/12500], Loss: 2.8984\n",
            "Epoch [2/2], Step [11800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11900/12500], Loss: 1.7955\n",
            "Epoch [2/2], Step [12000/12500], Loss: 1.8662\n",
            "Epoch [2/2], Step [12100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12200/12500], Loss: 1.3347\n",
            "Epoch [2/2], Step [12300/12500], Loss: 2.3115\n",
            "Epoch [2/2], Step [12400/12500], Loss: 1.7273\n",
            "Epoch [2/2], Step [12500/12500], Loss: 2.5799\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "def train(num_epochs, model, loss_func, optimizer):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Train the model\n",
        "    total_step = len(train_loader)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # For each batch in the training data\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute output and loss\n",
        "            output = model(images)\n",
        "            loss = loss_func(output, labels)\n",
        "\n",
        "            # Clear gradients for this training step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute gradients\n",
        "            loss.backward()\n",
        "            # Apply gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(\n",
        "                    \"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(\n",
        "                        epoch + 1, num_epochs, i + 1, total_step, loss.item()\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "train(2, model, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, test the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 15.80%\n"
          ]
        }
      ],
      "source": [
        "def test(model):\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            test_output = model(images)\n",
        "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
        "            correct += (pred_y == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        \n",
        "        accuracy = correct / float(total)\n",
        "        print('Test Accuracy of the model: %.2f%%' % (accuracy * 100))\n",
        "test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Now, let us train more and see how the result changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1], Step [100/12500], Loss: 1.7510\n",
            "Epoch [1/1], Step [200/12500], Loss: 1.8582\n",
            "Epoch [1/1], Step [300/12500], Loss: 1.3237\n",
            "Epoch [1/1], Step [400/12500], Loss: 2.5294\n",
            "Epoch [1/1], Step [500/12500], Loss: 2.1537\n",
            "Epoch [1/1], Step [600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [800/12500], Loss: 1.2785\n",
            "Epoch [1/1], Step [900/12500], Loss: 1.7785\n",
            "Epoch [1/1], Step [1000/12500], Loss: 2.3319\n",
            "Epoch [1/1], Step [1100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1200/12500], Loss: 1.9006\n",
            "Epoch [1/1], Step [1300/12500], Loss: 2.3645\n",
            "Epoch [1/1], Step [1400/12500], Loss: 2.2754\n",
            "Epoch [1/1], Step [1500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1600/12500], Loss: 1.7279\n",
            "Epoch [1/1], Step [1700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1900/12500], Loss: 1.9623\n",
            "Epoch [1/1], Step [2000/12500], Loss: 1.9422\n",
            "Epoch [1/1], Step [2100/12500], Loss: 1.7309\n",
            "Epoch [1/1], Step [2200/12500], Loss: 2.3160\n",
            "Epoch [1/1], Step [2300/12500], Loss: 1.8099\n",
            "Epoch [1/1], Step [2400/12500], Loss: 1.5449\n",
            "Epoch [1/1], Step [2500/12500], Loss: 1.8121\n",
            "Epoch [1/1], Step [2600/12500], Loss: 1.8047\n",
            "Epoch [1/1], Step [2700/12500], Loss: 1.9155\n",
            "Epoch [1/1], Step [2800/12500], Loss: 2.0464\n",
            "Epoch [1/1], Step [2900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3000/12500], Loss: 1.9192\n",
            "Epoch [1/1], Step [3100/12500], Loss: 1.7313\n",
            "Epoch [1/1], Step [3200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3300/12500], Loss: 1.7372\n",
            "Epoch [1/1], Step [3400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3500/12500], Loss: 1.4864\n",
            "Epoch [1/1], Step [3600/12500], Loss: 1.7605\n",
            "Epoch [1/1], Step [3700/12500], Loss: 2.3347\n",
            "Epoch [1/1], Step [3800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3900/12500], Loss: 1.7841\n",
            "Epoch [1/1], Step [4000/12500], Loss: 1.4079\n",
            "Epoch [1/1], Step [4100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4200/12500], Loss: 2.2132\n",
            "Epoch [1/1], Step [4300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4400/12500], Loss: 2.3240\n",
            "Epoch [1/1], Step [4500/12500], Loss: 2.5068\n",
            "Epoch [1/1], Step [4600/12500], Loss: 2.0372\n",
            "Epoch [1/1], Step [4700/12500], Loss: 2.4605\n",
            "Epoch [1/1], Step [4800/12500], Loss: 2.1269\n",
            "Epoch [1/1], Step [4900/12500], Loss: 1.8037\n",
            "Epoch [1/1], Step [5000/12500], Loss: 2.3131\n",
            "Epoch [1/1], Step [5100/12500], Loss: 1.8926\n",
            "Epoch [1/1], Step [5200/12500], Loss: 1.7934\n",
            "Epoch [1/1], Step [5300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5400/12500], Loss: 2.3146\n",
            "Epoch [1/1], Step [5500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5600/12500], Loss: 2.3645\n",
            "Epoch [1/1], Step [5700/12500], Loss: 1.8222\n",
            "Epoch [1/1], Step [5800/12500], Loss: 2.3699\n",
            "Epoch [1/1], Step [5900/12500], Loss: 2.3125\n",
            "Epoch [1/1], Step [6000/12500], Loss: 2.3367\n",
            "Epoch [1/1], Step [6100/12500], Loss: 1.8974\n",
            "Epoch [1/1], Step [6200/12500], Loss: 1.6413\n",
            "Epoch [1/1], Step [6300/12500], Loss: 1.8591\n",
            "Epoch [1/1], Step [6400/12500], Loss: 3.1849\n",
            "Epoch [1/1], Step [6500/12500], Loss: 2.3532\n",
            "Epoch [1/1], Step [6600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6700/12500], Loss: 1.5978\n",
            "Epoch [1/1], Step [6800/12500], Loss: 1.5320\n",
            "Epoch [1/1], Step [6900/12500], Loss: 1.6587\n",
            "Epoch [1/1], Step [7000/12500], Loss: 1.9443\n",
            "Epoch [1/1], Step [7100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7200/12500], Loss: 1.6190\n",
            "Epoch [1/1], Step [7300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7400/12500], Loss: 2.0932\n",
            "Epoch [1/1], Step [7500/12500], Loss: 1.7754\n",
            "Epoch [1/1], Step [7600/12500], Loss: 2.3487\n",
            "Epoch [1/1], Step [7700/12500], Loss: 0.6366\n",
            "Epoch [1/1], Step [7800/12500], Loss: 1.7559\n",
            "Epoch [1/1], Step [7900/12500], Loss: 1.2009\n",
            "Epoch [1/1], Step [8000/12500], Loss: 2.4816\n",
            "Epoch [1/1], Step [8100/12500], Loss: 1.3838\n",
            "Epoch [1/1], Step [8200/12500], Loss: 1.7905\n",
            "Epoch [1/1], Step [8300/12500], Loss: 3.3138\n",
            "Epoch [1/1], Step [8400/12500], Loss: 2.3837\n",
            "Epoch [1/1], Step [8500/12500], Loss: 1.7901\n",
            "Epoch [1/1], Step [8600/12500], Loss: 1.7735\n",
            "Epoch [1/1], Step [8700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8800/12500], Loss: 2.3205\n",
            "Epoch [1/1], Step [8900/12500], Loss: 2.0814\n",
            "Epoch [1/1], Step [9000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9200/12500], Loss: 2.3221\n",
            "Epoch [1/1], Step [9300/12500], Loss: 2.6732\n",
            "Epoch [1/1], Step [9400/12500], Loss: 1.8867\n",
            "Epoch [1/1], Step [9500/12500], Loss: 2.3168\n",
            "Epoch [1/1], Step [9600/12500], Loss: 2.7589\n",
            "Epoch [1/1], Step [9700/12500], Loss: 2.1340\n",
            "Epoch [1/1], Step [9800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10000/12500], Loss: 1.7625\n",
            "Epoch [1/1], Step [10100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10200/12500], Loss: 1.9480\n",
            "Epoch [1/1], Step [10300/12500], Loss: 2.1983\n",
            "Epoch [1/1], Step [10400/12500], Loss: 2.2098\n",
            "Epoch [1/1], Step [10500/12500], Loss: 1.8958\n",
            "Epoch [1/1], Step [10600/12500], Loss: 2.1168\n",
            "Epoch [1/1], Step [10700/12500], Loss: 2.8341\n",
            "Epoch [1/1], Step [10800/12500], Loss: 2.3051\n",
            "Epoch [1/1], Step [10900/12500], Loss: 2.7865\n",
            "Epoch [1/1], Step [11000/12500], Loss: 1.7587\n",
            "Epoch [1/1], Step [11100/12500], Loss: 1.2309\n",
            "Epoch [1/1], Step [11200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11300/12500], Loss: 1.7731\n",
            "Epoch [1/1], Step [11400/12500], Loss: 2.3070\n",
            "Epoch [1/1], Step [11500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11700/12500], Loss: 1.3992\n",
            "Epoch [1/1], Step [11800/12500], Loss: 2.9606\n",
            "Epoch [1/1], Step [11900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12000/12500], Loss: 2.1941\n",
            "Epoch [1/1], Step [12100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12200/12500], Loss: 2.8230\n",
            "Epoch [1/1], Step [12300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12400/12500], Loss: 1.7963\n",
            "Epoch [1/1], Step [12500/12500], Loss: 2.0432\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "train(1, model, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 16.43%\n"
          ]
        }
      ],
      "source": [
        "test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretrained ResNet18 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the pretrained model from torchvision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet18 = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "resnet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ImageNet has different image size and the number of classes.\n",
        "\n",
        "We can adapt this model by changing the first and the last layers to fit our needs.\n",
        "Those layers are untrained, but the knowledge from all other layers is still relevant and helps the model to be trained faster.\n",
        "\n",
        "The first convolutional layer of ResNet needs to receive as input an image with 3 channels, have 64 output channels, 3x3 filter, stride 1 and padding 1 with no bias.\n",
        "\n",
        "The last fully-connected layer should receive as input a vector with the same number of features as before, but the number of output dimensions should be equal to the number of classes in CIFAR10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "    <code>\n",
        "    resnet18.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)<br>\n",
        "    num_features = resnet18.fc.in_features<br>\n",
        "    resnet18.fc = torch.nn.Linear(num_features, num_classes)<br>\n",
        "    </code>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = 10\n",
        "resnet18.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "num_features = resnet18.fc.in_features\n",
        "resnet18.fc = torch.nn.Linear(num_features, num_classes)\n",
        "resnet18.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### It is pretrained on ImageNet, so the starting accuracy on CIFAR10 should be bad, let us check it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 10.17%\n"
          ]
        }
      ],
      "source": [
        "test(resnet18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare the training objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(resnet18.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/12500], Loss: 3.3114\n",
            "Epoch [1/2], Step [200/12500], Loss: 1.9990\n",
            "Epoch [1/2], Step [300/12500], Loss: 2.1024\n",
            "Epoch [1/2], Step [400/12500], Loss: 1.6189\n",
            "Epoch [1/2], Step [500/12500], Loss: 2.9877\n",
            "Epoch [1/2], Step [600/12500], Loss: 2.5482\n",
            "Epoch [1/2], Step [700/12500], Loss: 2.0533\n",
            "Epoch [1/2], Step [800/12500], Loss: 2.1024\n",
            "Epoch [1/2], Step [900/12500], Loss: 2.6723\n",
            "Epoch [1/2], Step [1000/12500], Loss: 1.6668\n",
            "Epoch [1/2], Step [1100/12500], Loss: 2.1073\n",
            "Epoch [1/2], Step [1200/12500], Loss: 2.5352\n",
            "Epoch [1/2], Step [1300/12500], Loss: 2.7733\n",
            "Epoch [1/2], Step [1400/12500], Loss: 1.5921\n",
            "Epoch [1/2], Step [1500/12500], Loss: 3.2069\n",
            "Epoch [1/2], Step [1600/12500], Loss: 1.4908\n",
            "Epoch [1/2], Step [1700/12500], Loss: 2.3276\n",
            "Epoch [1/2], Step [1800/12500], Loss: 2.0843\n",
            "Epoch [1/2], Step [1900/12500], Loss: 1.8644\n",
            "Epoch [1/2], Step [2000/12500], Loss: 2.2740\n",
            "Epoch [1/2], Step [2100/12500], Loss: 1.7453\n",
            "Epoch [1/2], Step [2200/12500], Loss: 2.4063\n",
            "Epoch [1/2], Step [2300/12500], Loss: 1.5958\n",
            "Epoch [1/2], Step [2400/12500], Loss: 2.1833\n",
            "Epoch [1/2], Step [2500/12500], Loss: 1.9029\n",
            "Epoch [1/2], Step [2600/12500], Loss: 1.5448\n",
            "Epoch [1/2], Step [2700/12500], Loss: 2.1984\n",
            "Epoch [1/2], Step [2800/12500], Loss: 1.7237\n",
            "Epoch [1/2], Step [2900/12500], Loss: 1.7510\n",
            "Epoch [1/2], Step [3000/12500], Loss: 1.6895\n",
            "Epoch [1/2], Step [3100/12500], Loss: 1.6569\n",
            "Epoch [1/2], Step [3200/12500], Loss: 2.1467\n",
            "Epoch [1/2], Step [3300/12500], Loss: 2.3937\n",
            "Epoch [1/2], Step [3400/12500], Loss: 2.2122\n",
            "Epoch [1/2], Step [3500/12500], Loss: 1.1284\n",
            "Epoch [1/2], Step [3600/12500], Loss: 1.4899\n",
            "Epoch [1/2], Step [3700/12500], Loss: 1.3374\n",
            "Epoch [1/2], Step [3800/12500], Loss: 1.9397\n",
            "Epoch [1/2], Step [3900/12500], Loss: 1.2444\n",
            "Epoch [1/2], Step [4000/12500], Loss: 1.2320\n",
            "Epoch [1/2], Step [4100/12500], Loss: 1.3933\n",
            "Epoch [1/2], Step [4200/12500], Loss: 1.5522\n",
            "Epoch [1/2], Step [4300/12500], Loss: 1.9722\n",
            "Epoch [1/2], Step [4400/12500], Loss: 1.6013\n",
            "Epoch [1/2], Step [4500/12500], Loss: 1.1647\n",
            "Epoch [1/2], Step [4600/12500], Loss: 1.7028\n",
            "Epoch [1/2], Step [4700/12500], Loss: 1.5453\n",
            "Epoch [1/2], Step [4800/12500], Loss: 1.7605\n",
            "Epoch [1/2], Step [4900/12500], Loss: 1.5031\n",
            "Epoch [1/2], Step [5000/12500], Loss: 1.4371\n",
            "Epoch [1/2], Step [5100/12500], Loss: 2.1451\n",
            "Epoch [1/2], Step [5200/12500], Loss: 2.0035\n",
            "Epoch [1/2], Step [5300/12500], Loss: 1.4644\n",
            "Epoch [1/2], Step [5400/12500], Loss: 1.8739\n",
            "Epoch [1/2], Step [5500/12500], Loss: 1.4795\n",
            "Epoch [1/2], Step [5600/12500], Loss: 1.1820\n",
            "Epoch [1/2], Step [5700/12500], Loss: 1.4129\n",
            "Epoch [1/2], Step [5800/12500], Loss: 0.7456\n",
            "Epoch [1/2], Step [5900/12500], Loss: 0.8453\n",
            "Epoch [1/2], Step [6000/12500], Loss: 0.9714\n",
            "Epoch [1/2], Step [6100/12500], Loss: 1.5350\n",
            "Epoch [1/2], Step [6200/12500], Loss: 1.2236\n",
            "Epoch [1/2], Step [6300/12500], Loss: 1.3819\n",
            "Epoch [1/2], Step [6400/12500], Loss: 1.1705\n",
            "Epoch [1/2], Step [6500/12500], Loss: 0.9262\n",
            "Epoch [1/2], Step [6600/12500], Loss: 2.0916\n",
            "Epoch [1/2], Step [6700/12500], Loss: 2.7999\n",
            "Epoch [1/2], Step [6800/12500], Loss: 0.6535\n",
            "Epoch [1/2], Step [6900/12500], Loss: 1.4966\n",
            "Epoch [1/2], Step [7000/12500], Loss: 1.0171\n",
            "Epoch [1/2], Step [7100/12500], Loss: 2.2859\n",
            "Epoch [1/2], Step [7200/12500], Loss: 1.5531\n",
            "Epoch [1/2], Step [7300/12500], Loss: 0.7695\n",
            "Epoch [1/2], Step [7400/12500], Loss: 1.5557\n",
            "Epoch [1/2], Step [7500/12500], Loss: 0.8473\n",
            "Epoch [1/2], Step [7600/12500], Loss: 1.2334\n",
            "Epoch [1/2], Step [7700/12500], Loss: 1.5490\n",
            "Epoch [1/2], Step [7800/12500], Loss: 2.2906\n",
            "Epoch [1/2], Step [7900/12500], Loss: 1.4020\n",
            "Epoch [1/2], Step [8000/12500], Loss: 0.7204\n",
            "Epoch [1/2], Step [8100/12500], Loss: 1.0869\n",
            "Epoch [1/2], Step [8200/12500], Loss: 1.7651\n",
            "Epoch [1/2], Step [8300/12500], Loss: 0.9396\n",
            "Epoch [1/2], Step [8400/12500], Loss: 2.2864\n",
            "Epoch [1/2], Step [8500/12500], Loss: 1.6503\n",
            "Epoch [1/2], Step [8600/12500], Loss: 1.4750\n",
            "Epoch [1/2], Step [8700/12500], Loss: 0.6717\n",
            "Epoch [1/2], Step [8800/12500], Loss: 1.3801\n",
            "Epoch [1/2], Step [8900/12500], Loss: 1.5083\n",
            "Epoch [1/2], Step [9000/12500], Loss: 0.9183\n",
            "Epoch [1/2], Step [9100/12500], Loss: 1.8469\n",
            "Epoch [1/2], Step [9200/12500], Loss: 0.6225\n",
            "Epoch [1/2], Step [9300/12500], Loss: 1.3818\n",
            "Epoch [1/2], Step [9400/12500], Loss: 0.8226\n",
            "Epoch [1/2], Step [9500/12500], Loss: 1.5360\n",
            "Epoch [1/2], Step [9600/12500], Loss: 1.9827\n",
            "Epoch [1/2], Step [9700/12500], Loss: 1.2284\n",
            "Epoch [1/2], Step [9800/12500], Loss: 1.5564\n",
            "Epoch [1/2], Step [9900/12500], Loss: 1.6788\n",
            "Epoch [1/2], Step [10000/12500], Loss: 1.1579\n",
            "Epoch [1/2], Step [10100/12500], Loss: 0.8903\n",
            "Epoch [1/2], Step [10200/12500], Loss: 0.1383\n",
            "Epoch [1/2], Step [10300/12500], Loss: 1.0195\n",
            "Epoch [1/2], Step [10400/12500], Loss: 0.4798\n",
            "Epoch [1/2], Step [10500/12500], Loss: 1.3600\n",
            "Epoch [1/2], Step [10600/12500], Loss: 0.6249\n",
            "Epoch [1/2], Step [10700/12500], Loss: 2.3511\n",
            "Epoch [1/2], Step [10800/12500], Loss: 0.7745\n",
            "Epoch [1/2], Step [10900/12500], Loss: 0.9792\n",
            "Epoch [1/2], Step [11000/12500], Loss: 0.5475\n",
            "Epoch [1/2], Step [11100/12500], Loss: 0.9106\n",
            "Epoch [1/2], Step [11200/12500], Loss: 1.3988\n",
            "Epoch [1/2], Step [11300/12500], Loss: 1.2308\n",
            "Epoch [1/2], Step [11400/12500], Loss: 0.9173\n",
            "Epoch [1/2], Step [11500/12500], Loss: 2.0848\n",
            "Epoch [1/2], Step [11600/12500], Loss: 1.1380\n",
            "Epoch [1/2], Step [11700/12500], Loss: 1.0197\n",
            "Epoch [1/2], Step [11800/12500], Loss: 2.3056\n",
            "Epoch [1/2], Step [11900/12500], Loss: 1.5014\n",
            "Epoch [1/2], Step [12000/12500], Loss: 1.5225\n",
            "Epoch [1/2], Step [12100/12500], Loss: 1.4917\n",
            "Epoch [1/2], Step [12200/12500], Loss: 1.3786\n",
            "Epoch [1/2], Step [12300/12500], Loss: 1.5684\n",
            "Epoch [1/2], Step [12400/12500], Loss: 0.8315\n",
            "Epoch [1/2], Step [12500/12500], Loss: 0.8461\n",
            "Epoch [2/2], Step [100/12500], Loss: 1.0093\n",
            "Epoch [2/2], Step [200/12500], Loss: 0.4761\n",
            "Epoch [2/2], Step [300/12500], Loss: 2.0536\n",
            "Epoch [2/2], Step [400/12500], Loss: 2.1402\n",
            "Epoch [2/2], Step [500/12500], Loss: 0.2235\n",
            "Epoch [2/2], Step [600/12500], Loss: 1.4133\n",
            "Epoch [2/2], Step [700/12500], Loss: 0.5649\n",
            "Epoch [2/2], Step [800/12500], Loss: 1.5248\n",
            "Epoch [2/2], Step [900/12500], Loss: 0.7719\n",
            "Epoch [2/2], Step [1000/12500], Loss: 1.2034\n",
            "Epoch [2/2], Step [1100/12500], Loss: 0.6004\n",
            "Epoch [2/2], Step [1200/12500], Loss: 0.7599\n",
            "Epoch [2/2], Step [1300/12500], Loss: 0.4923\n",
            "Epoch [2/2], Step [1400/12500], Loss: 1.2154\n",
            "Epoch [2/2], Step [1500/12500], Loss: 0.2403\n",
            "Epoch [2/2], Step [1600/12500], Loss: 0.2903\n",
            "Epoch [2/2], Step [1700/12500], Loss: 0.8779\n",
            "Epoch [2/2], Step [1800/12500], Loss: 1.0933\n",
            "Epoch [2/2], Step [1900/12500], Loss: 1.9301\n",
            "Epoch [2/2], Step [2000/12500], Loss: 0.7758\n",
            "Epoch [2/2], Step [2100/12500], Loss: 0.7369\n",
            "Epoch [2/2], Step [2200/12500], Loss: 0.4376\n",
            "Epoch [2/2], Step [2300/12500], Loss: 2.8331\n",
            "Epoch [2/2], Step [2400/12500], Loss: 0.3192\n",
            "Epoch [2/2], Step [2500/12500], Loss: 1.5433\n",
            "Epoch [2/2], Step [2600/12500], Loss: 0.8577\n",
            "Epoch [2/2], Step [2700/12500], Loss: 2.8061\n",
            "Epoch [2/2], Step [2800/12500], Loss: 1.0510\n",
            "Epoch [2/2], Step [2900/12500], Loss: 0.9304\n",
            "Epoch [2/2], Step [3000/12500], Loss: 0.4598\n",
            "Epoch [2/2], Step [3100/12500], Loss: 0.9561\n",
            "Epoch [2/2], Step [3200/12500], Loss: 1.7786\n",
            "Epoch [2/2], Step [3300/12500], Loss: 0.5026\n",
            "Epoch [2/2], Step [3400/12500], Loss: 0.7462\n",
            "Epoch [2/2], Step [3500/12500], Loss: 0.1517\n",
            "Epoch [2/2], Step [3600/12500], Loss: 0.5288\n",
            "Epoch [2/2], Step [3700/12500], Loss: 0.5061\n",
            "Epoch [2/2], Step [3800/12500], Loss: 1.6635\n",
            "Epoch [2/2], Step [3900/12500], Loss: 1.6685\n",
            "Epoch [2/2], Step [4000/12500], Loss: 1.4756\n",
            "Epoch [2/2], Step [4100/12500], Loss: 0.5677\n",
            "Epoch [2/2], Step [4200/12500], Loss: 1.6532\n",
            "Epoch [2/2], Step [4300/12500], Loss: 0.8385\n",
            "Epoch [2/2], Step [4400/12500], Loss: 2.4966\n",
            "Epoch [2/2], Step [4500/12500], Loss: 0.8981\n",
            "Epoch [2/2], Step [4600/12500], Loss: 1.7319\n",
            "Epoch [2/2], Step [4700/12500], Loss: 0.4940\n",
            "Epoch [2/2], Step [4800/12500], Loss: 0.1793\n",
            "Epoch [2/2], Step [4900/12500], Loss: 0.6043\n",
            "Epoch [2/2], Step [5000/12500], Loss: 1.5025\n",
            "Epoch [2/2], Step [5100/12500], Loss: 0.7914\n",
            "Epoch [2/2], Step [5200/12500], Loss: 0.6227\n",
            "Epoch [2/2], Step [5300/12500], Loss: 0.2452\n",
            "Epoch [2/2], Step [5400/12500], Loss: 0.4147\n",
            "Epoch [2/2], Step [5500/12500], Loss: 0.5646\n",
            "Epoch [2/2], Step [5600/12500], Loss: 0.8118\n",
            "Epoch [2/2], Step [5700/12500], Loss: 0.8720\n",
            "Epoch [2/2], Step [5800/12500], Loss: 0.9936\n",
            "Epoch [2/2], Step [5900/12500], Loss: 1.1352\n",
            "Epoch [2/2], Step [6000/12500], Loss: 0.9409\n",
            "Epoch [2/2], Step [6100/12500], Loss: 1.8002\n",
            "Epoch [2/2], Step [6200/12500], Loss: 2.4331\n",
            "Epoch [2/2], Step [6300/12500], Loss: 1.7819\n",
            "Epoch [2/2], Step [6400/12500], Loss: 1.0246\n",
            "Epoch [2/2], Step [6500/12500], Loss: 1.1458\n",
            "Epoch [2/2], Step [6600/12500], Loss: 0.9015\n",
            "Epoch [2/2], Step [6700/12500], Loss: 1.7477\n",
            "Epoch [2/2], Step [6800/12500], Loss: 0.7573\n",
            "Epoch [2/2], Step [6900/12500], Loss: 1.1345\n",
            "Epoch [2/2], Step [7000/12500], Loss: 0.6927\n",
            "Epoch [2/2], Step [7100/12500], Loss: 0.6808\n",
            "Epoch [2/2], Step [7200/12500], Loss: 0.1793\n",
            "Epoch [2/2], Step [7300/12500], Loss: 1.1287\n",
            "Epoch [2/2], Step [7400/12500], Loss: 0.5150\n",
            "Epoch [2/2], Step [7500/12500], Loss: 0.3723\n",
            "Epoch [2/2], Step [7600/12500], Loss: 0.8664\n",
            "Epoch [2/2], Step [7700/12500], Loss: 0.1940\n",
            "Epoch [2/2], Step [7800/12500], Loss: 1.6993\n",
            "Epoch [2/2], Step [7900/12500], Loss: 0.8959\n",
            "Epoch [2/2], Step [8000/12500], Loss: 1.2310\n",
            "Epoch [2/2], Step [8100/12500], Loss: 1.6584\n",
            "Epoch [2/2], Step [8200/12500], Loss: 0.6081\n",
            "Epoch [2/2], Step [8300/12500], Loss: 0.0899\n",
            "Epoch [2/2], Step [8400/12500], Loss: 0.3027\n",
            "Epoch [2/2], Step [8500/12500], Loss: 0.5855\n",
            "Epoch [2/2], Step [8600/12500], Loss: 0.8415\n",
            "Epoch [2/2], Step [8700/12500], Loss: 0.8299\n",
            "Epoch [2/2], Step [8800/12500], Loss: 2.0386\n",
            "Epoch [2/2], Step [8900/12500], Loss: 0.6506\n",
            "Epoch [2/2], Step [9000/12500], Loss: 0.6392\n",
            "Epoch [2/2], Step [9100/12500], Loss: 0.5764\n",
            "Epoch [2/2], Step [9200/12500], Loss: 0.5885\n",
            "Epoch [2/2], Step [9300/12500], Loss: 0.2305\n",
            "Epoch [2/2], Step [9400/12500], Loss: 1.4432\n",
            "Epoch [2/2], Step [9500/12500], Loss: 0.2849\n",
            "Epoch [2/2], Step [9600/12500], Loss: 0.2276\n",
            "Epoch [2/2], Step [9700/12500], Loss: 0.3202\n",
            "Epoch [2/2], Step [9800/12500], Loss: 1.1869\n",
            "Epoch [2/2], Step [9900/12500], Loss: 0.6478\n",
            "Epoch [2/2], Step [10000/12500], Loss: 1.3885\n",
            "Epoch [2/2], Step [10100/12500], Loss: 0.8856\n",
            "Epoch [2/2], Step [10200/12500], Loss: 0.1109\n",
            "Epoch [2/2], Step [10300/12500], Loss: 0.4623\n",
            "Epoch [2/2], Step [10400/12500], Loss: 1.0333\n",
            "Epoch [2/2], Step [10500/12500], Loss: 1.3763\n",
            "Epoch [2/2], Step [10600/12500], Loss: 0.3035\n",
            "Epoch [2/2], Step [10700/12500], Loss: 0.6836\n",
            "Epoch [2/2], Step [10800/12500], Loss: 2.1077\n",
            "Epoch [2/2], Step [10900/12500], Loss: 0.8317\n",
            "Epoch [2/2], Step [11000/12500], Loss: 0.2623\n",
            "Epoch [2/2], Step [11100/12500], Loss: 1.0406\n",
            "Epoch [2/2], Step [11200/12500], Loss: 1.5530\n",
            "Epoch [2/2], Step [11300/12500], Loss: 1.0325\n",
            "Epoch [2/2], Step [11400/12500], Loss: 1.5202\n",
            "Epoch [2/2], Step [11500/12500], Loss: 1.6583\n",
            "Epoch [2/2], Step [11600/12500], Loss: 0.6959\n",
            "Epoch [2/2], Step [11700/12500], Loss: 1.1591\n",
            "Epoch [2/2], Step [11800/12500], Loss: 1.9973\n",
            "Epoch [2/2], Step [11900/12500], Loss: 0.4742\n",
            "Epoch [2/2], Step [12000/12500], Loss: 0.3765\n",
            "Epoch [2/2], Step [12100/12500], Loss: 1.1753\n",
            "Epoch [2/2], Step [12200/12500], Loss: 1.7395\n",
            "Epoch [2/2], Step [12300/12500], Loss: 0.7806\n",
            "Epoch [2/2], Step [12400/12500], Loss: 0.3229\n",
            "Epoch [2/2], Step [12500/12500], Loss: 2.9128\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "train(2, resnet18, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 70.88%\n"
          ]
        }
      ],
      "source": [
        "test(resnet18)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
