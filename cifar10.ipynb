{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D53KDb2hVU_X"
      },
      "source": [
        "# Training a CNN model on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (3.7.5)\n",
            "Requirement already satisfied: seaborn in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (0.13.2)\n",
            "Requirement already satisfied: torch in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (2.3.0)\n",
            "Requirement already satisfied: torchvision in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (2.3.0)\n",
            "Requirement already satisfied: numpy in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (1.24.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (2.9.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (6.4.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: filelock in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib seaborn torch torchvision torchaudio numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0z7zspfLVU_Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # This is to load plotting functions\n",
        "import seaborn as sns; sns.set() # This is to make the plots prettier\n",
        "import torch # This is the ML library we will use\n",
        "import torchvision # This is the supporting ML library for computer vision\n",
        "from torchvision import datasets # This is to access the CIFAR10 dataset\n",
        "from torch.utils.data import DataLoader # This is used to load the data efficiently\n",
        "import torchvision.transforms as transforms # This is used to transform data when preparing the dataset\n",
        "import numpy as np # This is used to handle arrays of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Select the device from GPU and CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgaQ7FaCVU_g",
        "outputId": "2b0f9f23-03a1-47cf-f079-431bef89dd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If GPU is available, it is chosen for computations, and if not, CPU will be used\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download the CIFAR10 dataset and create train and test data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# This is used to normalize input images\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_set = datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "test_set = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = (\n",
        "    \"plane\",\n",
        "    \"car\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"deer\",\n",
        "    \"dog\",\n",
        "    \"frog\",\n",
        "    \"horse\",\n",
        "    \"ship\",\n",
        "    \"truck\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Show example images from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frog  deer  car   bird \n"
          ]
        }
      ],
      "source": [
        "\n",
        "def imshow(img):\n",
        "    # Unnormalize\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# Print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create a simple CNN classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Cifar10Classifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.body = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=6,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2,\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=6,\n",
        "                out_channels=16,\n",
        "                kernel_size=5,\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(16 * 6 * 6, 120),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(120, 84),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(84, 10),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.body(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Cifar10Classifier(\n",
              "  (body): Sequential(\n",
              "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Flatten(start_dim=1, end_dim=-1)\n",
              "    (7): Linear(in_features=576, out_features=120, bias=True)\n",
              "    (8): ReLU()\n",
              "    (9): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (10): ReLU()\n",
              "    (11): Linear(in_features=84, out_features=10, bias=True)\n",
              "    (12): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Cifar10Classifier()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create a loss function and an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/io/miniconda3/envs/diip/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/home/io/miniconda3/envs/diip/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12500/12500], Loss: 2.3026\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "def train(num_epochs, model, loss_func, optimizer):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Train the model\n",
        "    total_step = len(train_loader)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # For each batch in the training data\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute output and loss\n",
        "            output = model(images)\n",
        "            loss = loss_func(output, labels)\n",
        "\n",
        "            # Clear gradients for this training step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute gradients\n",
        "            loss.backward()\n",
        "            # Apply gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(\n",
        "                    \"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(\n",
        "                        epoch + 1, num_epochs, i + 1, total_step, loss.item()\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "train(2, model, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, test the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 10.00%\n"
          ]
        }
      ],
      "source": [
        "def test(model):\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            test_output = model(images)\n",
        "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
        "            correct += (pred_y == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        \n",
        "        accuracy = correct / float(total)\n",
        "        print('Test Accuracy of the model: %.2f%%' % (accuracy * 100))\n",
        "test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Now, let us train more and see how the result changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1], Step [100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12500/12500], Loss: 2.3026\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "train(1, model, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 10.00%\n"
          ]
        }
      ],
      "source": [
        "test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretrained ResNet18 model on CIFAR100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the pretrained model from torchhub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/io/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CifarResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet20_cifar100 = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_resnet20\", pretrained=True)\n",
        "resnet20_cifar100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### CIFAR100 has different number of classes, but input images are compatible in shape.\n",
        "\n",
        "We can adapt this model by changing the last layer to fit our needs.\n",
        "The last layer is untrained, but we can freeze the network and train only it, since the earlier model layers are capable of processing visual information from CIFAR100.\n",
        "\n",
        "The last fully-connected layer should receive as input a vector with the same number of features as before, but the number of output dimensions should be equal to the number of classes in CIFAR10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "    <code>\n",
        "    num_features = resnet18.fc.in_features<br>\n",
        "    resnet20_cifar100.fc = torch.nn.Linear(num_features, num_classes)<br>\n",
        "    </code>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CifarResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = 10\n",
        "num_features = resnet20_cifar100.fc.in_features\n",
        "resnet20_cifar100.fc = torch.nn.Linear(num_features, num_classes)\n",
        "resnet20_cifar100.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Freeze all layers except last."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(False, True)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for param in resnet20_cifar100.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "resnet20_cifar100.fc.weight.requires_grad = True\n",
        "resnet20_cifar100.fc.bias.requires_grad = True\n",
        "\n",
        "resnet20_cifar100.conv1.weight.requires_grad, resnet20_cifar100.fc.weight.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 8.61%\n"
          ]
        }
      ],
      "source": [
        "test(resnet20_cifar100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare the training objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(resnet20_cifar100.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train and test (only the last layer is trained)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/12500], Loss: 2.4560\n",
            "Epoch [1/2], Step [200/12500], Loss: 1.8224\n",
            "Epoch [1/2], Step [300/12500], Loss: 1.8141\n",
            "Epoch [1/2], Step [400/12500], Loss: 2.1924\n",
            "Epoch [1/2], Step [500/12500], Loss: 2.3591\n",
            "Epoch [1/2], Step [600/12500], Loss: 1.7922\n",
            "Epoch [1/2], Step [700/12500], Loss: 2.0056\n",
            "Epoch [1/2], Step [800/12500], Loss: 1.3924\n",
            "Epoch [1/2], Step [900/12500], Loss: 1.1074\n",
            "Epoch [1/2], Step [1000/12500], Loss: 1.7147\n",
            "Epoch [1/2], Step [1100/12500], Loss: 1.4108\n",
            "Epoch [1/2], Step [1200/12500], Loss: 2.2930\n",
            "Epoch [1/2], Step [1300/12500], Loss: 2.3694\n",
            "Epoch [1/2], Step [1400/12500], Loss: 2.5404\n",
            "Epoch [1/2], Step [1500/12500], Loss: 1.6672\n",
            "Epoch [1/2], Step [1600/12500], Loss: 1.8191\n",
            "Epoch [1/2], Step [1700/12500], Loss: 1.7181\n",
            "Epoch [1/2], Step [1800/12500], Loss: 1.7697\n",
            "Epoch [1/2], Step [1900/12500], Loss: 1.5407\n",
            "Epoch [1/2], Step [2000/12500], Loss: 2.9034\n",
            "Epoch [1/2], Step [2100/12500], Loss: 0.7634\n",
            "Epoch [1/2], Step [2200/12500], Loss: 1.1981\n",
            "Epoch [1/2], Step [2300/12500], Loss: 2.1651\n",
            "Epoch [1/2], Step [2400/12500], Loss: 1.0406\n",
            "Epoch [1/2], Step [2500/12500], Loss: 2.2023\n",
            "Epoch [1/2], Step [2600/12500], Loss: 0.8378\n",
            "Epoch [1/2], Step [2700/12500], Loss: 1.2428\n",
            "Epoch [1/2], Step [2800/12500], Loss: 2.1929\n",
            "Epoch [1/2], Step [2900/12500], Loss: 2.1309\n",
            "Epoch [1/2], Step [3000/12500], Loss: 1.1936\n",
            "Epoch [1/2], Step [3100/12500], Loss: 1.5002\n",
            "Epoch [1/2], Step [3200/12500], Loss: 1.0745\n",
            "Epoch [1/2], Step [3300/12500], Loss: 1.9485\n",
            "Epoch [1/2], Step [3400/12500], Loss: 1.8644\n",
            "Epoch [1/2], Step [3500/12500], Loss: 1.1324\n",
            "Epoch [1/2], Step [3600/12500], Loss: 0.7448\n",
            "Epoch [1/2], Step [3700/12500], Loss: 0.9270\n",
            "Epoch [1/2], Step [3800/12500], Loss: 1.4190\n",
            "Epoch [1/2], Step [3900/12500], Loss: 1.8888\n",
            "Epoch [1/2], Step [4000/12500], Loss: 1.1597\n",
            "Epoch [1/2], Step [4100/12500], Loss: 1.8272\n",
            "Epoch [1/2], Step [4200/12500], Loss: 1.6425\n",
            "Epoch [1/2], Step [4300/12500], Loss: 0.9703\n",
            "Epoch [1/2], Step [4400/12500], Loss: 0.7603\n",
            "Epoch [1/2], Step [4500/12500], Loss: 1.2661\n",
            "Epoch [1/2], Step [4600/12500], Loss: 1.2206\n",
            "Epoch [1/2], Step [4700/12500], Loss: 0.9931\n",
            "Epoch [1/2], Step [4800/12500], Loss: 0.5937\n",
            "Epoch [1/2], Step [4900/12500], Loss: 0.6043\n",
            "Epoch [1/2], Step [5000/12500], Loss: 0.9840\n",
            "Epoch [1/2], Step [5100/12500], Loss: 0.4191\n",
            "Epoch [1/2], Step [5200/12500], Loss: 1.0812\n",
            "Epoch [1/2], Step [5300/12500], Loss: 0.7740\n",
            "Epoch [1/2], Step [5400/12500], Loss: 2.5171\n",
            "Epoch [1/2], Step [5500/12500], Loss: 0.4747\n",
            "Epoch [1/2], Step [5600/12500], Loss: 2.1934\n",
            "Epoch [1/2], Step [5700/12500], Loss: 1.4485\n",
            "Epoch [1/2], Step [5800/12500], Loss: 0.9310\n",
            "Epoch [1/2], Step [5900/12500], Loss: 0.7693\n",
            "Epoch [1/2], Step [6000/12500], Loss: 1.7815\n",
            "Epoch [1/2], Step [6100/12500], Loss: 1.5695\n",
            "Epoch [1/2], Step [6200/12500], Loss: 1.1001\n",
            "Epoch [1/2], Step [6300/12500], Loss: 1.1104\n",
            "Epoch [1/2], Step [6400/12500], Loss: 1.1915\n",
            "Epoch [1/2], Step [6500/12500], Loss: 1.4935\n",
            "Epoch [1/2], Step [6600/12500], Loss: 0.6068\n",
            "Epoch [1/2], Step [6700/12500], Loss: 1.1873\n",
            "Epoch [1/2], Step [6800/12500], Loss: 1.8188\n",
            "Epoch [1/2], Step [6900/12500], Loss: 2.2042\n",
            "Epoch [1/2], Step [7000/12500], Loss: 1.5075\n",
            "Epoch [1/2], Step [7100/12500], Loss: 1.7114\n",
            "Epoch [1/2], Step [7200/12500], Loss: 1.5084\n",
            "Epoch [1/2], Step [7300/12500], Loss: 1.1538\n",
            "Epoch [1/2], Step [7400/12500], Loss: 1.3414\n",
            "Epoch [1/2], Step [7500/12500], Loss: 1.1070\n",
            "Epoch [1/2], Step [7600/12500], Loss: 1.0179\n",
            "Epoch [1/2], Step [7700/12500], Loss: 1.1578\n",
            "Epoch [1/2], Step [7800/12500], Loss: 1.7972\n",
            "Epoch [1/2], Step [7900/12500], Loss: 1.1316\n",
            "Epoch [1/2], Step [8000/12500], Loss: 1.2311\n",
            "Epoch [1/2], Step [8100/12500], Loss: 0.9602\n",
            "Epoch [1/2], Step [8200/12500], Loss: 2.5034\n",
            "Epoch [1/2], Step [8300/12500], Loss: 1.8977\n",
            "Epoch [1/2], Step [8400/12500], Loss: 1.8819\n",
            "Epoch [1/2], Step [8500/12500], Loss: 0.6678\n",
            "Epoch [1/2], Step [8600/12500], Loss: 1.3247\n",
            "Epoch [1/2], Step [8700/12500], Loss: 1.6513\n",
            "Epoch [1/2], Step [8800/12500], Loss: 2.3262\n",
            "Epoch [1/2], Step [8900/12500], Loss: 1.1607\n",
            "Epoch [1/2], Step [9000/12500], Loss: 0.6557\n",
            "Epoch [1/2], Step [9100/12500], Loss: 1.2742\n",
            "Epoch [1/2], Step [9200/12500], Loss: 1.7343\n",
            "Epoch [1/2], Step [9300/12500], Loss: 1.0014\n",
            "Epoch [1/2], Step [9400/12500], Loss: 1.2629\n",
            "Epoch [1/2], Step [9500/12500], Loss: 0.8961\n",
            "Epoch [1/2], Step [9600/12500], Loss: 3.3624\n",
            "Epoch [1/2], Step [9700/12500], Loss: 1.8896\n",
            "Epoch [1/2], Step [9800/12500], Loss: 1.2393\n",
            "Epoch [1/2], Step [9900/12500], Loss: 1.8086\n",
            "Epoch [1/2], Step [10000/12500], Loss: 1.0060\n",
            "Epoch [1/2], Step [10100/12500], Loss: 1.9027\n",
            "Epoch [1/2], Step [10200/12500], Loss: 0.9398\n",
            "Epoch [1/2], Step [10300/12500], Loss: 1.2152\n",
            "Epoch [1/2], Step [10400/12500], Loss: 1.8133\n",
            "Epoch [1/2], Step [10500/12500], Loss: 1.5292\n",
            "Epoch [1/2], Step [10600/12500], Loss: 1.1266\n",
            "Epoch [1/2], Step [10700/12500], Loss: 1.3640\n",
            "Epoch [1/2], Step [10800/12500], Loss: 2.2696\n",
            "Epoch [1/2], Step [10900/12500], Loss: 1.2661\n",
            "Epoch [1/2], Step [11000/12500], Loss: 1.4381\n",
            "Epoch [1/2], Step [11100/12500], Loss: 0.5930\n",
            "Epoch [1/2], Step [11200/12500], Loss: 0.6445\n",
            "Epoch [1/2], Step [11300/12500], Loss: 1.6929\n",
            "Epoch [1/2], Step [11400/12500], Loss: 2.0488\n",
            "Epoch [1/2], Step [11500/12500], Loss: 1.1066\n",
            "Epoch [1/2], Step [11600/12500], Loss: 0.6041\n",
            "Epoch [1/2], Step [11700/12500], Loss: 1.0576\n",
            "Epoch [1/2], Step [11800/12500], Loss: 0.5553\n",
            "Epoch [1/2], Step [11900/12500], Loss: 1.7274\n",
            "Epoch [1/2], Step [12000/12500], Loss: 0.9930\n",
            "Epoch [1/2], Step [12100/12500], Loss: 1.6500\n",
            "Epoch [1/2], Step [12200/12500], Loss: 1.4059\n",
            "Epoch [1/2], Step [12300/12500], Loss: 2.5600\n",
            "Epoch [1/2], Step [12400/12500], Loss: 0.7576\n",
            "Epoch [1/2], Step [12500/12500], Loss: 1.9814\n",
            "Epoch [2/2], Step [100/12500], Loss: 1.0518\n",
            "Epoch [2/2], Step [200/12500], Loss: 1.3524\n",
            "Epoch [2/2], Step [300/12500], Loss: 1.7662\n",
            "Epoch [2/2], Step [400/12500], Loss: 0.4995\n",
            "Epoch [2/2], Step [500/12500], Loss: 1.1769\n",
            "Epoch [2/2], Step [600/12500], Loss: 0.9279\n",
            "Epoch [2/2], Step [700/12500], Loss: 0.6635\n",
            "Epoch [2/2], Step [800/12500], Loss: 1.2634\n",
            "Epoch [2/2], Step [900/12500], Loss: 2.2493\n",
            "Epoch [2/2], Step [1000/12500], Loss: 1.5488\n",
            "Epoch [2/2], Step [1100/12500], Loss: 0.7746\n",
            "Epoch [2/2], Step [1200/12500], Loss: 1.6725\n",
            "Epoch [2/2], Step [1300/12500], Loss: 1.7363\n",
            "Epoch [2/2], Step [1400/12500], Loss: 1.2880\n",
            "Epoch [2/2], Step [1500/12500], Loss: 1.1741\n",
            "Epoch [2/2], Step [1600/12500], Loss: 1.4177\n",
            "Epoch [2/2], Step [1700/12500], Loss: 0.5129\n",
            "Epoch [2/2], Step [1800/12500], Loss: 0.8096\n",
            "Epoch [2/2], Step [1900/12500], Loss: 0.7288\n",
            "Epoch [2/2], Step [2000/12500], Loss: 2.7656\n",
            "Epoch [2/2], Step [2100/12500], Loss: 1.1477\n",
            "Epoch [2/2], Step [2200/12500], Loss: 1.1705\n",
            "Epoch [2/2], Step [2300/12500], Loss: 1.0349\n",
            "Epoch [2/2], Step [2400/12500], Loss: 1.4961\n",
            "Epoch [2/2], Step [2500/12500], Loss: 1.1818\n",
            "Epoch [2/2], Step [2600/12500], Loss: 1.1256\n",
            "Epoch [2/2], Step [2700/12500], Loss: 1.5040\n",
            "Epoch [2/2], Step [2800/12500], Loss: 1.0124\n",
            "Epoch [2/2], Step [2900/12500], Loss: 1.0302\n",
            "Epoch [2/2], Step [3000/12500], Loss: 0.6526\n",
            "Epoch [2/2], Step [3100/12500], Loss: 1.9446\n",
            "Epoch [2/2], Step [3200/12500], Loss: 1.1945\n",
            "Epoch [2/2], Step [3300/12500], Loss: 1.8522\n",
            "Epoch [2/2], Step [3400/12500], Loss: 1.4690\n",
            "Epoch [2/2], Step [3500/12500], Loss: 1.0188\n",
            "Epoch [2/2], Step [3600/12500], Loss: 1.2570\n",
            "Epoch [2/2], Step [3700/12500], Loss: 0.7069\n",
            "Epoch [2/2], Step [3800/12500], Loss: 1.2790\n",
            "Epoch [2/2], Step [3900/12500], Loss: 1.4761\n",
            "Epoch [2/2], Step [4000/12500], Loss: 1.9279\n",
            "Epoch [2/2], Step [4100/12500], Loss: 1.6650\n",
            "Epoch [2/2], Step [4200/12500], Loss: 1.0979\n",
            "Epoch [2/2], Step [4300/12500], Loss: 1.1039\n",
            "Epoch [2/2], Step [4400/12500], Loss: 2.0829\n",
            "Epoch [2/2], Step [4500/12500], Loss: 1.6186\n",
            "Epoch [2/2], Step [4600/12500], Loss: 0.9534\n",
            "Epoch [2/2], Step [4700/12500], Loss: 1.7591\n",
            "Epoch [2/2], Step [4800/12500], Loss: 0.4570\n",
            "Epoch [2/2], Step [4900/12500], Loss: 0.9533\n",
            "Epoch [2/2], Step [5000/12500], Loss: 0.9261\n",
            "Epoch [2/2], Step [5100/12500], Loss: 1.7896\n",
            "Epoch [2/2], Step [5200/12500], Loss: 1.3057\n",
            "Epoch [2/2], Step [5300/12500], Loss: 2.6435\n",
            "Epoch [2/2], Step [5400/12500], Loss: 1.9678\n",
            "Epoch [2/2], Step [5500/12500], Loss: 1.1601\n",
            "Epoch [2/2], Step [5600/12500], Loss: 1.5571\n",
            "Epoch [2/2], Step [5700/12500], Loss: 0.3267\n",
            "Epoch [2/2], Step [5800/12500], Loss: 1.6313\n",
            "Epoch [2/2], Step [5900/12500], Loss: 1.4062\n",
            "Epoch [2/2], Step [6000/12500], Loss: 1.9029\n",
            "Epoch [2/2], Step [6100/12500], Loss: 0.4350\n",
            "Epoch [2/2], Step [6200/12500], Loss: 1.0426\n",
            "Epoch [2/2], Step [6300/12500], Loss: 0.7628\n",
            "Epoch [2/2], Step [6400/12500], Loss: 0.5080\n",
            "Epoch [2/2], Step [6500/12500], Loss: 1.1402\n",
            "Epoch [2/2], Step [6600/12500], Loss: 1.2995\n",
            "Epoch [2/2], Step [6700/12500], Loss: 1.2277\n",
            "Epoch [2/2], Step [6800/12500], Loss: 1.8789\n",
            "Epoch [2/2], Step [6900/12500], Loss: 0.9820\n",
            "Epoch [2/2], Step [7000/12500], Loss: 0.6276\n",
            "Epoch [2/2], Step [7100/12500], Loss: 1.5677\n",
            "Epoch [2/2], Step [7200/12500], Loss: 0.5505\n",
            "Epoch [2/2], Step [7300/12500], Loss: 1.0776\n",
            "Epoch [2/2], Step [7400/12500], Loss: 0.9137\n",
            "Epoch [2/2], Step [7500/12500], Loss: 1.6791\n",
            "Epoch [2/2], Step [7600/12500], Loss: 1.3480\n",
            "Epoch [2/2], Step [7700/12500], Loss: 1.5157\n",
            "Epoch [2/2], Step [7800/12500], Loss: 2.4920\n",
            "Epoch [2/2], Step [7900/12500], Loss: 1.5806\n",
            "Epoch [2/2], Step [8000/12500], Loss: 1.0164\n",
            "Epoch [2/2], Step [8100/12500], Loss: 1.3512\n",
            "Epoch [2/2], Step [8200/12500], Loss: 1.7561\n",
            "Epoch [2/2], Step [8300/12500], Loss: 1.8623\n",
            "Epoch [2/2], Step [8400/12500], Loss: 0.7234\n",
            "Epoch [2/2], Step [8500/12500], Loss: 2.1165\n",
            "Epoch [2/2], Step [8600/12500], Loss: 0.5507\n",
            "Epoch [2/2], Step [8700/12500], Loss: 1.2715\n",
            "Epoch [2/2], Step [8800/12500], Loss: 1.8729\n",
            "Epoch [2/2], Step [8900/12500], Loss: 0.5432\n",
            "Epoch [2/2], Step [9000/12500], Loss: 1.2971\n",
            "Epoch [2/2], Step [9100/12500], Loss: 2.0490\n",
            "Epoch [2/2], Step [9200/12500], Loss: 1.1809\n",
            "Epoch [2/2], Step [9300/12500], Loss: 1.1543\n",
            "Epoch [2/2], Step [9400/12500], Loss: 1.9086\n",
            "Epoch [2/2], Step [9500/12500], Loss: 1.8787\n",
            "Epoch [2/2], Step [9600/12500], Loss: 1.1081\n",
            "Epoch [2/2], Step [9700/12500], Loss: 0.3788\n",
            "Epoch [2/2], Step [9800/12500], Loss: 0.9873\n",
            "Epoch [2/2], Step [9900/12500], Loss: 1.6911\n",
            "Epoch [2/2], Step [10000/12500], Loss: 1.1955\n",
            "Epoch [2/2], Step [10100/12500], Loss: 1.0133\n",
            "Epoch [2/2], Step [10200/12500], Loss: 0.8087\n",
            "Epoch [2/2], Step [10300/12500], Loss: 1.4734\n",
            "Epoch [2/2], Step [10400/12500], Loss: 0.9487\n",
            "Epoch [2/2], Step [10500/12500], Loss: 1.9077\n",
            "Epoch [2/2], Step [10600/12500], Loss: 0.8931\n",
            "Epoch [2/2], Step [10700/12500], Loss: 1.6999\n",
            "Epoch [2/2], Step [10800/12500], Loss: 0.9710\n",
            "Epoch [2/2], Step [10900/12500], Loss: 0.9858\n",
            "Epoch [2/2], Step [11000/12500], Loss: 1.9378\n",
            "Epoch [2/2], Step [11100/12500], Loss: 1.0063\n",
            "Epoch [2/2], Step [11200/12500], Loss: 1.8938\n",
            "Epoch [2/2], Step [11300/12500], Loss: 1.7527\n",
            "Epoch [2/2], Step [11400/12500], Loss: 0.8309\n",
            "Epoch [2/2], Step [11500/12500], Loss: 0.4262\n",
            "Epoch [2/2], Step [11600/12500], Loss: 1.3239\n",
            "Epoch [2/2], Step [11700/12500], Loss: 1.2542\n",
            "Epoch [2/2], Step [11800/12500], Loss: 0.6032\n",
            "Epoch [2/2], Step [11900/12500], Loss: 2.7843\n",
            "Epoch [2/2], Step [12000/12500], Loss: 1.5293\n",
            "Epoch [2/2], Step [12100/12500], Loss: 1.1505\n",
            "Epoch [2/2], Step [12200/12500], Loss: 1.7407\n",
            "Epoch [2/2], Step [12300/12500], Loss: 1.9249\n",
            "Epoch [2/2], Step [12400/12500], Loss: 1.3588\n",
            "Epoch [2/2], Step [12500/12500], Loss: 0.5995\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "train(2, resnet20_cifar100, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 62.94%\n"
          ]
        }
      ],
      "source": [
        "test(resnet20_cifar100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretrained ResNet18 model on ImageNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the pretrained model from torchvision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet18 = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "resnet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ImageNet has different image size and the number of classes.\n",
        "\n",
        "We can adapt this model by changing the first and the last layers to fit our needs.\n",
        "Those layers are untrained, but the knowledge from all other layers is still relevant and helps the model to be trained faster.\n",
        "\n",
        "The first convolutional layer of ResNet needs to receive as input an image with 3 channels, have 64 output channels, 3x3 filter, stride 1 and padding 1 with no bias.\n",
        "\n",
        "The last fully-connected layer should receive as input a vector with the same number of features as before, but the number of output dimensions should be equal to the number of classes in CIFAR10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "    <code>\n",
        "    resnet18.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)<br>\n",
        "    num_features = resnet18.fc.in_features<br>\n",
        "    resnet18.fc = torch.nn.Linear(num_features, num_classes)<br>\n",
        "    </code>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = 10\n",
        "resnet18.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "num_features = resnet18.fc.in_features\n",
        "resnet18.fc = torch.nn.Linear(num_features, num_classes)\n",
        "resnet18.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### It is pretrained on ImageNet, so the starting accuracy on CIFAR10 should be bad, let us check it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 9.90%\n"
          ]
        }
      ],
      "source": [
        "test(resnet18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare the training objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(resnet18.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/12500], Loss: 2.7609\n",
            "Epoch [1/2], Step [200/12500], Loss: 2.4524\n",
            "Epoch [1/2], Step [300/12500], Loss: 2.2776\n",
            "Epoch [1/2], Step [400/12500], Loss: 2.3939\n",
            "Epoch [1/2], Step [500/12500], Loss: 1.8081\n",
            "Epoch [1/2], Step [600/12500], Loss: 2.1802\n",
            "Epoch [1/2], Step [700/12500], Loss: 3.1815\n",
            "Epoch [1/2], Step [800/12500], Loss: 2.0593\n",
            "Epoch [1/2], Step [900/12500], Loss: 2.2240\n",
            "Epoch [1/2], Step [1000/12500], Loss: 2.4946\n",
            "Epoch [1/2], Step [1100/12500], Loss: 2.3982\n",
            "Epoch [1/2], Step [1200/12500], Loss: 2.1547\n",
            "Epoch [1/2], Step [1300/12500], Loss: 1.6972\n",
            "Epoch [1/2], Step [1400/12500], Loss: 1.7882\n",
            "Epoch [1/2], Step [1500/12500], Loss: 2.2530\n",
            "Epoch [1/2], Step [1600/12500], Loss: 2.1752\n",
            "Epoch [1/2], Step [1700/12500], Loss: 1.4363\n",
            "Epoch [1/2], Step [1800/12500], Loss: 1.4488\n",
            "Epoch [1/2], Step [1900/12500], Loss: 2.8319\n",
            "Epoch [1/2], Step [2000/12500], Loss: 1.8800\n",
            "Epoch [1/2], Step [2100/12500], Loss: 2.0016\n",
            "Epoch [1/2], Step [2200/12500], Loss: 2.1406\n",
            "Epoch [1/2], Step [2300/12500], Loss: 1.5556\n",
            "Epoch [1/2], Step [2400/12500], Loss: 2.7937\n",
            "Epoch [1/2], Step [2500/12500], Loss: 1.8012\n",
            "Epoch [1/2], Step [2600/12500], Loss: 2.0917\n",
            "Epoch [1/2], Step [2700/12500], Loss: 2.8777\n",
            "Epoch [1/2], Step [2800/12500], Loss: 2.8977\n",
            "Epoch [1/2], Step [2900/12500], Loss: 1.4187\n",
            "Epoch [1/2], Step [3000/12500], Loss: 1.2241\n",
            "Epoch [1/2], Step [3100/12500], Loss: 1.9747\n",
            "Epoch [1/2], Step [3200/12500], Loss: 2.2541\n",
            "Epoch [1/2], Step [3300/12500], Loss: 1.6765\n",
            "Epoch [1/2], Step [3400/12500], Loss: 2.3932\n",
            "Epoch [1/2], Step [3500/12500], Loss: 1.6816\n",
            "Epoch [1/2], Step [3600/12500], Loss: 1.6176\n",
            "Epoch [1/2], Step [3700/12500], Loss: 0.9526\n",
            "Epoch [1/2], Step [3800/12500], Loss: 2.1589\n",
            "Epoch [1/2], Step [3900/12500], Loss: 2.0277\n",
            "Epoch [1/2], Step [4000/12500], Loss: 1.8960\n",
            "Epoch [1/2], Step [4100/12500], Loss: 1.3854\n",
            "Epoch [1/2], Step [4200/12500], Loss: 1.9930\n",
            "Epoch [1/2], Step [4300/12500], Loss: 1.4732\n",
            "Epoch [1/2], Step [4400/12500], Loss: 1.7298\n",
            "Epoch [1/2], Step [4500/12500], Loss: 1.2662\n",
            "Epoch [1/2], Step [4600/12500], Loss: 1.6555\n",
            "Epoch [1/2], Step [4700/12500], Loss: 1.9001\n",
            "Epoch [1/2], Step [4800/12500], Loss: 1.1137\n",
            "Epoch [1/2], Step [4900/12500], Loss: 2.1514\n",
            "Epoch [1/2], Step [5000/12500], Loss: 1.8225\n",
            "Epoch [1/2], Step [5100/12500], Loss: 1.7693\n",
            "Epoch [1/2], Step [5200/12500], Loss: 1.2982\n",
            "Epoch [1/2], Step [5300/12500], Loss: 1.3779\n",
            "Epoch [1/2], Step [5400/12500], Loss: 1.1605\n",
            "Epoch [1/2], Step [5500/12500], Loss: 2.6467\n",
            "Epoch [1/2], Step [5600/12500], Loss: 1.7410\n",
            "Epoch [1/2], Step [5700/12500], Loss: 0.8301\n",
            "Epoch [1/2], Step [5800/12500], Loss: 1.1408\n",
            "Epoch [1/2], Step [5900/12500], Loss: 1.3805\n",
            "Epoch [1/2], Step [6000/12500], Loss: 1.4472\n",
            "Epoch [1/2], Step [6100/12500], Loss: 0.6480\n",
            "Epoch [1/2], Step [6200/12500], Loss: 1.3216\n",
            "Epoch [1/2], Step [6300/12500], Loss: 1.7455\n",
            "Epoch [1/2], Step [6400/12500], Loss: 1.1412\n",
            "Epoch [1/2], Step [6500/12500], Loss: 2.3959\n",
            "Epoch [1/2], Step [6600/12500], Loss: 2.0012\n",
            "Epoch [1/2], Step [6700/12500], Loss: 1.6188\n",
            "Epoch [1/2], Step [6800/12500], Loss: 1.1632\n",
            "Epoch [1/2], Step [6900/12500], Loss: 1.6705\n",
            "Epoch [1/2], Step [7000/12500], Loss: 1.0986\n",
            "Epoch [1/2], Step [7100/12500], Loss: 1.2240\n",
            "Epoch [1/2], Step [7200/12500], Loss: 0.8025\n",
            "Epoch [1/2], Step [7300/12500], Loss: 1.5741\n",
            "Epoch [1/2], Step [7400/12500], Loss: 2.7257\n",
            "Epoch [1/2], Step [7500/12500], Loss: 1.0559\n",
            "Epoch [1/2], Step [7600/12500], Loss: 1.2315\n",
            "Epoch [1/2], Step [7700/12500], Loss: 2.4411\n",
            "Epoch [1/2], Step [7800/12500], Loss: 1.5549\n",
            "Epoch [1/2], Step [7900/12500], Loss: 1.6417\n",
            "Epoch [1/2], Step [8000/12500], Loss: 2.6686\n",
            "Epoch [1/2], Step [8100/12500], Loss: 1.6188\n",
            "Epoch [1/2], Step [8200/12500], Loss: 1.3320\n",
            "Epoch [1/2], Step [8300/12500], Loss: 1.3163\n",
            "Epoch [1/2], Step [8400/12500], Loss: 1.1594\n",
            "Epoch [1/2], Step [8500/12500], Loss: 2.8430\n",
            "Epoch [1/2], Step [8600/12500], Loss: 1.5893\n",
            "Epoch [1/2], Step [8700/12500], Loss: 0.8455\n",
            "Epoch [1/2], Step [8800/12500], Loss: 1.6983\n",
            "Epoch [1/2], Step [8900/12500], Loss: 1.3654\n",
            "Epoch [1/2], Step [9000/12500], Loss: 1.7936\n",
            "Epoch [1/2], Step [9100/12500], Loss: 1.6703\n",
            "Epoch [1/2], Step [9200/12500], Loss: 1.1612\n",
            "Epoch [1/2], Step [9300/12500], Loss: 1.2173\n",
            "Epoch [1/2], Step [9400/12500], Loss: 0.8455\n",
            "Epoch [1/2], Step [9500/12500], Loss: 1.0490\n",
            "Epoch [1/2], Step [9600/12500], Loss: 1.8341\n",
            "Epoch [1/2], Step [9700/12500], Loss: 0.9767\n",
            "Epoch [1/2], Step [9800/12500], Loss: 1.2248\n",
            "Epoch [1/2], Step [9900/12500], Loss: 1.8563\n",
            "Epoch [1/2], Step [10000/12500], Loss: 2.2208\n",
            "Epoch [1/2], Step [10100/12500], Loss: 0.4102\n",
            "Epoch [1/2], Step [10200/12500], Loss: 1.5639\n",
            "Epoch [1/2], Step [10300/12500], Loss: 1.3410\n",
            "Epoch [1/2], Step [10400/12500], Loss: 1.7687\n",
            "Epoch [1/2], Step [10500/12500], Loss: 1.2449\n",
            "Epoch [1/2], Step [10600/12500], Loss: 0.7599\n",
            "Epoch [1/2], Step [10700/12500], Loss: 1.0096\n",
            "Epoch [1/2], Step [10800/12500], Loss: 1.9440\n",
            "Epoch [1/2], Step [10900/12500], Loss: 1.2681\n",
            "Epoch [1/2], Step [11000/12500], Loss: 0.9078\n",
            "Epoch [1/2], Step [11100/12500], Loss: 2.0108\n",
            "Epoch [1/2], Step [11200/12500], Loss: 1.6324\n",
            "Epoch [1/2], Step [11300/12500], Loss: 1.0748\n",
            "Epoch [1/2], Step [11400/12500], Loss: 0.8606\n",
            "Epoch [1/2], Step [11500/12500], Loss: 1.7708\n",
            "Epoch [1/2], Step [11600/12500], Loss: 1.4337\n",
            "Epoch [1/2], Step [11700/12500], Loss: 2.0963\n",
            "Epoch [1/2], Step [11800/12500], Loss: 0.8937\n",
            "Epoch [1/2], Step [11900/12500], Loss: 1.3486\n",
            "Epoch [1/2], Step [12000/12500], Loss: 0.8095\n",
            "Epoch [1/2], Step [12100/12500], Loss: 0.5158\n",
            "Epoch [1/2], Step [12200/12500], Loss: 0.9607\n",
            "Epoch [1/2], Step [12300/12500], Loss: 0.8281\n",
            "Epoch [1/2], Step [12400/12500], Loss: 0.8113\n",
            "Epoch [1/2], Step [12500/12500], Loss: 0.6108\n",
            "Epoch [2/2], Step [100/12500], Loss: 0.8931\n",
            "Epoch [2/2], Step [200/12500], Loss: 0.5367\n",
            "Epoch [2/2], Step [300/12500], Loss: 1.0708\n",
            "Epoch [2/2], Step [400/12500], Loss: 2.4635\n",
            "Epoch [2/2], Step [500/12500], Loss: 1.1651\n",
            "Epoch [2/2], Step [600/12500], Loss: 1.5048\n",
            "Epoch [2/2], Step [700/12500], Loss: 1.1762\n",
            "Epoch [2/2], Step [800/12500], Loss: 0.5480\n",
            "Epoch [2/2], Step [900/12500], Loss: 0.5119\n",
            "Epoch [2/2], Step [1000/12500], Loss: 0.7276\n",
            "Epoch [2/2], Step [1100/12500], Loss: 0.7439\n",
            "Epoch [2/2], Step [1200/12500], Loss: 0.3279\n",
            "Epoch [2/2], Step [1300/12500], Loss: 1.2950\n",
            "Epoch [2/2], Step [1400/12500], Loss: 0.8427\n",
            "Epoch [2/2], Step [1500/12500], Loss: 2.1524\n",
            "Epoch [2/2], Step [1600/12500], Loss: 0.7305\n",
            "Epoch [2/2], Step [1700/12500], Loss: 0.8780\n",
            "Epoch [2/2], Step [1800/12500], Loss: 0.2056\n",
            "Epoch [2/2], Step [1900/12500], Loss: 0.4010\n",
            "Epoch [2/2], Step [2000/12500], Loss: 0.9959\n",
            "Epoch [2/2], Step [2100/12500], Loss: 0.3781\n",
            "Epoch [2/2], Step [2200/12500], Loss: 1.4256\n",
            "Epoch [2/2], Step [2300/12500], Loss: 2.9002\n",
            "Epoch [2/2], Step [2400/12500], Loss: 1.1481\n",
            "Epoch [2/2], Step [2500/12500], Loss: 0.7869\n",
            "Epoch [2/2], Step [2600/12500], Loss: 0.5850\n",
            "Epoch [2/2], Step [2700/12500], Loss: 0.4272\n",
            "Epoch [2/2], Step [2800/12500], Loss: 0.5910\n",
            "Epoch [2/2], Step [2900/12500], Loss: 0.4306\n",
            "Epoch [2/2], Step [3000/12500], Loss: 1.6476\n",
            "Epoch [2/2], Step [3100/12500], Loss: 1.0183\n",
            "Epoch [2/2], Step [3200/12500], Loss: 1.4069\n",
            "Epoch [2/2], Step [3300/12500], Loss: 0.7164\n",
            "Epoch [2/2], Step [3400/12500], Loss: 1.2481\n",
            "Epoch [2/2], Step [3500/12500], Loss: 0.9121\n",
            "Epoch [2/2], Step [3600/12500], Loss: 0.4600\n",
            "Epoch [2/2], Step [3700/12500], Loss: 1.2458\n",
            "Epoch [2/2], Step [3800/12500], Loss: 0.4780\n",
            "Epoch [2/2], Step [3900/12500], Loss: 0.8888\n",
            "Epoch [2/2], Step [4000/12500], Loss: 1.8695\n",
            "Epoch [2/2], Step [4100/12500], Loss: 0.7876\n",
            "Epoch [2/2], Step [4200/12500], Loss: 0.9519\n",
            "Epoch [2/2], Step [4300/12500], Loss: 0.7202\n",
            "Epoch [2/2], Step [4400/12500], Loss: 1.2578\n",
            "Epoch [2/2], Step [4500/12500], Loss: 0.1600\n",
            "Epoch [2/2], Step [4600/12500], Loss: 0.9938\n",
            "Epoch [2/2], Step [4700/12500], Loss: 0.5322\n",
            "Epoch [2/2], Step [4800/12500], Loss: 1.7522\n",
            "Epoch [2/2], Step [4900/12500], Loss: 0.3805\n",
            "Epoch [2/2], Step [5000/12500], Loss: 1.0460\n",
            "Epoch [2/2], Step [5100/12500], Loss: 1.8050\n",
            "Epoch [2/2], Step [5200/12500], Loss: 0.7823\n",
            "Epoch [2/2], Step [5300/12500], Loss: 0.8175\n",
            "Epoch [2/2], Step [5400/12500], Loss: 0.2372\n",
            "Epoch [2/2], Step [5500/12500], Loss: 0.1779\n",
            "Epoch [2/2], Step [5600/12500], Loss: 0.2686\n",
            "Epoch [2/2], Step [5700/12500], Loss: 1.1594\n",
            "Epoch [2/2], Step [5800/12500], Loss: 0.8103\n",
            "Epoch [2/2], Step [5900/12500], Loss: 1.0962\n",
            "Epoch [2/2], Step [6000/12500], Loss: 0.4921\n",
            "Epoch [2/2], Step [6100/12500], Loss: 0.7975\n",
            "Epoch [2/2], Step [6200/12500], Loss: 1.5842\n",
            "Epoch [2/2], Step [6300/12500], Loss: 0.7535\n",
            "Epoch [2/2], Step [6400/12500], Loss: 1.2122\n",
            "Epoch [2/2], Step [6500/12500], Loss: 0.3869\n",
            "Epoch [2/2], Step [6600/12500], Loss: 0.8558\n",
            "Epoch [2/2], Step [6700/12500], Loss: 0.6862\n",
            "Epoch [2/2], Step [6800/12500], Loss: 1.1343\n",
            "Epoch [2/2], Step [6900/12500], Loss: 0.3582\n",
            "Epoch [2/2], Step [7000/12500], Loss: 1.2579\n",
            "Epoch [2/2], Step [7100/12500], Loss: 2.5530\n",
            "Epoch [2/2], Step [7200/12500], Loss: 0.5083\n",
            "Epoch [2/2], Step [7300/12500], Loss: 1.1419\n",
            "Epoch [2/2], Step [7400/12500], Loss: 0.3631\n",
            "Epoch [2/2], Step [7500/12500], Loss: 1.1009\n",
            "Epoch [2/2], Step [7600/12500], Loss: 0.3070\n",
            "Epoch [2/2], Step [7700/12500], Loss: 0.3650\n",
            "Epoch [2/2], Step [7800/12500], Loss: 0.4380\n",
            "Epoch [2/2], Step [7900/12500], Loss: 1.0658\n",
            "Epoch [2/2], Step [8000/12500], Loss: 1.1252\n",
            "Epoch [2/2], Step [8100/12500], Loss: 0.5293\n",
            "Epoch [2/2], Step [8200/12500], Loss: 0.7692\n",
            "Epoch [2/2], Step [8300/12500], Loss: 0.6403\n",
            "Epoch [2/2], Step [8400/12500], Loss: 1.1278\n",
            "Epoch [2/2], Step [8500/12500], Loss: 0.2593\n",
            "Epoch [2/2], Step [8600/12500], Loss: 1.4002\n",
            "Epoch [2/2], Step [8700/12500], Loss: 0.7756\n",
            "Epoch [2/2], Step [8800/12500], Loss: 0.9228\n",
            "Epoch [2/2], Step [8900/12500], Loss: 1.5848\n",
            "Epoch [2/2], Step [9000/12500], Loss: 1.0970\n",
            "Epoch [2/2], Step [9100/12500], Loss: 1.2470\n",
            "Epoch [2/2], Step [9200/12500], Loss: 1.0322\n",
            "Epoch [2/2], Step [9300/12500], Loss: 0.3618\n",
            "Epoch [2/2], Step [9400/12500], Loss: 0.5673\n",
            "Epoch [2/2], Step [9500/12500], Loss: 1.3083\n",
            "Epoch [2/2], Step [9600/12500], Loss: 0.3757\n",
            "Epoch [2/2], Step [9700/12500], Loss: 1.1198\n",
            "Epoch [2/2], Step [9800/12500], Loss: 0.6019\n",
            "Epoch [2/2], Step [9900/12500], Loss: 0.9947\n",
            "Epoch [2/2], Step [10000/12500], Loss: 0.6474\n",
            "Epoch [2/2], Step [10100/12500], Loss: 0.9712\n",
            "Epoch [2/2], Step [10200/12500], Loss: 0.3793\n",
            "Epoch [2/2], Step [10300/12500], Loss: 1.0348\n",
            "Epoch [2/2], Step [10400/12500], Loss: 0.9784\n",
            "Epoch [2/2], Step [10500/12500], Loss: 0.3517\n",
            "Epoch [2/2], Step [10600/12500], Loss: 0.5204\n",
            "Epoch [2/2], Step [10700/12500], Loss: 1.2928\n",
            "Epoch [2/2], Step [10800/12500], Loss: 1.0045\n",
            "Epoch [2/2], Step [10900/12500], Loss: 1.4968\n",
            "Epoch [2/2], Step [11000/12500], Loss: 0.6552\n",
            "Epoch [2/2], Step [11100/12500], Loss: 0.1996\n",
            "Epoch [2/2], Step [11200/12500], Loss: 1.1146\n",
            "Epoch [2/2], Step [11300/12500], Loss: 2.1171\n",
            "Epoch [2/2], Step [11400/12500], Loss: 0.5678\n",
            "Epoch [2/2], Step [11500/12500], Loss: 1.4761\n",
            "Epoch [2/2], Step [11600/12500], Loss: 1.5614\n",
            "Epoch [2/2], Step [11700/12500], Loss: 0.4618\n",
            "Epoch [2/2], Step [11800/12500], Loss: 0.5112\n",
            "Epoch [2/2], Step [11900/12500], Loss: 0.8448\n",
            "Epoch [2/2], Step [12000/12500], Loss: 0.8763\n",
            "Epoch [2/2], Step [12100/12500], Loss: 0.7755\n",
            "Epoch [2/2], Step [12200/12500], Loss: 0.5271\n",
            "Epoch [2/2], Step [12300/12500], Loss: 0.0917\n",
            "Epoch [2/2], Step [12400/12500], Loss: 0.9809\n",
            "Epoch [2/2], Step [12500/12500], Loss: 0.9610\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "train(2, resnet18, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 70.02%\n"
          ]
        }
      ],
      "source": [
        "test(resnet18)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
