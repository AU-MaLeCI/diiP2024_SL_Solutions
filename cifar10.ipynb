{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D53KDb2hVU_X"
      },
      "source": [
        "# Training a CNN model on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (3.7.5)\n",
            "Requirement already satisfied: seaborn in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (0.13.2)\n",
            "Requirement already satisfied: torch in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (2.3.0)\n",
            "Requirement already satisfied: torchvision in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (2.3.0)\n",
            "Requirement already satisfied: numpy in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (1.24.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (2.9.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from matplotlib) (6.4.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: filelock in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/io/miniconda3/envs/diip/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib seaborn torch torchvision torchaudio numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0z7zspfLVU_Y"
      },
      "outputs": [],
      "source": [
        "# Dependencies\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set() # this import just makes the plots prettier\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgaQ7FaCVU_g",
        "outputId": "2b0f9f23-03a1-47cf-f079-431bef89dd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n",
        "Collect the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_set = datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "test_set = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = (\n",
        "    \"plane\",\n",
        "    \"car\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"deer\",\n",
        "    \"dog\",\n",
        "    \"frog\",\n",
        "    \"horse\",\n",
        "    \"ship\",\n",
        "    \"truck\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show example images from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deer  truck deer  cat  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "def imshow(img):\n",
        "    # Unnormalize\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# Print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Cifar10Classifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.body = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=6,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2,\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=6,\n",
        "                out_channels=16,\n",
        "                kernel_size=5,\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(16 * 6 * 6, 120),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(120, 84),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(84, 10),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.body(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Cifar10Classifier(\n",
              "  (body): Sequential(\n",
              "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Flatten(start_dim=1, end_dim=-1)\n",
              "    (7): Linear(in_features=576, out_features=120, bias=True)\n",
              "    (8): ReLU()\n",
              "    (9): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (10): ReLU()\n",
              "    (11): Linear(in_features=84, out_features=10, bias=True)\n",
              "    (12): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Cifar10Classifier()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a loss function and an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/io/miniconda3/envs/diip/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/home/io/miniconda3/envs/diip/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/12500], Loss: 2.2675\n",
            "Epoch [1/2], Step [200/12500], Loss: 2.3394\n",
            "Epoch [1/2], Step [300/12500], Loss: 2.1170\n",
            "Epoch [1/2], Step [400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [700/12500], Loss: 2.2930\n",
            "Epoch [1/2], Step [800/12500], Loss: 2.1989\n",
            "Epoch [1/2], Step [900/12500], Loss: 1.9521\n",
            "Epoch [1/2], Step [1000/12500], Loss: 2.3357\n",
            "Epoch [1/2], Step [1100/12500], Loss: 2.1976\n",
            "Epoch [1/2], Step [1200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1300/12500], Loss: 2.3188\n",
            "Epoch [1/2], Step [1400/12500], Loss: 2.3043\n",
            "Epoch [1/2], Step [1500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [1900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [2900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [3900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [4900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [5900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [6900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [7900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [8900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [9900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [10900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11500/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11600/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11700/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11800/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [11900/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12000/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12100/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12200/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12300/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12400/12500], Loss: 2.3026\n",
            "Epoch [1/2], Step [12500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [1900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [2900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [3900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [4900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [5900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [6900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [7900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [8900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [9900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [10900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11500/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11600/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11700/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11800/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [11900/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12000/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12100/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12200/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12300/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12400/12500], Loss: 2.3026\n",
            "Epoch [2/2], Step [12500/12500], Loss: 2.3026\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "def train(num_epochs, model, loss_func, optimizer):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Train the model\n",
        "    total_step = len(train_loader)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # For each batch in the training data\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute output and loss\n",
        "            output = model(images)\n",
        "            loss = loss_func(output, labels)\n",
        "\n",
        "            # Clear gradients for this training step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute gradients\n",
        "            loss.backward()\n",
        "            # Apply gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(\n",
        "                    \"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(\n",
        "                        epoch + 1, num_epochs, i + 1, total_step, loss.item()\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "train(2, model, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, test the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 10.00%\n"
          ]
        }
      ],
      "source": [
        "def test(model):\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            test_output = model(images)\n",
        "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
        "            correct += (pred_y == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        \n",
        "        accuracy = correct / float(total)\n",
        "        print('Test Accuracy of the model: %.2f%%' % (accuracy * 100))\n",
        "test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let us train more and see how the result changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1], Step [100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [1900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [2900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [3900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [4900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [5900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [6900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [7900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [8900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [9900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [10900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11500/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11600/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11700/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11800/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [11900/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12000/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12100/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12200/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12300/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12400/12500], Loss: 2.3026\n",
            "Epoch [1/1], Step [12500/12500], Loss: 2.3026\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "train(1, model, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 10.00%\n"
          ]
        }
      ],
      "source": [
        "test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretrained ResNet18 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the pretrained model from torchvision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet18 = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "resnet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ImageNet has different image size and the number of classes.\n",
        "We can adapt tghis model by changing the first and the last layers to fit our needs.\n",
        "Those layers are untrained, but the knowledge from all other layers is still relevant and helps the model to be trained faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = 10\n",
        "resnet18.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "num_features = resnet18.fc.in_features\n",
        "resnet18.fc = torch.nn.Linear(num_features, num_classes)\n",
        "resnet18.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is pretrained on ImageNet, so the starting accuracy on CIFAR10 should be bad, let us check it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 8.72%\n"
          ]
        }
      ],
      "source": [
        "test(resnet18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the training objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(resnet18.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/12500], Loss: 2.3442\n",
            "Epoch [1/2], Step [200/12500], Loss: 1.7272\n",
            "Epoch [1/2], Step [300/12500], Loss: 2.5202\n",
            "Epoch [1/2], Step [400/12500], Loss: 1.8865\n",
            "Epoch [1/2], Step [500/12500], Loss: 2.3240\n",
            "Epoch [1/2], Step [600/12500], Loss: 2.2769\n",
            "Epoch [1/2], Step [700/12500], Loss: 2.9590\n",
            "Epoch [1/2], Step [800/12500], Loss: 3.0510\n",
            "Epoch [1/2], Step [900/12500], Loss: 1.8837\n",
            "Epoch [1/2], Step [1000/12500], Loss: 1.0588\n",
            "Epoch [1/2], Step [1100/12500], Loss: 2.0606\n",
            "Epoch [1/2], Step [1200/12500], Loss: 1.9962\n",
            "Epoch [1/2], Step [1300/12500], Loss: 2.7324\n",
            "Epoch [1/2], Step [1400/12500], Loss: 1.5882\n",
            "Epoch [1/2], Step [1500/12500], Loss: 1.8309\n",
            "Epoch [1/2], Step [1600/12500], Loss: 2.2709\n",
            "Epoch [1/2], Step [1700/12500], Loss: 1.7544\n",
            "Epoch [1/2], Step [1800/12500], Loss: 1.6873\n",
            "Epoch [1/2], Step [1900/12500], Loss: 3.4426\n",
            "Epoch [1/2], Step [2000/12500], Loss: 2.3010\n",
            "Epoch [1/2], Step [2100/12500], Loss: 1.5945\n",
            "Epoch [1/2], Step [2200/12500], Loss: 2.4755\n",
            "Epoch [1/2], Step [2300/12500], Loss: 1.8823\n",
            "Epoch [1/2], Step [2400/12500], Loss: 1.2052\n",
            "Epoch [1/2], Step [2500/12500], Loss: 2.8632\n",
            "Epoch [1/2], Step [2600/12500], Loss: 2.1613\n",
            "Epoch [1/2], Step [2700/12500], Loss: 1.4825\n",
            "Epoch [1/2], Step [2800/12500], Loss: 2.5429\n",
            "Epoch [1/2], Step [2900/12500], Loss: 1.8747\n",
            "Epoch [1/2], Step [3000/12500], Loss: 1.4624\n",
            "Epoch [1/2], Step [3100/12500], Loss: 1.6853\n",
            "Epoch [1/2], Step [3200/12500], Loss: 1.8594\n",
            "Epoch [1/2], Step [3300/12500], Loss: 2.1747\n",
            "Epoch [1/2], Step [3400/12500], Loss: 1.3980\n",
            "Epoch [1/2], Step [3500/12500], Loss: 1.0723\n",
            "Epoch [1/2], Step [3600/12500], Loss: 1.9567\n",
            "Epoch [1/2], Step [3700/12500], Loss: 2.1933\n",
            "Epoch [1/2], Step [3800/12500], Loss: 1.6778\n",
            "Epoch [1/2], Step [3900/12500], Loss: 2.0090\n",
            "Epoch [1/2], Step [4000/12500], Loss: 1.7454\n",
            "Epoch [1/2], Step [4100/12500], Loss: 2.3299\n",
            "Epoch [1/2], Step [4200/12500], Loss: 1.9187\n",
            "Epoch [1/2], Step [4300/12500], Loss: 1.7201\n",
            "Epoch [1/2], Step [4400/12500], Loss: 1.1602\n",
            "Epoch [1/2], Step [4500/12500], Loss: 1.7966\n",
            "Epoch [1/2], Step [4600/12500], Loss: 2.0867\n",
            "Epoch [1/2], Step [4700/12500], Loss: 2.0519\n",
            "Epoch [1/2], Step [4800/12500], Loss: 1.1459\n",
            "Epoch [1/2], Step [4900/12500], Loss: 1.6732\n",
            "Epoch [1/2], Step [5000/12500], Loss: 1.6768\n",
            "Epoch [1/2], Step [5100/12500], Loss: 1.2618\n",
            "Epoch [1/2], Step [5200/12500], Loss: 2.3532\n",
            "Epoch [1/2], Step [5300/12500], Loss: 1.4463\n",
            "Epoch [1/2], Step [5400/12500], Loss: 1.8100\n",
            "Epoch [1/2], Step [5500/12500], Loss: 1.0459\n",
            "Epoch [1/2], Step [5600/12500], Loss: 1.5479\n",
            "Epoch [1/2], Step [5700/12500], Loss: 1.5147\n",
            "Epoch [1/2], Step [5800/12500], Loss: 1.0481\n",
            "Epoch [1/2], Step [5900/12500], Loss: 1.8220\n",
            "Epoch [1/2], Step [6000/12500], Loss: 1.6885\n",
            "Epoch [1/2], Step [6100/12500], Loss: 1.1684\n",
            "Epoch [1/2], Step [6200/12500], Loss: 0.4428\n",
            "Epoch [1/2], Step [6300/12500], Loss: 1.9230\n",
            "Epoch [1/2], Step [6400/12500], Loss: 1.1482\n",
            "Epoch [1/2], Step [6500/12500], Loss: 2.4801\n",
            "Epoch [1/2], Step [6600/12500], Loss: 1.4909\n",
            "Epoch [1/2], Step [6700/12500], Loss: 1.8763\n",
            "Epoch [1/2], Step [6800/12500], Loss: 1.4872\n",
            "Epoch [1/2], Step [6900/12500], Loss: 1.3289\n",
            "Epoch [1/2], Step [7000/12500], Loss: 1.7470\n",
            "Epoch [1/2], Step [7100/12500], Loss: 1.4157\n",
            "Epoch [1/2], Step [7200/12500], Loss: 1.6578\n",
            "Epoch [1/2], Step [7300/12500], Loss: 1.8286\n",
            "Epoch [1/2], Step [7400/12500], Loss: 0.8235\n",
            "Epoch [1/2], Step [7500/12500], Loss: 1.0596\n",
            "Epoch [1/2], Step [7600/12500], Loss: 2.2788\n",
            "Epoch [1/2], Step [7700/12500], Loss: 0.4855\n",
            "Epoch [1/2], Step [7800/12500], Loss: 0.9796\n",
            "Epoch [1/2], Step [7900/12500], Loss: 2.2915\n",
            "Epoch [1/2], Step [8000/12500], Loss: 1.1675\n",
            "Epoch [1/2], Step [8100/12500], Loss: 0.8261\n",
            "Epoch [1/2], Step [8200/12500], Loss: 0.8608\n",
            "Epoch [1/2], Step [8300/12500], Loss: 0.6904\n",
            "Epoch [1/2], Step [8400/12500], Loss: 1.1089\n",
            "Epoch [1/2], Step [8500/12500], Loss: 0.7291\n",
            "Epoch [1/2], Step [8600/12500], Loss: 0.6403\n",
            "Epoch [1/2], Step [8700/12500], Loss: 0.9978\n",
            "Epoch [1/2], Step [8800/12500], Loss: 2.0949\n",
            "Epoch [1/2], Step [8900/12500], Loss: 1.4688\n",
            "Epoch [1/2], Step [9000/12500], Loss: 0.4754\n",
            "Epoch [1/2], Step [9100/12500], Loss: 0.8695\n",
            "Epoch [1/2], Step [9200/12500], Loss: 0.7650\n",
            "Epoch [1/2], Step [9300/12500], Loss: 2.2352\n",
            "Epoch [1/2], Step [9400/12500], Loss: 0.4088\n",
            "Epoch [1/2], Step [9500/12500], Loss: 0.8588\n",
            "Epoch [1/2], Step [9600/12500], Loss: 1.7576\n",
            "Epoch [1/2], Step [9700/12500], Loss: 1.3926\n",
            "Epoch [1/2], Step [9800/12500], Loss: 1.6782\n",
            "Epoch [1/2], Step [9900/12500], Loss: 1.5519\n",
            "Epoch [1/2], Step [10000/12500], Loss: 1.1263\n",
            "Epoch [1/2], Step [10100/12500], Loss: 0.6090\n",
            "Epoch [1/2], Step [10200/12500], Loss: 1.1461\n",
            "Epoch [1/2], Step [10300/12500], Loss: 0.5662\n",
            "Epoch [1/2], Step [10400/12500], Loss: 0.8035\n",
            "Epoch [1/2], Step [10500/12500], Loss: 0.7111\n",
            "Epoch [1/2], Step [10600/12500], Loss: 1.1257\n",
            "Epoch [1/2], Step [10700/12500], Loss: 0.4847\n",
            "Epoch [1/2], Step [10800/12500], Loss: 1.0841\n",
            "Epoch [1/2], Step [10900/12500], Loss: 0.6632\n",
            "Epoch [1/2], Step [11000/12500], Loss: 2.6125\n",
            "Epoch [1/2], Step [11100/12500], Loss: 1.4882\n",
            "Epoch [1/2], Step [11200/12500], Loss: 2.8655\n",
            "Epoch [1/2], Step [11300/12500], Loss: 1.4134\n",
            "Epoch [1/2], Step [11400/12500], Loss: 0.4699\n",
            "Epoch [1/2], Step [11500/12500], Loss: 0.5927\n",
            "Epoch [1/2], Step [11600/12500], Loss: 0.6923\n",
            "Epoch [1/2], Step [11700/12500], Loss: 0.8926\n",
            "Epoch [1/2], Step [11800/12500], Loss: 1.4267\n",
            "Epoch [1/2], Step [11900/12500], Loss: 2.1707\n",
            "Epoch [1/2], Step [12000/12500], Loss: 0.9562\n",
            "Epoch [1/2], Step [12100/12500], Loss: 1.7124\n",
            "Epoch [1/2], Step [12200/12500], Loss: 1.3449\n",
            "Epoch [1/2], Step [12300/12500], Loss: 0.6681\n",
            "Epoch [1/2], Step [12400/12500], Loss: 1.0421\n",
            "Epoch [1/2], Step [12500/12500], Loss: 1.2401\n",
            "Epoch [2/2], Step [100/12500], Loss: 2.4359\n",
            "Epoch [2/2], Step [200/12500], Loss: 0.5035\n",
            "Epoch [2/2], Step [300/12500], Loss: 1.1041\n",
            "Epoch [2/2], Step [400/12500], Loss: 1.0348\n",
            "Epoch [2/2], Step [500/12500], Loss: 1.3129\n",
            "Epoch [2/2], Step [600/12500], Loss: 0.2128\n",
            "Epoch [2/2], Step [700/12500], Loss: 2.5799\n",
            "Epoch [2/2], Step [800/12500], Loss: 2.2375\n",
            "Epoch [2/2], Step [900/12500], Loss: 0.8038\n",
            "Epoch [2/2], Step [1000/12500], Loss: 0.4477\n",
            "Epoch [2/2], Step [1100/12500], Loss: 0.4359\n",
            "Epoch [2/2], Step [1200/12500], Loss: 1.0976\n",
            "Epoch [2/2], Step [1300/12500], Loss: 0.5426\n",
            "Epoch [2/2], Step [1400/12500], Loss: 0.8855\n",
            "Epoch [2/2], Step [1500/12500], Loss: 1.5345\n",
            "Epoch [2/2], Step [1600/12500], Loss: 1.2470\n",
            "Epoch [2/2], Step [1700/12500], Loss: 2.3185\n",
            "Epoch [2/2], Step [1800/12500], Loss: 1.5927\n",
            "Epoch [2/2], Step [1900/12500], Loss: 0.9535\n",
            "Epoch [2/2], Step [2000/12500], Loss: 1.6655\n",
            "Epoch [2/2], Step [2100/12500], Loss: 0.8293\n",
            "Epoch [2/2], Step [2200/12500], Loss: 0.5499\n",
            "Epoch [2/2], Step [2300/12500], Loss: 0.8972\n",
            "Epoch [2/2], Step [2400/12500], Loss: 0.6470\n",
            "Epoch [2/2], Step [2500/12500], Loss: 0.9567\n",
            "Epoch [2/2], Step [2600/12500], Loss: 1.2286\n",
            "Epoch [2/2], Step [2700/12500], Loss: 0.6583\n",
            "Epoch [2/2], Step [2800/12500], Loss: 0.6180\n",
            "Epoch [2/2], Step [2900/12500], Loss: 1.0797\n",
            "Epoch [2/2], Step [3000/12500], Loss: 3.2484\n",
            "Epoch [2/2], Step [3100/12500], Loss: 1.5786\n",
            "Epoch [2/2], Step [3200/12500], Loss: 1.2803\n",
            "Epoch [2/2], Step [3300/12500], Loss: 0.4812\n",
            "Epoch [2/2], Step [3400/12500], Loss: 1.7642\n",
            "Epoch [2/2], Step [3500/12500], Loss: 1.9313\n",
            "Epoch [2/2], Step [3600/12500], Loss: 0.6933\n",
            "Epoch [2/2], Step [3700/12500], Loss: 0.7555\n",
            "Epoch [2/2], Step [3800/12500], Loss: 0.5174\n",
            "Epoch [2/2], Step [3900/12500], Loss: 0.7166\n",
            "Epoch [2/2], Step [4000/12500], Loss: 0.8795\n",
            "Epoch [2/2], Step [4100/12500], Loss: 0.6360\n",
            "Epoch [2/2], Step [4200/12500], Loss: 1.8680\n",
            "Epoch [2/2], Step [4300/12500], Loss: 0.8027\n",
            "Epoch [2/2], Step [4400/12500], Loss: 0.7266\n",
            "Epoch [2/2], Step [4500/12500], Loss: 0.9636\n",
            "Epoch [2/2], Step [4600/12500], Loss: 1.0285\n",
            "Epoch [2/2], Step [4700/12500], Loss: 0.0368\n",
            "Epoch [2/2], Step [4800/12500], Loss: 0.7809\n",
            "Epoch [2/2], Step [4900/12500], Loss: 0.8449\n",
            "Epoch [2/2], Step [5000/12500], Loss: 0.6647\n",
            "Epoch [2/2], Step [5100/12500], Loss: 0.6252\n",
            "Epoch [2/2], Step [5200/12500], Loss: 0.4428\n",
            "Epoch [2/2], Step [5300/12500], Loss: 1.2211\n",
            "Epoch [2/2], Step [5400/12500], Loss: 0.7598\n",
            "Epoch [2/2], Step [5500/12500], Loss: 0.4955\n",
            "Epoch [2/2], Step [5600/12500], Loss: 0.4890\n",
            "Epoch [2/2], Step [5700/12500], Loss: 0.3409\n",
            "Epoch [2/2], Step [5800/12500], Loss: 0.6851\n",
            "Epoch [2/2], Step [5900/12500], Loss: 0.2337\n",
            "Epoch [2/2], Step [6000/12500], Loss: 0.6961\n",
            "Epoch [2/2], Step [6100/12500], Loss: 1.5273\n",
            "Epoch [2/2], Step [6200/12500], Loss: 1.2010\n",
            "Epoch [2/2], Step [6300/12500], Loss: 1.6356\n",
            "Epoch [2/2], Step [6400/12500], Loss: 1.6361\n",
            "Epoch [2/2], Step [6500/12500], Loss: 0.3054\n",
            "Epoch [2/2], Step [6600/12500], Loss: 1.8107\n",
            "Epoch [2/2], Step [6700/12500], Loss: 1.1395\n",
            "Epoch [2/2], Step [6800/12500], Loss: 1.0711\n",
            "Epoch [2/2], Step [6900/12500], Loss: 0.7252\n",
            "Epoch [2/2], Step [7000/12500], Loss: 0.9587\n",
            "Epoch [2/2], Step [7100/12500], Loss: 0.9804\n",
            "Epoch [2/2], Step [7200/12500], Loss: 1.1428\n",
            "Epoch [2/2], Step [7300/12500], Loss: 2.1742\n",
            "Epoch [2/2], Step [7400/12500], Loss: 1.3809\n",
            "Epoch [2/2], Step [7500/12500], Loss: 1.0588\n",
            "Epoch [2/2], Step [7600/12500], Loss: 0.5344\n",
            "Epoch [2/2], Step [7700/12500], Loss: 0.4599\n",
            "Epoch [2/2], Step [7800/12500], Loss: 1.1997\n",
            "Epoch [2/2], Step [7900/12500], Loss: 1.0588\n",
            "Epoch [2/2], Step [8000/12500], Loss: 0.7932\n",
            "Epoch [2/2], Step [8100/12500], Loss: 0.3265\n",
            "Epoch [2/2], Step [8200/12500], Loss: 0.7567\n",
            "Epoch [2/2], Step [8300/12500], Loss: 0.4242\n",
            "Epoch [2/2], Step [8400/12500], Loss: 0.1175\n",
            "Epoch [2/2], Step [8500/12500], Loss: 0.8834\n",
            "Epoch [2/2], Step [8600/12500], Loss: 0.1521\n",
            "Epoch [2/2], Step [8700/12500], Loss: 2.1000\n",
            "Epoch [2/2], Step [8800/12500], Loss: 2.0479\n",
            "Epoch [2/2], Step [8900/12500], Loss: 1.0649\n",
            "Epoch [2/2], Step [9000/12500], Loss: 1.9120\n",
            "Epoch [2/2], Step [9100/12500], Loss: 0.7277\n",
            "Epoch [2/2], Step [9200/12500], Loss: 1.1878\n",
            "Epoch [2/2], Step [9300/12500], Loss: 0.0327\n",
            "Epoch [2/2], Step [9400/12500], Loss: 1.4459\n",
            "Epoch [2/2], Step [9500/12500], Loss: 0.5285\n",
            "Epoch [2/2], Step [9600/12500], Loss: 0.9284\n",
            "Epoch [2/2], Step [9700/12500], Loss: 0.9764\n",
            "Epoch [2/2], Step [9800/12500], Loss: 0.2745\n",
            "Epoch [2/2], Step [9900/12500], Loss: 0.6382\n",
            "Epoch [2/2], Step [10000/12500], Loss: 0.9198\n",
            "Epoch [2/2], Step [10100/12500], Loss: 0.8560\n",
            "Epoch [2/2], Step [10200/12500], Loss: 0.4647\n",
            "Epoch [2/2], Step [10300/12500], Loss: 0.6682\n",
            "Epoch [2/2], Step [10400/12500], Loss: 0.2001\n",
            "Epoch [2/2], Step [10500/12500], Loss: 1.3436\n",
            "Epoch [2/2], Step [10600/12500], Loss: 1.0870\n",
            "Epoch [2/2], Step [10700/12500], Loss: 2.6281\n",
            "Epoch [2/2], Step [10800/12500], Loss: 0.4937\n",
            "Epoch [2/2], Step [10900/12500], Loss: 0.6521\n",
            "Epoch [2/2], Step [11000/12500], Loss: 1.3905\n",
            "Epoch [2/2], Step [11100/12500], Loss: 1.7922\n",
            "Epoch [2/2], Step [11200/12500], Loss: 0.2164\n",
            "Epoch [2/2], Step [11300/12500], Loss: 0.3727\n",
            "Epoch [2/2], Step [11400/12500], Loss: 1.3540\n",
            "Epoch [2/2], Step [11500/12500], Loss: 0.7606\n",
            "Epoch [2/2], Step [11600/12500], Loss: 1.6142\n",
            "Epoch [2/2], Step [11700/12500], Loss: 0.6908\n",
            "Epoch [2/2], Step [11800/12500], Loss: 0.5966\n",
            "Epoch [2/2], Step [11900/12500], Loss: 0.8271\n",
            "Epoch [2/2], Step [12000/12500], Loss: 0.9325\n",
            "Epoch [2/2], Step [12100/12500], Loss: 1.0438\n",
            "Epoch [2/2], Step [12200/12500], Loss: 1.7019\n",
            "Epoch [2/2], Step [12300/12500], Loss: 1.1028\n",
            "Epoch [2/2], Step [12400/12500], Loss: 0.8105\n",
            "Epoch [2/2], Step [12500/12500], Loss: 0.2448\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "train(2, resnet18, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model: 71.52%\n"
          ]
        }
      ],
      "source": [
        "test(resnet18)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
